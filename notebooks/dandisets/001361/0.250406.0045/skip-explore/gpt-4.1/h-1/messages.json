[
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata about Dandiset 001361 version 0.250406.0045."
      },
      {
        "type": "text",
        "text": "{\n  \"id\": \"DANDI:001361/0.250406.0045\",\n  \"doi\": \"10.48324/dandi.001361/0.250406.0045\",\n  \"url\": \"https://dandiarchive.org/dandiset/001361/0.250406.0045\",\n  \"name\": \"A flexible hippocampal population code for experience relative to reward\",\n  \"about\": [\n    {\n      \"name\": \"hippocampus\",\n      \"schemaKey\": \"Anatomy\"\n    }\n  ],\n  \"access\": [\n    {\n      \"status\": \"dandi:OpenAccess\",\n      \"schemaKey\": \"AccessRequirements\"\n    }\n  ],\n  \"license\": [\n    \"spdx:CC-BY-4.0\"\n  ],\n  \"version\": \"0.250406.0045\",\n  \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.9/context.json\",\n  \"citation\": \"Sosa, Marielena; Plitt, Mark H.; Giocomo, Lisa M. (2025) A flexible hippocampal population code for experience relative to reward (Version 0.250406.0045) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.001361/0.250406.0045\",\n  \"keywords\": [\n    \"hippocampus, navigation, learning, memory, 2-photon imaging, place cells\"\n  ],\n  \"protocol\": [],\n  \"schemaKey\": \"Dandiset\",\n  \"identifier\": \"DANDI:001361\",\n  \"repository\": \"https://dandiarchive.org\",\n  \"contributor\": [\n    {\n      \"name\": \"National Institutes of Health (NIH)\",\n      \"roleName\": [\n        \"dcite:Funder\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"awardNumber\": \"1 K99 MH 135993-01\"\n    },\n    {\n      \"name\": \"Sosa, Marielena\",\n      \"roleName\": [\n        \"dcite:Author\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-0762-1128\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Plitt, Mark H.\",\n      \"roleName\": [\n        \"dcite:Author\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-7154-6292\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Giocomo, Lisa M.\",\n      \"email\": \"giocomo@stanford.edu\",\n      \"roleName\": [\n        \"dcite:Author\",\n        \"dcite:ContactPerson\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-0416-2528\",\n      \"includeInCitation\": true\n    }\n  ],\n  \"dateCreated\": \"2025-03-13T06:23:11.234153+00:00\",\n  \"description\": \"2-photon imaging and behavioral data from hippocampal area CA1 during virtual reality navigation in mice. Included in Sosa, Plitt, & Giocomo, \\\"A flexible hippocampal population code for experience relative to reward,\\\" Nature Neuroscience.\\n\\nTo reinforce rewarding behaviors, events leading up to and following rewards must be remembered. Hippocampal place cell activity spans spatial and non-spatial episodes, but whether hippocampal activity encodes entire sequences of events relative to reward is unknown. To test this, we performed two-photon imaging of hippocampal CA1 as mice navigated virtual environments with changing hidden reward locations. When the reward moved, a subpopulation of neurons updated their firing fields to the same relative position with respect to reward, constructing behavioral timescale sequences spanning the entire task. Over learning, this reward-relative representation became more robust as additional neurons were recruited, and changes in reward-relative firing often preceded behavioral adaptations following reward relocation. Concurrently, the spatial environment code was maintained through a parallel, dynamic subpopulation rather than through dedicated cell classes. These findings reveal how hippocampal ensembles flexibly encode multiple aspects of experience while amplifying behaviorally relevant information.\",\n  \"publishedBy\": {\n    \"id\": \"urn:uuid:1a375579-42d8-4ca6-ae14-2162860f5fc0\",\n    \"name\": \"DANDI publish\",\n    \"endDate\": \"2025-04-06T00:45:27.761521+00:00\",\n    \"schemaKey\": \"PublishActivity\",\n    \"startDate\": \"2025-04-06T00:45:27.761521+00:00\",\n    \"wasAssociatedWith\": [\n      {\n        \"id\": \"urn:uuid:3ea3f3df-9c3f-43b6-b04a-964aa33da62b\",\n        \"name\": \"DANDI API\",\n        \"version\": \"0.1.0\",\n        \"schemaKey\": \"Software\",\n        \"identifier\": \"RRID:SCR_017571\"\n      }\n    ]\n  },\n  \"studyTarget\": [],\n  \"assetsSummary\": {\n    \"species\": [\n      {\n        \"name\": \"Mus musculus - House mouse\",\n        \"schemaKey\": \"SpeciesType\",\n        \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n      }\n    ],\n    \"approach\": [\n      {\n        \"name\": \"microscopy approach; cell population imaging\",\n        \"schemaKey\": \"ApproachType\"\n      },\n      {\n        \"name\": \"behavioral approach\",\n        \"schemaKey\": \"ApproachType\"\n      }\n    ],\n    \"schemaKey\": \"AssetsSummary\",\n    \"dataStandard\": [\n      {\n        \"name\": \"Neurodata Without Borders (NWB)\",\n        \"schemaKey\": \"StandardsType\",\n        \"identifier\": \"RRID:SCR_015242\"\n      }\n    ],\n    \"numberOfBytes\": 92560251500,\n    \"numberOfFiles\": 152,\n    \"numberOfSubjects\": 11,\n    \"variableMeasured\": [\n      \"TwoPhotonSeries\",\n      \"PlaneSegmentation\",\n      \"BehavioralTimeSeries\",\n      \"ProcessingModule\",\n      \"ImagingPlane\",\n      \"OpticalChannel\"\n    ],\n    \"measurementTechnique\": [\n      {\n        \"name\": \"two-photon microscopy technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"analytical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"behavioral technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"surgical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      }\n    ]\n  },\n  \"datePublished\": \"2025-04-06T00:45:27.761521+00:00\",\n  \"schemaVersion\": \"0.6.9\",\n  \"ethicsApproval\": [],\n  \"wasGeneratedBy\": [\n    {\n      \"name\": \"Neural circuit dynamics of spatial reward memory\",\n      \"endDate\": \"2026-04-30T23:59:00-07:00\",\n      \"schemaKey\": \"Project\",\n      \"startDate\": \"2024-05-03T00:00:00-07:00\",\n      \"identifier\": \"K99MH135993\",\n      \"description\": \"BRAIN Initiative K99\"\n    }\n  ],\n  \"relatedResource\": [\n    {\n      \"url\": \"https://github.com/GiocomoLab/Sosa_et_al_2024\",\n      \"name\": \"Custom analysis code\",\n      \"relation\": \"dcite:IsSupplementedBy\",\n      \"schemaKey\": \"Resource\",\n      \"repository\": \"Github\",\n      \"resourceType\": \"dcite:Software\"\n    }\n  ],\n  \"manifestLocation\": [\n    \"https://dandiarchive.s3.amazonaws.com/dandisets/001361/0.250406.0045/assets.yaml\"\n  ]\n}"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is a listing of some of the files in the Dandiset."
      },
      {
        "type": "text",
        "text": "sub-m11/sub-m11_ses-05_behavior+ophys.nwb\nsub-m11/sub-m11_ses-04_behavior+ophys.nwb\nsub-m11/sub-m11_ses-07_behavior+ophys.nwb\nsub-m11/sub-m11_ses-06_behavior+ophys.nwb\nsub-m11/sub-m11_ses-08_behavior+ophys.nwb\nsub-m11/sub-m11_ses-10_behavior+ophys.nwb\nsub-m11/sub-m11_ses-03_behavior+ophys.nwb\nsub-m11/sub-m11_ses-09_behavior+ophys.nwb\nsub-m11/sub-m11_ses-11_behavior+ophys.nwb\nsub-m11/sub-m11_ses-12_behavior+ophys.nwb"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata and usage information about the file sub-m11/sub-m11_ses-05_behavior+ophys.nwb."
      },
      {
        "type": "text",
        "text": "\nimport pynwb\nimport h5py\nimport remfile\n\n# Load\nfrom dandi.dandiapi import DandiAPIClient\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"001361\", \"0.250406.0045\")\nurl = next(dandiset.get_assets_by_glob(\"sub-m11/sub-m11_ses-05_behavior+ophys.nwb\")).download_url\n\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\nnwb # (NWBFile)\nnwb.session_description # (str) processed suite2p data\nnwb.identifier # (str) /data/InVivoDA/GCAMP11/25_02_2023/Env1_LocationA_to_C\nnwb.session_start_time # (datetime) 2023-02-25T00:00:00-08:00\nnwb.timestamps_reference_time # (datetime) 2023-02-25T00:00:00-08:00\nnwb.file_create_date # (list) [datetime.datetime(2025, 3, 12, 23, 45, 33, 486687, tzinfo=tzoffset(None, -25200))]\nnwb.experimenter # (tuple) ['Mari Sosa']\nnwb.acquisition # (LabelledDict)\nacquisition = nwb.acquisition\nTwoPhotonSeries = acquisition[\"TwoPhotonSeries\"]\nTwoPhotonSeries # (TwoPhotonSeries)\nTwoPhotonSeries.starting_time # (float64) 0.0\nTwoPhotonSeries.rate # (float64) 15.5078125\nTwoPhotonSeries.resolution # (float64) -1.0\nTwoPhotonSeries.comments # (str) no comments\nTwoPhotonSeries.description # (str) no description\nTwoPhotonSeries.conversion # (float64) 1.0\nTwoPhotonSeries.offset # (float64) 0.0\nTwoPhotonSeries.unit # (str) volt\nTwoPhotonSeries.data # (Dataset) shape (1, 1, 1); dtype float64\n# TwoPhotonSeries.data[:, :, :] # Access all data\n# TwoPhotonSeries.data[0, :, :] # Access first plane\nTwoPhotonSeries.starting_time_unit # (str) seconds\nTwoPhotonSeries.dimension # (Dataset) shape (2,); dtype int64\n# TwoPhotonSeries.dimension[:] # Access all data\n# TwoPhotonSeries.dimension[0:n] # Access first n elements\n# First few values of TwoPhotonSeries.dimension: [512 796]\nTwoPhotonSeries.format # (str) raw\nTwoPhotonSeries.imaging_plane # (ImagingPlane)\nTwoPhotonSeries.imaging_plane.optical_channel # (list)\nTwoPhotonSeries.imaging_plane.description # (str) standard\nTwoPhotonSeries.imaging_plane.excitation_lambda # (float64) 920.0\nTwoPhotonSeries.imaging_plane.imaging_rate # (float64) 15.5078125\nTwoPhotonSeries.imaging_plane.indicator # (str) GCaMP7f\nTwoPhotonSeries.imaging_plane.location # (str) hippocampus, CA1\nTwoPhotonSeries.imaging_plane.conversion # (float) 1.0\nTwoPhotonSeries.imaging_plane.unit # (str) meters\nTwoPhotonSeries.imaging_plane.origin_coords_unit # (str) meters\nTwoPhotonSeries.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# TwoPhotonSeries.imaging_plane.grid_spacing[:] # Access all data\n# TwoPhotonSeries.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of TwoPhotonSeries.imaging_plane.grid_spacing: [2. 2.]\nTwoPhotonSeries.imaging_plane.grid_spacing_unit # (str) microns\nTwoPhotonSeries.imaging_plane.device # (Device)\nTwoPhotonSeries.imaging_plane.device.description # (str) My two-photon microscope\nTwoPhotonSeries.imaging_plane.device.manufacturer # (str) Neurolabware\nnwb.processing # (LabelledDict)\nprocessing = nwb.processing\nbehavior = processing[\"behavior\"]\nbehavior # (ProcessingModule)\nbehavior.description # (str) behavior data\nbehavior.data_interfaces # (LabelledDict)\ndata_interfaces = behavior.data_interfaces\nBehavioralTimeSeries = data_interfaces[\"BehavioralTimeSeries\"]\nBehavioralTimeSeries # (BehavioralTimeSeries)\nBehavioralTimeSeries.time_series # (LabelledDict)\ntime_series = BehavioralTimeSeries.time_series\nReward = time_series[\"Reward\"]\nReward # (TimeSeries)\nReward.resolution # (float64) -1.0\nReward.comments # (str) no comments\nReward.description # (str) reward delivery\nReward.conversion # (float64) 1.0\nReward.offset # (float64) 0.0\nReward.unit # (str) mL\nReward.data # (Dataset) shape (69,); dtype float64\n# Reward.data[:] # Access all data\n# Reward.data[0:n] # Access first n elements\nReward.timestamps # (Dataset) shape (69,); dtype float64\n# Reward.timestamps[:] # Access all data\n# Reward.timestamps[0:n] # Access first n elements\nReward.timestamps_unit # (str) seconds\nReward.interval # (int) 1\nautoreward = time_series[\"autoreward\"]\nautoreward # (TimeSeries)\nautoreward.resolution # (float64) -1.0\nautoreward.comments # (str) no comments\nautoreward.description # (str) whether trial was automatically rewarded if the subject failed to lick\nautoreward.conversion # (float64) 1.0\nautoreward.offset # (float64) 0.0\nautoreward.unit # (str) integer\nautoreward.data # (Dataset) shape (20684,); dtype float64\n# autoreward.data[:] # Access all data\n# autoreward.data[0:n] # Access first n elements\nautoreward.timestamps # (Dataset) shape (20684,); dtype float64\n# autoreward.timestamps[:] # Access all data\n# autoreward.timestamps[0:n] # Access first n elements\nautoreward.timestamps_unit # (str) seconds\nautoreward.interval # (int) 1\nenvironment = time_series[\"environment\"]\nenvironment # (TimeSeries)\nenvironment.resolution # (float64) -1.0\nenvironment.comments # (str) no comments\nenvironment.description # (str) Virtual reality environment\nenvironment.conversion # (float64) 1.0\nenvironment.offset # (float64) 0.0\nenvironment.unit # (str) AU\nenvironment.data # (Dataset) shape (20684,); dtype float64\n# environment.data[:] # Access all data\n# environment.data[0:n] # Access first n elements\nenvironment.timestamps # (Dataset) shape (20684,); dtype float64\n# environment.timestamps[:] # Access all data\n# environment.timestamps[0:n] # Access first n elements\nenvironment.timestamps_unit # (str) seconds\nenvironment.interval # (int) 1\nlick = time_series[\"lick\"]\nlick # (TimeSeries)\nlick.resolution # (float64) -1.0\nlick.comments # (str) no comments\nlick.description # (str) lick detection by capacitive sensor, cumulative per imaging frame\nlick.conversion # (float64) 1.0\nlick.offset # (float64) 0.0\nlick.unit # (str) AU\nlick.data # (Dataset) shape (20684,); dtype float64\n# lick.data[:] # Access all data\n# lick.data[0:n] # Access first n elements\nlick.timestamps # (Dataset) shape (20684,); dtype float64\n# lick.timestamps[:] # Access all data\n# lick.timestamps[0:n] # Access first n elements\nlick.timestamps_unit # (str) seconds\nlick.interval # (int) 1\nposition = time_series[\"position\"]\nposition # (TimeSeries)\nposition.resolution # (float64) -1.0\nposition.comments # (str) no comments\nposition.description # (str) Position in a virtual linear track\nposition.conversion # (float64) 1.0\nposition.offset # (float64) 0.0\nposition.unit # (str) cm\nposition.data # (Dataset) shape (20684,); dtype float64\n# position.data[:] # Access all data\n# position.data[0:n] # Access first n elements\nposition.timestamps # (Dataset) shape (20684,); dtype float64\n# position.timestamps[:] # Access all data\n# position.timestamps[0:n] # Access first n elements\nposition.timestamps_unit # (str) seconds\nposition.interval # (int) 1\nreward_zone = time_series[\"reward_zone\"]\nreward_zone # (TimeSeries)\nreward_zone.resolution # (float64) -1.0\nreward_zone.comments # (str) no comments\nreward_zone.description # (str) reward zone entry (binary)\nreward_zone.conversion # (float64) 1.0\nreward_zone.offset # (float64) 0.0\nreward_zone.unit # (str) integer\nreward_zone.data # (Dataset) shape (20684,); dtype float64\n# reward_zone.data[:] # Access all data\n# reward_zone.data[0:n] # Access first n elements\nreward_zone.timestamps # (Dataset) shape (20684,); dtype float64\n# reward_zone.timestamps[:] # Access all data\n# reward_zone.timestamps[0:n] # Access first n elements\nreward_zone.timestamps_unit # (str) seconds\nreward_zone.interval # (int) 1\nscanning = time_series[\"scanning\"]\nscanning # (TimeSeries)\nscanning.resolution # (float64) -1.0\nscanning.comments # (str) no comments\nscanning.description # (str) whether scanning occurred to collect ophys data\nscanning.conversion # (float64) 1.0\nscanning.offset # (float64) 0.0\nscanning.unit # (str) integer\nscanning.data # (Dataset) shape (20684,); dtype float64\n# scanning.data[:] # Access all data\n# scanning.data[0:n] # Access first n elements\nscanning.timestamps # (Dataset) shape (20684,); dtype float64\n# scanning.timestamps[:] # Access all data\n# scanning.timestamps[0:n] # Access first n elements\nscanning.timestamps_unit # (str) seconds\nscanning.interval # (int) 1\nspeed = time_series[\"speed\"]\nspeed # (TimeSeries)\nspeed.resolution # (float64) -1.0\nspeed.comments # (str) no comments\nspeed.description # (str) the speed of the subject measured over time\nspeed.conversion # (float64) 1.0\nspeed.offset # (float64) 0.0\nspeed.unit # (str) cm/s\nspeed.data # (Dataset) shape (20684,); dtype float64\n# speed.data[:] # Access all data\n# speed.data[0:n] # Access first n elements\nspeed.timestamps # (Dataset) shape (20684,); dtype float64\n# speed.timestamps[:] # Access all data\n# speed.timestamps[0:n] # Access first n elements\nspeed.timestamps_unit # (str) seconds\nspeed.interval # (int) 1\n# ...\n# Other fields: teleport, trial number, trial_start\nophys = processing[\"ophys\"]\nophys # (ProcessingModule)\nophys.description # (str) optical physiology processed data\nophys.data_interfaces # (LabelledDict)\ndata_interfaces = ophys.data_interfaces\nBackgrounds_0 = data_interfaces[\"Backgrounds_0\"]\nBackgrounds_0 # (Images)\nBackgrounds_0.description # (str) no description\nBackgrounds_0.images # (LabelledDict)\nimages = Backgrounds_0.images\nVcorr = images[\"Vcorr\"]\nVcorr # (GrayscaleImage)\nmax_proj = images[\"max_proj\"]\nmax_proj # (GrayscaleImage)\nmeanImg = images[\"meanImg\"]\nmeanImg # (GrayscaleImage)\nDeconvolved = data_interfaces[\"Deconvolved\"]\nDeconvolved # (Fluorescence)\nDeconvolved.roi_response_series # (LabelledDict)\nroi_response_series = Deconvolved.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nFluorescence = data_interfaces[\"Fluorescence\"]\nFluorescence # (Fluorescence)\nFluorescence.roi_response_series # (LabelledDict)\nroi_response_series = Fluorescence.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nImageSegmentation = data_interfaces[\"ImageSegmentation\"]\nImageSegmentation # (ImageSegmentation)\nImageSegmentation.plane_segmentations # (LabelledDict)\nplane_segmentations = ImageSegmentation.plane_segmentations\nPlaneSegmentation = plane_segmentations[\"PlaneSegmentation\"]\nPlaneSegmentation # (PlaneSegmentation)\nPlaneSegmentation.description # (str) suite2p output\nPlaneSegmentation.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nPlaneSegmentation.columns # (tuple)\nPlaneSegmentation.reference_images # (list)\nPlaneSegmentation.id # (ElementIdentifiers)\nPlaneSegmentation.imaging_plane # (ImagingPlane)\nPlaneSegmentation.imaging_plane.optical_channel # (list)\nPlaneSegmentation.imaging_plane.description # (str) standard\nPlaneSegmentation.imaging_plane.excitation_lambda # (float64) 920.0\nPlaneSegmentation.imaging_plane.imaging_rate # (float64) 15.5078125\nPlaneSegmentation.imaging_plane.indicator # (str) GCaMP7f\nPlaneSegmentation.imaging_plane.location # (str) hippocampus, CA1\nPlaneSegmentation.imaging_plane.conversion # (float) 1.0\nPlaneSegmentation.imaging_plane.unit # (str) meters\nPlaneSegmentation.imaging_plane.origin_coords_unit # (str) meters\nPlaneSegmentation.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# PlaneSegmentation.imaging_plane.grid_spacing[:] # Access all data\n# PlaneSegmentation.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of PlaneSegmentation.imaging_plane.grid_spacing: [2. 2.]\nPlaneSegmentation.imaging_plane.grid_spacing_unit # (str) microns\nPlaneSegmentation.imaging_plane.device # (Device)\nPlaneSegmentation.imaging_plane.device.description # (str) My two-photon microscope\nPlaneSegmentation.imaging_plane.device.manufacturer # (str) Neurolabware\n# PlaneSegmentation.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# PlaneSegmentation.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nPlaneSegmentation.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# PlaneSegmentation.pixel_mask_index[0] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[1] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[2] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[3] # (ndarray)\n# ...\nPlaneSegmentation.iscell # (VectorData) two columns - iscell & probcell\nPlaneSegmentation.planeIdx # (VectorData) rec plane for each roi\nNeuropil = data_interfaces[\"Neuropil\"]\nNeuropil # (Fluorescence)\nNeuropil.roi_response_series # (LabelledDict)\nroi_response_series = Neuropil.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nnwb.devices # (LabelledDict)\ndevices = nwb.devices\nMicroscope = devices[\"Microscope\"]\nMicroscope # (Device)\nMicroscope.description # (str) My two-photon microscope\nMicroscope.manufacturer # (str) Neurolabware\nnwb.imaging_planes # (LabelledDict)\nimaging_planes = nwb.imaging_planes\nImagingPlane = imaging_planes[\"ImagingPlane\"]\nImagingPlane # (ImagingPlane)\nImagingPlane.optical_channel # (list)\nImagingPlane.description # (str) standard\nImagingPlane.excitation_lambda # (float64) 920.0\nImagingPlane.imaging_rate # (float64) 15.5078125\nImagingPlane.indicator # (str) GCaMP7f\nImagingPlane.location # (str) hippocampus, CA1\nImagingPlane.conversion # (float) 1.0\nImagingPlane.unit # (str) meters\nImagingPlane.origin_coords_unit # (str) meters\nImagingPlane.grid_spacing # (Dataset) shape (2,); dtype float64\n# ImagingPlane.grid_spacing[:] # Access all data\n# ImagingPlane.grid_spacing[0:n] # Access first n elements\n# First few values of ImagingPlane.grid_spacing: [2. 2.]\nImagingPlane.grid_spacing_unit # (str) microns\nImagingPlane.device # (Device)\nImagingPlane.device.description # (str) My two-photon microscope\nImagingPlane.device.manufacturer # (str) Neurolabware\nnwb.session_id # (str) 05\nnwb.subject # (Subject)\nnwb.subject.age__reference # (str) birth\nnwb.subject.sex # (str) M\nnwb.subject.species # (str) Mus musculus\nnwb.subject.subject_id # (str) m11\nnwb.subject.date_of_birth # (datetime) 2022-09-20T00:00:00-07:00"
      }
    ]
  },
  {
    "role": "user",
    "content": "Please summarize the above information in a jupytext notebook that introduces dandiset 001361 using the format described below.\n\nThe purpose of the notebook is to help researchers explore the dandiset and get started with a reanalysis.\n\nStart with an appropriate title for the notebook, such as \"Exploring Dandiset 001361: ...\" (that should be a markdown heading).\n\nInform the user that the notebook was generated with the assistance of AI, and that they should be cautious when interpreting the code or results.\n\nProvide an overview of the Dandiset. Include a link to the Dandiset of the form https://dandiarchive.org/dandiset/001361/0.250406.0045.\n\nSummarize what the notebook covers.\n\nList the packages that are required to run the notebook. Assume that these are already installed on the user's system. Do not include any pip install commands in the notebook.\n\nShow how to use the DANDI API to load assets.\n\nShow how to load and visualize data from the dandiset based on the above chat.\n\nGenerate all the good quality plots (without being redundant).\n\nLoad NWB data by streaming the remote file (as done in the chat) rather than downloading it.\n\nDo not use any functionality of pynwb or dandi that is not covered in the chat.\n\nYou should stick to material that is covered in the chat and do not hallucinate.\n\nThroughout the notebook, include explanatory markdown cells that guide the user through the process.\n\nThe notebook should be well-documented, and follow best practices Include comments in code cells to explain what each step does.\n\nThe Jupytext should use `# %% [markdown]` for markdown cells and `# %%` delimiters for the code cells.\n\nYour output should be in the format:\n\n<notebook>\nThe text of the jupytext notebook should appear here.\n</notebook>\n\nNo other text should be included in the response.\n"
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 001361: A flexible hippocampal population code for experience relative to reward\n#\n# *This notebook was generated with the assistance of AI. Please verify all code and results before relying on them for research.*\n#\n# **Dandiset DOI:** [10.48324/dandi.001361/0.250406.0045](https://dandiarchive.org/dandiset/001361/0.250406.0045)\n#\n# ## Overview\n#\n# Dandiset 001361 contains two-photon imaging data and behavioral measurements from hippocampal CA1 in mice navigating virtual reality environments with changing hidden reward zones. The data, part of the study:\n#\n# > Sosa, Marielena; Plitt, Mark H.; Giocomo, Lisa M. (2025) A flexible hippocampal population code for experience relative to reward (Version 0.250406.0045) [Data set]. DANDI Archive.\n#\n# includes neural activity, behavioral events (e.g., licking, reward), and detailed processed outputs (Suite2p) for reanalysis.\n#\n# ## What This Notebook Covers\n#\n# - An introduction to the dataset and relevant metadata.\n# - How to access and stream NWB files directly from the DANDI archive using the Dandi API.\n# - Loading and basic inspection of imaging and behavioral data from a representative session.\n# - Example code to visualize behavior (virtual position, lick, and reward events) and neural activity (Suite2p ROIs and fluorescence traces).\n#\n# ---\n#\n# ## Required Packages\n#\n# This notebook assumes the following packages are available:\n# - dandi\n# - remfile\n# - h5py\n# - pynwb\n# - numpy\n# - pandas\n# - matplotlib\n#\n# (No installation commands are provided; please ensure the above packages are installed.)\n#\n# ---\n\n# %% [markdown]\n# ## 1. Access Dandiset 001361 Programmatically\n#\n# Here we access the DANDI Archive via the DandiAPIClient and obtain the download URL for a specific NWB file (`sub-m11_ses-05_behavior+ophys.nwb`). All reading is done via streaming (no file downloads necessary).\n\n# %%\n# Import required packages\nfrom dandi.dandiapi import DandiAPIClient\nimport remfile\nimport h5py\nimport pynwb\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# %% [markdown]\n# ### Connect to the Dandi Archive and Select a NWB File\n\n# %%\n# Initialize DANDI API client and specify dandiset/version\ndandiset_id = \"001361\"\nversion = \"0.250406.0045\"\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(dandiset_id, version)\n\n# Select an example asset (NWB file) for analysis\nnwb_asset = next(dandiset.get_assets_by_glob(\"sub-m11/sub-m11_ses-05_behavior+ophys.nwb\"))\nnwb_url = nwb_asset.download_url\nprint(\"Streaming NWB file from URL:\")\nprint(nwb_url)\n\n# %% [markdown]\n# ## 2. Stream and Read the NWB File\n#\n# We stream the file from the archive, then use `h5py` and `pynwb` to open and inspect the data.\n\n# %%\n# Stream the NWB file using remfile, open with h5py, and use pynwb to read\nremote_file = remfile.File(nwb_url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\n# %% [markdown]\n# ## 3. NWB Session Metadata\n\n# %%\n# Display session and subject metadata\nprint(\"Session description:\", nwb.session_description)\nprint(\"Session start time:\", nwb.session_start_time)\nprint(\"Subject ID:\", nwb.subject.subject_id)\nprint(\"Subject sex:\", nwb.subject.sex)\nprint(\"Subject age reference:\", getattr(nwb.subject, \"age__reference\", None))\nprint(\"Subject species:\", nwb.subject.species)\nprint(\"Date of birth:\", nwb.subject.date_of_birth)\nprint(\"Experimenter(s):\", nwb.experimenter)\n\n# %% [markdown]\n# ## 4. Behavioral Data: Overview and Visualization\n#\n# The behavioral data includes timestamped position in virtual space, speed, lick events, reward delivery, and more. We'll extract and visualize:\n#\n# - Mouse position on the virtual track\n# - Lick events (cumulative licks per imaging frame)\n# - Reward events (times and values)\n\n# %%\n# Access behavioral time series data\nbehavior = nwb.processing[\"behavior\"].data_interfaces[\"BehavioralTimeSeries\"].time_series\n\n# Extract position data\nposition = behavior[\"position\"]\nposition_data = position.data[:]\nposition_time = position.timestamps[:]\n\n# Extract lick data\nlick = behavior[\"lick\"]\nlick_data = lick.data[:]\nlick_time = lick.timestamps[:]\n\n# Extract reward data\nreward = behavior[\"Reward\"]\nreward_data = reward.data[:]\nreward_time = reward.timestamps[:]\n\n# Extract reward zone entry binary\nreward_zone = behavior[\"reward_zone\"]\nreward_zone_data = reward_zone.data[:]\nreward_zone_time = reward_zone.timestamps[:]\n\n# %% [markdown]\n# ### Plot: Mouse Position and Behavioral Events\n\n# %%\nplt.figure(figsize=(15, 5))\n\nplt.plot(position_time, position_data, label='Virtual Position (cm)', color='C0', alpha=0.7)\nplt.xlabel('Time (s)')\nplt.ylabel('Track Position (cm)')\nplt.title('Mouse Position on Virtual Track (Session 05)')\nplt.twinx()\n\n# Plot cumulative lick count (scaled for visual reference)\nplt.plot(lick_time, lick_data, color='C2', label='Cumulative Licks', alpha=0.3)\n\n# Overlay reward delivery times\nfor rt in reward_time:\n    plt.axvline(rt, color='red', linestyle='--', alpha=0.4, label='Reward' if rt == reward_time[0] else \"\")\n\nplt.legend(['Position', 'Licks', 'Reward'], loc='upper right')\nplt.show()\n\n# %% [markdown]\n# ## 5. Imaging Data: Suite2p ROI Table and Fluorescence Traces\n#\n# The processed imaging data contains fluorescence (raw, neuropil, deconvolved), pixel masks for ROIs, and suite2p ROI annotations: all available under the `'ophys'` and `'ImageSegmentation'` processing modules.\n#\n# Here, we extract the Suite2p plane segmentation table and plot traces for a few ROIs.\n\n# %%\n# Access Suite2p plane segmentation table\n# All plane0 ROI tables are equivalent (Fluorescence, Deconvolved, etc)\nplane_segmentation = nwb.processing['ophys'].data_interfaces['Fluorescence'].roi_response_series['plane0'].rois.table\nroi_df = plane_segmentation.to_dataframe()   # n_rois x columns\nprint(\"Suite2p ROI Table shape:\", roi_df.shape)\nroi_df.head()\n\n# %% [markdown]\n# ### Plot: Raw Fluorescence Traces for Example ROIs\n\n# %%\n# Access raw fluorescence data: [n_frames, n_rois]\nfluo = nwb.processing['ophys'].data_interfaces['Fluorescence'].roi_response_series['plane0'].data[:]\nsample_roi_indices = np.arange(5)  # Plot first 5 ROIs for illustration\n\nplt.figure(figsize=(15, 7))\nfor idx in sample_roi_indices:\n    plt.plot(np.arange(fluo.shape[0]) / 15.5078125, fluo[:, idx] + idx * np.nanmax(fluo[:, idx]), label=f'ROI {idx}')\n\nplt.xlabel('Time (s)')\nplt.ylabel('Raw Fluorescence (offset for clarity)')\nplt.title('Raw Fluorescence Traces (First 5 ROIs)')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## 6. Imaging Plane and Device Metadata\n\n# %%\n# Show imaging plane and device metadata\niplane = nwb.imaging_planes[\"ImagingPlane\"]\nprint(\"Imaging rate:\", iplane.imaging_rate)\nprint(\"Excitation lambda:\", iplane.excitation_lambda)\nprint(\"Indicator:\", iplane.indicator)\nprint(\"Location:\", iplane.location)\nprint(\"Device:\", iplane.device.description)\nprint(\"Manufacturer:\", iplane.device.manufacturer)\nprint(\"Grid spacing (microns):\", iplane.grid_spacing[:], iplane.grid_spacing_unit)\n\n# %% [markdown]\n# ## 7. Summary and Next Steps\n#\n# - **Data access**: Shown how to connect to the DANDI archive and stream NWB data using the DandiClient, remfile, h5py, and pynwb.\n# - **Inspection**: Summarized session metadata, subject details, behavioral variables, and imaging (Suite2p ROI and traces).\n# - **Visualization**: Example plots for position, licks, rewards, and neural activity.\n#\n# ### Next Steps for Analysis\n# - Identification of reward-related and place cell activity using the fluorescence and behavioral event data\n# - More advanced analysis can be developed using the linked [custom analysis code on GitHub](https://github.com/GiocomoLab/Sosa_et_al_2024)\n#\n# ---\n#\n# *For further exploration and reanalysis, see the full [Dandiset](https://dandiarchive.org/dandiset/001361/0.250406.0045) and original publication (Nature Neuroscience, Sosa, Plitt, & Giocomo, 2025).*\n#\n# *Reminder: This notebook was AI-generated and should be reviewed before use in research or publication.*\n</notebook>\n"
  }
]