[
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata about Dandiset 001361 version 0.250406.0045."
      },
      {
        "type": "text",
        "text": "{\n  \"id\": \"DANDI:001361/0.250406.0045\",\n  \"doi\": \"10.48324/dandi.001361/0.250406.0045\",\n  \"url\": \"https://dandiarchive.org/dandiset/001361/0.250406.0045\",\n  \"name\": \"A flexible hippocampal population code for experience relative to reward\",\n  \"about\": [\n    {\n      \"name\": \"hippocampus\",\n      \"schemaKey\": \"Anatomy\"\n    }\n  ],\n  \"access\": [\n    {\n      \"status\": \"dandi:OpenAccess\",\n      \"schemaKey\": \"AccessRequirements\"\n    }\n  ],\n  \"license\": [\n    \"spdx:CC-BY-4.0\"\n  ],\n  \"version\": \"0.250406.0045\",\n  \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.9/context.json\",\n  \"citation\": \"Sosa, Marielena; Plitt, Mark H.; Giocomo, Lisa M. (2025) A flexible hippocampal population code for experience relative to reward (Version 0.250406.0045) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.001361/0.250406.0045\",\n  \"keywords\": [\n    \"hippocampus, navigation, learning, memory, 2-photon imaging, place cells\"\n  ],\n  \"protocol\": [],\n  \"schemaKey\": \"Dandiset\",\n  \"identifier\": \"DANDI:001361\",\n  \"repository\": \"https://dandiarchive.org\",\n  \"contributor\": [\n    {\n      \"name\": \"National Institutes of Health (NIH)\",\n      \"roleName\": [\n        \"dcite:Funder\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"awardNumber\": \"1 K99 MH 135993-01\"\n    },\n    {\n      \"name\": \"Sosa, Marielena\",\n      \"roleName\": [\n        \"dcite:Author\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-0762-1128\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Plitt, Mark H.\",\n      \"roleName\": [\n        \"dcite:Author\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-7154-6292\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Giocomo, Lisa M.\",\n      \"email\": \"giocomo@stanford.edu\",\n      \"roleName\": [\n        \"dcite:Author\",\n        \"dcite:ContactPerson\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-0416-2528\",\n      \"includeInCitation\": true\n    }\n  ],\n  \"dateCreated\": \"2025-03-13T06:23:11.234153+00:00\",\n  \"description\": \"2-photon imaging and behavioral data from hippocampal area CA1 during virtual reality navigation in mice. Included in Sosa, Plitt, & Giocomo, \\\"A flexible hippocampal population code for experience relative to reward,\\\" Nature Neuroscience.\\n\\nTo reinforce rewarding behaviors, events leading up to and following rewards must be remembered. Hippocampal place cell activity spans spatial and non-spatial episodes, but whether hippocampal activity encodes entire sequences of events relative to reward is unknown. To test this, we performed two-photon imaging of hippocampal CA1 as mice navigated virtual environments with changing hidden reward locations. When the reward moved, a subpopulation of neurons updated their firing fields to the same relative position with respect to reward, constructing behavioral timescale sequences spanning the entire task. Over learning, this reward-relative representation became more robust as additional neurons were recruited, and changes in reward-relative firing often preceded behavioral adaptations following reward relocation. Concurrently, the spatial environment code was maintained through a parallel, dynamic subpopulation rather than through dedicated cell classes. These findings reveal how hippocampal ensembles flexibly encode multiple aspects of experience while amplifying behaviorally relevant information.\",\n  \"publishedBy\": {\n    \"id\": \"urn:uuid:1a375579-42d8-4ca6-ae14-2162860f5fc0\",\n    \"name\": \"DANDI publish\",\n    \"endDate\": \"2025-04-06T00:45:27.761521+00:00\",\n    \"schemaKey\": \"PublishActivity\",\n    \"startDate\": \"2025-04-06T00:45:27.761521+00:00\",\n    \"wasAssociatedWith\": [\n      {\n        \"id\": \"urn:uuid:3ea3f3df-9c3f-43b6-b04a-964aa33da62b\",\n        \"name\": \"DANDI API\",\n        \"version\": \"0.1.0\",\n        \"schemaKey\": \"Software\",\n        \"identifier\": \"RRID:SCR_017571\"\n      }\n    ]\n  },\n  \"studyTarget\": [],\n  \"assetsSummary\": {\n    \"species\": [\n      {\n        \"name\": \"Mus musculus - House mouse\",\n        \"schemaKey\": \"SpeciesType\",\n        \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n      }\n    ],\n    \"approach\": [\n      {\n        \"name\": \"microscopy approach; cell population imaging\",\n        \"schemaKey\": \"ApproachType\"\n      },\n      {\n        \"name\": \"behavioral approach\",\n        \"schemaKey\": \"ApproachType\"\n      }\n    ],\n    \"schemaKey\": \"AssetsSummary\",\n    \"dataStandard\": [\n      {\n        \"name\": \"Neurodata Without Borders (NWB)\",\n        \"schemaKey\": \"StandardsType\",\n        \"identifier\": \"RRID:SCR_015242\"\n      }\n    ],\n    \"numberOfBytes\": 92560251500,\n    \"numberOfFiles\": 152,\n    \"numberOfSubjects\": 11,\n    \"variableMeasured\": [\n      \"TwoPhotonSeries\",\n      \"PlaneSegmentation\",\n      \"BehavioralTimeSeries\",\n      \"ProcessingModule\",\n      \"ImagingPlane\",\n      \"OpticalChannel\"\n    ],\n    \"measurementTechnique\": [\n      {\n        \"name\": \"two-photon microscopy technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"analytical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"behavioral technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"surgical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      }\n    ]\n  },\n  \"datePublished\": \"2025-04-06T00:45:27.761521+00:00\",\n  \"schemaVersion\": \"0.6.9\",\n  \"ethicsApproval\": [],\n  \"wasGeneratedBy\": [\n    {\n      \"name\": \"Neural circuit dynamics of spatial reward memory\",\n      \"endDate\": \"2026-04-30T23:59:00-07:00\",\n      \"schemaKey\": \"Project\",\n      \"startDate\": \"2024-05-03T00:00:00-07:00\",\n      \"identifier\": \"K99MH135993\",\n      \"description\": \"BRAIN Initiative K99\"\n    }\n  ],\n  \"relatedResource\": [\n    {\n      \"url\": \"https://github.com/GiocomoLab/Sosa_et_al_2024\",\n      \"name\": \"Custom analysis code\",\n      \"relation\": \"dcite:IsSupplementedBy\",\n      \"schemaKey\": \"Resource\",\n      \"repository\": \"Github\",\n      \"resourceType\": \"dcite:Software\"\n    }\n  ],\n  \"manifestLocation\": [\n    \"https://dandiarchive.s3.amazonaws.com/dandisets/001361/0.250406.0045/assets.yaml\"\n  ]\n}"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is a listing of some of the files in the Dandiset."
      },
      {
        "type": "text",
        "text": "sub-m11/sub-m11_ses-05_behavior+ophys.nwb\nsub-m11/sub-m11_ses-04_behavior+ophys.nwb\nsub-m11/sub-m11_ses-07_behavior+ophys.nwb\nsub-m11/sub-m11_ses-06_behavior+ophys.nwb\nsub-m11/sub-m11_ses-08_behavior+ophys.nwb\nsub-m11/sub-m11_ses-10_behavior+ophys.nwb\nsub-m11/sub-m11_ses-03_behavior+ophys.nwb\nsub-m11/sub-m11_ses-09_behavior+ophys.nwb\nsub-m11/sub-m11_ses-11_behavior+ophys.nwb\nsub-m11/sub-m11_ses-12_behavior+ophys.nwb"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata and usage information about the file sub-m11/sub-m11_ses-05_behavior+ophys.nwb."
      },
      {
        "type": "text",
        "text": "\nimport pynwb\nimport h5py\nimport remfile\n\n# Load\nfrom dandi.dandiapi import DandiAPIClient\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"001361\", \"0.250406.0045\")\nurl = next(dandiset.get_assets_by_glob(\"sub-m11/sub-m11_ses-05_behavior+ophys.nwb\")).download_url\n\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\nnwb # (NWBFile)\nnwb.session_description # (str) processed suite2p data\nnwb.identifier # (str) /data/InVivoDA/GCAMP11/25_02_2023/Env1_LocationA_to_C\nnwb.session_start_time # (datetime) 2023-02-25T00:00:00-08:00\nnwb.timestamps_reference_time # (datetime) 2023-02-25T00:00:00-08:00\nnwb.file_create_date # (list) [datetime.datetime(2025, 3, 12, 23, 45, 33, 486687, tzinfo=tzoffset(None, -25200))]\nnwb.experimenter # (tuple) ['Mari Sosa']\nnwb.acquisition # (LabelledDict)\nacquisition = nwb.acquisition\nTwoPhotonSeries = acquisition[\"TwoPhotonSeries\"]\nTwoPhotonSeries # (TwoPhotonSeries)\nTwoPhotonSeries.starting_time # (float64) 0.0\nTwoPhotonSeries.rate # (float64) 15.5078125\nTwoPhotonSeries.resolution # (float64) -1.0\nTwoPhotonSeries.comments # (str) no comments\nTwoPhotonSeries.description # (str) no description\nTwoPhotonSeries.conversion # (float64) 1.0\nTwoPhotonSeries.offset # (float64) 0.0\nTwoPhotonSeries.unit # (str) volt\nTwoPhotonSeries.data # (Dataset) shape (1, 1, 1); dtype float64\n# TwoPhotonSeries.data[:, :, :] # Access all data\n# TwoPhotonSeries.data[0, :, :] # Access first plane\nTwoPhotonSeries.starting_time_unit # (str) seconds\nTwoPhotonSeries.dimension # (Dataset) shape (2,); dtype int64\n# TwoPhotonSeries.dimension[:] # Access all data\n# TwoPhotonSeries.dimension[0:n] # Access first n elements\n# First few values of TwoPhotonSeries.dimension: [512 796]\nTwoPhotonSeries.format # (str) raw\nTwoPhotonSeries.imaging_plane # (ImagingPlane)\nTwoPhotonSeries.imaging_plane.optical_channel # (list)\nTwoPhotonSeries.imaging_plane.description # (str) standard\nTwoPhotonSeries.imaging_plane.excitation_lambda # (float64) 920.0\nTwoPhotonSeries.imaging_plane.imaging_rate # (float64) 15.5078125\nTwoPhotonSeries.imaging_plane.indicator # (str) GCaMP7f\nTwoPhotonSeries.imaging_plane.location # (str) hippocampus, CA1\nTwoPhotonSeries.imaging_plane.conversion # (float) 1.0\nTwoPhotonSeries.imaging_plane.unit # (str) meters\nTwoPhotonSeries.imaging_plane.origin_coords_unit # (str) meters\nTwoPhotonSeries.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# TwoPhotonSeries.imaging_plane.grid_spacing[:] # Access all data\n# TwoPhotonSeries.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of TwoPhotonSeries.imaging_plane.grid_spacing: [2. 2.]\nTwoPhotonSeries.imaging_plane.grid_spacing_unit # (str) microns\nTwoPhotonSeries.imaging_plane.device # (Device)\nTwoPhotonSeries.imaging_plane.device.description # (str) My two-photon microscope\nTwoPhotonSeries.imaging_plane.device.manufacturer # (str) Neurolabware\nnwb.processing # (LabelledDict)\nprocessing = nwb.processing\nbehavior = processing[\"behavior\"]\nbehavior # (ProcessingModule)\nbehavior.description # (str) behavior data\nbehavior.data_interfaces # (LabelledDict)\ndata_interfaces = behavior.data_interfaces\nBehavioralTimeSeries = data_interfaces[\"BehavioralTimeSeries\"]\nBehavioralTimeSeries # (BehavioralTimeSeries)\nBehavioralTimeSeries.time_series # (LabelledDict)\ntime_series = BehavioralTimeSeries.time_series\nReward = time_series[\"Reward\"]\nReward # (TimeSeries)\nReward.resolution # (float64) -1.0\nReward.comments # (str) no comments\nReward.description # (str) reward delivery\nReward.conversion # (float64) 1.0\nReward.offset # (float64) 0.0\nReward.unit # (str) mL\nReward.data # (Dataset) shape (69,); dtype float64\n# Reward.data[:] # Access all data\n# Reward.data[0:n] # Access first n elements\nReward.timestamps # (Dataset) shape (69,); dtype float64\n# Reward.timestamps[:] # Access all data\n# Reward.timestamps[0:n] # Access first n elements\nReward.timestamps_unit # (str) seconds\nReward.interval # (int) 1\nautoreward = time_series[\"autoreward\"]\nautoreward # (TimeSeries)\nautoreward.resolution # (float64) -1.0\nautoreward.comments # (str) no comments\nautoreward.description # (str) whether trial was automatically rewarded if the subject failed to lick\nautoreward.conversion # (float64) 1.0\nautoreward.offset # (float64) 0.0\nautoreward.unit # (str) integer\nautoreward.data # (Dataset) shape (20684,); dtype float64\n# autoreward.data[:] # Access all data\n# autoreward.data[0:n] # Access first n elements\nautoreward.timestamps # (Dataset) shape (20684,); dtype float64\n# autoreward.timestamps[:] # Access all data\n# autoreward.timestamps[0:n] # Access first n elements\nautoreward.timestamps_unit # (str) seconds\nautoreward.interval # (int) 1\nenvironment = time_series[\"environment\"]\nenvironment # (TimeSeries)\nenvironment.resolution # (float64) -1.0\nenvironment.comments # (str) no comments\nenvironment.description # (str) Virtual reality environment\nenvironment.conversion # (float64) 1.0\nenvironment.offset # (float64) 0.0\nenvironment.unit # (str) AU\nenvironment.data # (Dataset) shape (20684,); dtype float64\n# environment.data[:] # Access all data\n# environment.data[0:n] # Access first n elements\nenvironment.timestamps # (Dataset) shape (20684,); dtype float64\n# environment.timestamps[:] # Access all data\n# environment.timestamps[0:n] # Access first n elements\nenvironment.timestamps_unit # (str) seconds\nenvironment.interval # (int) 1\nlick = time_series[\"lick\"]\nlick # (TimeSeries)\nlick.resolution # (float64) -1.0\nlick.comments # (str) no comments\nlick.description # (str) lick detection by capacitive sensor, cumulative per imaging frame\nlick.conversion # (float64) 1.0\nlick.offset # (float64) 0.0\nlick.unit # (str) AU\nlick.data # (Dataset) shape (20684,); dtype float64\n# lick.data[:] # Access all data\n# lick.data[0:n] # Access first n elements\nlick.timestamps # (Dataset) shape (20684,); dtype float64\n# lick.timestamps[:] # Access all data\n# lick.timestamps[0:n] # Access first n elements\nlick.timestamps_unit # (str) seconds\nlick.interval # (int) 1\nposition = time_series[\"position\"]\nposition # (TimeSeries)\nposition.resolution # (float64) -1.0\nposition.comments # (str) no comments\nposition.description # (str) Position in a virtual linear track\nposition.conversion # (float64) 1.0\nposition.offset # (float64) 0.0\nposition.unit # (str) cm\nposition.data # (Dataset) shape (20684,); dtype float64\n# position.data[:] # Access all data\n# position.data[0:n] # Access first n elements\nposition.timestamps # (Dataset) shape (20684,); dtype float64\n# position.timestamps[:] # Access all data\n# position.timestamps[0:n] # Access first n elements\nposition.timestamps_unit # (str) seconds\nposition.interval # (int) 1\nreward_zone = time_series[\"reward_zone\"]\nreward_zone # (TimeSeries)\nreward_zone.resolution # (float64) -1.0\nreward_zone.comments # (str) no comments\nreward_zone.description # (str) reward zone entry (binary)\nreward_zone.conversion # (float64) 1.0\nreward_zone.offset # (float64) 0.0\nreward_zone.unit # (str) integer\nreward_zone.data # (Dataset) shape (20684,); dtype float64\n# reward_zone.data[:] # Access all data\n# reward_zone.data[0:n] # Access first n elements\nreward_zone.timestamps # (Dataset) shape (20684,); dtype float64\n# reward_zone.timestamps[:] # Access all data\n# reward_zone.timestamps[0:n] # Access first n elements\nreward_zone.timestamps_unit # (str) seconds\nreward_zone.interval # (int) 1\nscanning = time_series[\"scanning\"]\nscanning # (TimeSeries)\nscanning.resolution # (float64) -1.0\nscanning.comments # (str) no comments\nscanning.description # (str) whether scanning occurred to collect ophys data\nscanning.conversion # (float64) 1.0\nscanning.offset # (float64) 0.0\nscanning.unit # (str) integer\nscanning.data # (Dataset) shape (20684,); dtype float64\n# scanning.data[:] # Access all data\n# scanning.data[0:n] # Access first n elements\nscanning.timestamps # (Dataset) shape (20684,); dtype float64\n# scanning.timestamps[:] # Access all data\n# scanning.timestamps[0:n] # Access first n elements\nscanning.timestamps_unit # (str) seconds\nscanning.interval # (int) 1\nspeed = time_series[\"speed\"]\nspeed # (TimeSeries)\nspeed.resolution # (float64) -1.0\nspeed.comments # (str) no comments\nspeed.description # (str) the speed of the subject measured over time\nspeed.conversion # (float64) 1.0\nspeed.offset # (float64) 0.0\nspeed.unit # (str) cm/s\nspeed.data # (Dataset) shape (20684,); dtype float64\n# speed.data[:] # Access all data\n# speed.data[0:n] # Access first n elements\nspeed.timestamps # (Dataset) shape (20684,); dtype float64\n# speed.timestamps[:] # Access all data\n# speed.timestamps[0:n] # Access first n elements\nspeed.timestamps_unit # (str) seconds\nspeed.interval # (int) 1\n# ...\n# Other fields: teleport, trial number, trial_start\nophys = processing[\"ophys\"]\nophys # (ProcessingModule)\nophys.description # (str) optical physiology processed data\nophys.data_interfaces # (LabelledDict)\ndata_interfaces = ophys.data_interfaces\nBackgrounds_0 = data_interfaces[\"Backgrounds_0\"]\nBackgrounds_0 # (Images)\nBackgrounds_0.description # (str) no description\nBackgrounds_0.images # (LabelledDict)\nimages = Backgrounds_0.images\nVcorr = images[\"Vcorr\"]\nVcorr # (GrayscaleImage)\nmax_proj = images[\"max_proj\"]\nmax_proj # (GrayscaleImage)\nmeanImg = images[\"meanImg\"]\nmeanImg # (GrayscaleImage)\nDeconvolved = data_interfaces[\"Deconvolved\"]\nDeconvolved # (Fluorescence)\nDeconvolved.roi_response_series # (LabelledDict)\nroi_response_series = Deconvolved.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nFluorescence = data_interfaces[\"Fluorescence\"]\nFluorescence # (Fluorescence)\nFluorescence.roi_response_series # (LabelledDict)\nroi_response_series = Fluorescence.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nImageSegmentation = data_interfaces[\"ImageSegmentation\"]\nImageSegmentation # (ImageSegmentation)\nImageSegmentation.plane_segmentations # (LabelledDict)\nplane_segmentations = ImageSegmentation.plane_segmentations\nPlaneSegmentation = plane_segmentations[\"PlaneSegmentation\"]\nPlaneSegmentation # (PlaneSegmentation)\nPlaneSegmentation.description # (str) suite2p output\nPlaneSegmentation.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nPlaneSegmentation.columns # (tuple)\nPlaneSegmentation.reference_images # (list)\nPlaneSegmentation.id # (ElementIdentifiers)\nPlaneSegmentation.imaging_plane # (ImagingPlane)\nPlaneSegmentation.imaging_plane.optical_channel # (list)\nPlaneSegmentation.imaging_plane.description # (str) standard\nPlaneSegmentation.imaging_plane.excitation_lambda # (float64) 920.0\nPlaneSegmentation.imaging_plane.imaging_rate # (float64) 15.5078125\nPlaneSegmentation.imaging_plane.indicator # (str) GCaMP7f\nPlaneSegmentation.imaging_plane.location # (str) hippocampus, CA1\nPlaneSegmentation.imaging_plane.conversion # (float) 1.0\nPlaneSegmentation.imaging_plane.unit # (str) meters\nPlaneSegmentation.imaging_plane.origin_coords_unit # (str) meters\nPlaneSegmentation.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# PlaneSegmentation.imaging_plane.grid_spacing[:] # Access all data\n# PlaneSegmentation.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of PlaneSegmentation.imaging_plane.grid_spacing: [2. 2.]\nPlaneSegmentation.imaging_plane.grid_spacing_unit # (str) microns\nPlaneSegmentation.imaging_plane.device # (Device)\nPlaneSegmentation.imaging_plane.device.description # (str) My two-photon microscope\nPlaneSegmentation.imaging_plane.device.manufacturer # (str) Neurolabware\n# PlaneSegmentation.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# PlaneSegmentation.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nPlaneSegmentation.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# PlaneSegmentation.pixel_mask_index[0] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[1] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[2] # (ndarray)\n# PlaneSegmentation.pixel_mask_index[3] # (ndarray)\n# ...\nPlaneSegmentation.iscell # (VectorData) two columns - iscell & probcell\nPlaneSegmentation.planeIdx # (VectorData) rec plane for each roi\nNeuropil = data_interfaces[\"Neuropil\"]\nNeuropil # (Fluorescence)\nNeuropil.roi_response_series # (LabelledDict)\nroi_response_series = Neuropil.roi_response_series\nplane0 = roi_response_series[\"plane0\"]\nplane0 # (RoiResponseSeries)\nplane0.starting_time # (float64) 0.0\nplane0.rate # (float64) 15.5078125\nplane0.resolution # (float64) -1.0\nplane0.comments # (str) no comments\nplane0.description # (str) no description\nplane0.conversion # (float64) 1.0\nplane0.offset # (float64) 0.0\nplane0.unit # (str) lumens\nplane0.data # (Dataset) shape (20684, 358); dtype float32\n# plane0.data[:, :] # Access all data\n# plane0.data[0:n, :] # Access first n rows\n# plane0.data[:, 0:n] # Access first n columns\nplane0.starting_time_unit # (str) seconds\nplane0.rois # (DynamicTableRegion)\nplane0.rois.description # (str) ROIs for plane0\nplane0.rois.table # (PlaneSegmentation)\nplane0.rois.table.description # (str) suite2p output\nplane0.rois.table.colnames # (tuple) ['pixel_mask', 'iscell', 'planeIdx']\nplane0.rois.table.columns # (tuple)\nplane0.rois.table.reference_images # (list)\nplane0.rois.table.id # (ElementIdentifiers)\nplane0.rois.table.imaging_plane # (ImagingPlane)\nplane0.rois.table.imaging_plane.optical_channel # (list)\nplane0.rois.table.imaging_plane.description # (str) standard\nplane0.rois.table.imaging_plane.excitation_lambda # (float64) 920.0\nplane0.rois.table.imaging_plane.imaging_rate # (float64) 15.5078125\nplane0.rois.table.imaging_plane.indicator # (str) GCaMP7f\nplane0.rois.table.imaging_plane.location # (str) hippocampus, CA1\nplane0.rois.table.imaging_plane.conversion # (float) 1.0\nplane0.rois.table.imaging_plane.unit # (str) meters\nplane0.rois.table.imaging_plane.origin_coords_unit # (str) meters\nplane0.rois.table.imaging_plane.grid_spacing # (Dataset) shape (2,); dtype float64\n# plane0.rois.table.imaging_plane.grid_spacing[:] # Access all data\n# plane0.rois.table.imaging_plane.grid_spacing[0:n] # Access first n elements\n# First few values of plane0.rois.table.imaging_plane.grid_spacing: [2. 2.]\nplane0.rois.table.imaging_plane.grid_spacing_unit # (str) microns\nplane0.rois.table.imaging_plane.device # (Device)\nplane0.rois.table.imaging_plane.device.description # (str) My two-photon microscope\nplane0.rois.table.imaging_plane.device.manufacturer # (str) Neurolabware\n# plane0.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 358 rows and 4 columns\n# plane0.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nplane0.rois.table.pixel_mask # (VectorIndex) Index for VectorData 'pixel_mask'\n# plane0.rois.table.pixel_mask_index[0] # (ndarray)\n# plane0.rois.table.pixel_mask_index[1] # (ndarray)\n# plane0.rois.table.pixel_mask_index[2] # (ndarray)\n# plane0.rois.table.pixel_mask_index[3] # (ndarray)\n# ...\nplane0.rois.table.iscell # (VectorData) two columns - iscell & probcell\nplane0.rois.table.planeIdx # (VectorData) rec plane for each roi\nnwb.devices # (LabelledDict)\ndevices = nwb.devices\nMicroscope = devices[\"Microscope\"]\nMicroscope # (Device)\nMicroscope.description # (str) My two-photon microscope\nMicroscope.manufacturer # (str) Neurolabware\nnwb.imaging_planes # (LabelledDict)\nimaging_planes = nwb.imaging_planes\nImagingPlane = imaging_planes[\"ImagingPlane\"]\nImagingPlane # (ImagingPlane)\nImagingPlane.optical_channel # (list)\nImagingPlane.description # (str) standard\nImagingPlane.excitation_lambda # (float64) 920.0\nImagingPlane.imaging_rate # (float64) 15.5078125\nImagingPlane.indicator # (str) GCaMP7f\nImagingPlane.location # (str) hippocampus, CA1\nImagingPlane.conversion # (float) 1.0\nImagingPlane.unit # (str) meters\nImagingPlane.origin_coords_unit # (str) meters\nImagingPlane.grid_spacing # (Dataset) shape (2,); dtype float64\n# ImagingPlane.grid_spacing[:] # Access all data\n# ImagingPlane.grid_spacing[0:n] # Access first n elements\n# First few values of ImagingPlane.grid_spacing: [2. 2.]\nImagingPlane.grid_spacing_unit # (str) microns\nImagingPlane.device # (Device)\nImagingPlane.device.description # (str) My two-photon microscope\nImagingPlane.device.manufacturer # (str) Neurolabware\nnwb.session_id # (str) 05\nnwb.subject # (Subject)\nnwb.subject.age__reference # (str) birth\nnwb.subject.sex # (str) M\nnwb.subject.species # (str) Mus musculus\nnwb.subject.subject_id # (str) m11\nnwb.subject.date_of_birth # (datetime) 2022-09-20T00:00:00-07:00"
      }
    ]
  },
  {
    "role": "user",
    "content": "On the basis of the above information, please create a jupytext notebook that introduces dandiset 001361 using the format described below.\n\nThe purpose of the notebook is to help researchers explore the dandiset and get started with a reanalysis.\n\nStart with an appropriate title for the notebook, such as \"Exploring Dandiset 001361: ...\" (that should be a markdown heading).\n\nInform the user that the notebook was generated with the assistance of AI, and that they should be cautious when interpreting the code or results.\n\nProvide an overview of the Dandiset. Include a link to the Dandiset of the form https://dandiarchive.org/dandiset/001361/0.250406.0045.\n\nSummarize what the notebook covers.\n\nList the packages that are required to run the notebook. Assume that these are already installed on the user's system. Do not include any pip install commands in the notebook.\n\nShow how to use the DANDI API to load assets.\n\nShow how to load and visualize data from the dandiset based on the above information. Feel free to organize things differently from how they are in the above information, but do not make up new information.\n\nGenerate good quality plots (without being redundant).\n\nLoad NWB data by streaming the remote file (as done in the above information) rather than downloading it.\n\nDo not use any functionality of pynwb or dandi that is not covered in the above information.\n\nYou should stick to material that is covered in the above information and do not hallucinate.\n\nThroughout the notebook, include explanatory markdown cells that guide the user through the process.\n\nThe notebook should be well-documented, and follow best practices Include comments in code cells to explain what each step does.\n\nThe Jupytext should use `# %% [markdown]` for markdown cells and `# %%` delimiters for the code cells.\n\nIf any NWB have units objects, you should know the following:\nunits.spike_times_index[i] provides the vector of spike times for the i^th unit. It is actually not an index. Do not use units.spike_times.\n\nYour notebook will be evaluated based on the following criteria:\n1. How well did the notebook help you understand the purpose and content of the Dandiset?\n2. After reviewing the notebook, do you feel confident in how to access the different types of data from this Dandiset?\n3. Did the notebook help you understand the structure of the NWB file(s) and how to work with them?\n4. Did the visualizations in the notebook generally help you understand key aspects of the data?\n5. Did any of the visualizations make it harder to understand the data (e.g., due to poor formatting, unclear axes, or misleading displays)?\n6. Do you feel more confident creating your own visualizations of the data after seeing the examples in the notebook?\n7. How well did the visualizations show the structure or complexity of the data?\n8. Were there any interpretations or conclusions in the notebook that felt unclear or not well supported by the data shown?\n9. Did any of the plots or examples feel unnecessarily repetitive or redundant?\n10. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n11. How clear and easy was the notebook to follow?\n12. Did the notebook provide code you could easily reuse or adapt to explore the Dandiset yourself?\n13. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n14. Overall, how helpful was this notebook for getting started with this Dandiset?\n\nYour output should be in the format:\n\n<notebook>\nThe text of the jupytext notebook should appear here.\n</notebook>\n\nNo other text should be included in the response.\n"
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 001361: A flexible hippocampal population code for experience relative to reward\n#\n# **Notebook generated with AI assistance. Please interpret code and results with caution and consult original data and documentation as needed.**\n#\n# ---\n#\n# This notebook provides an interactive introduction to [Dandiset 001361, version 0.250406.0045](https://dandiarchive.org/dandiset/001361/0.250406.0045):  \n# **\"A flexible hippocampal population code for experience relative to reward\"** by Sosa, Plitt, & Giocomo (2025, Nature Neuroscience).  \n#\n# ---\n#\n# ## Overview\n#\n# This Dandiset contains two-photon calcium imaging and behavioral datasets from the CA1 area of the mouse hippocampus. Mice navigate virtual linear tracks with changing hidden reward locations, and their neural and behavioral responses are recorded. The data include:\n# - Raw and processed optical physiology (ophys) measurements (e.g., calcium traces)\n# - Behavioral data (licking, position, speed, rewards)\n# - ROI segmentation and metadata\n#\n# Data are provided in NWB format, compatible with the [Neurodata Without Borders](https://www.nwb.org/) ecosystem.\n#\n# ---\n#\n# ## What this notebook covers\n#\n# - How to use the DANDI API to explore Dandiset 001361\n# - How to remotely stream NWB files from the archive\n# - Navigating the structure of a representative NWB file\n# - Visualizing behavioral and calcium imaging data  \n# - Laying groundwork for further reanalysis\n#\n# ---\n#\n# ## Requirements\n#\n# This notebook requires the following packages (assumed installed):\n#\n# - pynwb\n# - h5py\n# - remfile\n# - dandi\n# - numpy\n# - pandas\n# - matplotlib\n#\n# ---\n#\n# Let's get started!\n#\n# %% [markdown]\n# ## 1. Connect to DANDI and List Available Assets\n#\n# We'll use the DANDI Python client library to access the public assets in this Dandiset.  \n# For illustration, we'll focus on NWB files corresponding to mouse `sub-m11`.\n\n# %%\nfrom dandi.dandiapi import DandiAPIClient\n\n# Connect to the DANDI Archive and access the specific Dandiset and version\ndandiset_id = \"001361\"\ndandiset_version = \"0.250406.0045\"\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(dandiset_id, dandiset_version)\n\n# List all NWB files for subject m11 in this dandiset\nnwb_assets = [\n    asset\n    for asset in dandiset.get_assets()\n    if asset.path.startswith(\"sub-m11/\") and asset.path.endswith(\".nwb\")\n]\n\nprint(f\"Found {len(nwb_assets)} NWB files for sub-m11:\")\nfor asset in nwb_assets:\n    print(f\" - {asset.path}\")\n\n# %% [markdown]\n# ## 2. Load a Sample NWB File Remotely\n#\n# We'll pick one session (`sub-m11/sub-m11_ses-05_behavior+ophys.nwb`) to demonstrate data access and visualization.\n#\n# We use `remfile` and `h5py` to stream the NWB file directly from the cloud\u2014no download required!\n\n# %%\nimport remfile\nimport h5py\nimport pynwb\n\n# Choose a session to explore\nasset_path = \"sub-m11/sub-m11_ses-05_behavior+ophys.nwb\"\nasset = next(dandiset.get_assets_by_glob(asset_path))\n\nprint(f\"Streaming remote NWB file: {asset.path}\")\n\n# Open remote NWB file for reading\nremote_file = remfile.File(asset.download_url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\nprint(f\"Loaded NWB session: {nwb.identifier}\")\n\n# %% [markdown]\n# ## 3. Inspecting the NWB File: Session and Subject Information\n\n# %%\nfrom datetime import datetime\n\nprint(\"Session description:\", nwb.session_description)\nprint(\"Session start time:\", nwb.session_start_time)\nprint(\"Session ID:\", nwb.session_id)\nprint(\"---\")\nprint(\"Subject ID:\", nwb.subject.subject_id)\nprint(\"Species:\", nwb.subject.species)\nprint(\"Sex:\", nwb.subject.sex)\nprint(\"Date of birth:\", nwb.subject.date_of_birth.date())\nif hasattr(nwb.subject, \"age__reference\"):\n    print(\"Age at reference (if provided):\", nwb.subject.age__reference)\nprint(\"---\")\nprint(\"Experimenter:\", nwb.experimenter)\n\n# %% [markdown]\n# ## 4. Overview of Acquisition and Processed Data\n#\n# The file stores:\n# - **Acquisition**: Raw two-photon imaging\n# - **Processing modules**: Behavior, ophys (processed), segmentation data\n#\n# Let's see the available acquisition and processing modules:\n\n# %%\nprint(\"Acquisition modules:\", list(nwb.acquisition.keys()))\nprint(\"Processing modules:\", list(nwb.processing.keys()))\n\n# %% [markdown]\n# ## 5. Exploring Behavioral Data: Virtual Navigation, Licking, Rewards\n#\n# All behavioral data are in the `behavior` processing module, under a `BehavioralTimeSeries` interface.  \n# Let's extract key behavioral variables: position, speed, licking, and rewards.\n\n# %%\nimport numpy as np\n\nbehavior = nwb.processing[\"behavior\"]\nbts = behavior.data_interfaces[\"BehavioralTimeSeries\"]\n\n# Grab time series of interest\nposition = bts.time_series[\"position\"]\nspeed = bts.time_series[\"speed\"]\nlick = bts.time_series[\"lick\"]\nreward_zone = bts.time_series[\"reward_zone\"]\nreward = bts.time_series[\"Reward\"]\n\nprint(\"Position data:\", position.data.shape)\nprint(\"Speed data:\", speed.data.shape)\nprint(\"Lick data:\", lick.data.shape)\nprint(\"Reward zone data:\", reward_zone.data.shape)\nprint(\"Reward times:\", reward.timestamps.shape)\n\n# %% [markdown]\n# Let's visualize the mouse's trajectory and behavior around reward delivery events.\n\n# %%\nimport matplotlib.pyplot as plt\n\n# For illustration, let's plot data from the first ~2 minutes (if available)\nfs = int(speed.data.shape[0])\nsampling_rate = speed.rate if hasattr(speed, \"rate\") else 15.5  # (Hz)\nduration_sec = fs / sampling_rate\nmax_time = min(120, duration_sec)  # Max 120 seconds\n\nmask = position.timestamps[:] < max_time\n\n# Extract data for plotting\ntime = position.timestamps[mask]\npos = position.data[mask]\nspd = speed.data[mask]\nlicks = lick.data[mask]\nreward_events = reward.timestamps[:]\nreward_zone_bin = reward_zone.data[mask] > 0.5\n\nfig, axs = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\naxs[0].plot(time, pos, label=\"Position (cm)\", color=\"C0\")\naxs[0].set_ylabel(\"Position (cm)\")\naxs[0].set_title(\"Mouse Trajectory & Behavior (first 2 min)\")\naxs[1].plot(time, spd, label=\"Speed (cm/s)\", color=\"C1\")\naxs[1].set_ylabel(\"Speed (cm/s)\")\naxs[2].plot(time, licks, label=\"Cumulative Licks\", color=\"C2\")\naxs[2].set_ylabel(\"Lick Count\")\naxs[3].plot(time, reward_zone_bin, label=\"In Reward Zone\", color=\"C3\")\naxs[3].set_ylabel(\"Reward Zone\")\naxs[3].set_xlabel(\"Time (s)\")\n\n# Mark reward times\nfor ax in axs:\n    for rt in reward_events:\n        if rt < max_time:\n            ax.axvline(rt, color=\"red\", linestyle=\"--\", linewidth=0.7, alpha=0.7)\n\naxs[0].legend(), axs[1].legend(), axs[2].legend(), axs[3].legend()\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# **Interpretation:**  \n# - Each vertical red dashed line indicates a reward delivery time.  \n# - Observe licking and speeds relative to rewards and reward zone entry.\n#\n# This view provides a quick glance at the behavioral structure of the experiment.\n\n# %% [markdown]\n# ## 6. Exploring Calcium Imaging Data (Processed Fluorescence)\n#\n# The `ophys` processing module contains processed calcium traces (Fluorescence and Deconvolved), segmentation metadata, and neuropil traces.\n#\n# Let's examine the main `Fluorescence` traces for all detected ROIs (cells) and visualize the activity of a random subset across time.\n\n# %%\nophys = nwb.processing[\"ophys\"]\nFluorescence = ophys.data_interfaces[\"Fluorescence\"]\nf_plane0 = Fluorescence.roi_response_series[\"plane0\"]\n\nflu_data = f_plane0.data         # shape (time, cells)\nflu_time = np.arange(flu_data.shape[0]) / f_plane0.rate  # seconds\n\nprint(f\"Fluorescence data shape: {flu_data.shape}  (frames x ROIs)\")\nprint(f\"Calcium imaging sampling rate: {f_plane0.rate:.2f} Hz\")\n\n# Subselect cells for plotting (up to 10 random ROIs)\nn_cells = flu_data.shape[1]\nn_show = min(10, n_cells)\nnp.random.seed(42)\nsample_cells = np.random.choice(n_cells, n_show, replace=False)\n\nplt.figure(figsize=(10, 6))\noffset = 4\nfor idx, cell in enumerate(sample_cells):\n    plt.plot(flu_time, flu_data[:, cell] + idx * offset, label=f\"Cell {cell}\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Fluorescence (offset per cell)\")\nplt.title(f\"Raw Calcium Traces from {n_show} Random ROIs\")\nplt.yticks([])\nplt.legend(ncol=2, fontsize=8)\nplt.xlim([0, min(120, flu_time[-1])])\nplt.show()\n\n# %% [markdown]\n# **Interpretation:**  \n# - Each trace is a cell's raw fluorescence, vertically offset for clarity.  \n# - These signals reflect neural population activity in hippocampal CA1 during virtual navigation.\n\n# %% [markdown]\n# ## 7. ROI Segmentation Metadata: How Many Cells are Recorded?\n#\n# ROIs (\"cells\") are derived from Suite2p and stored as a segmentation table.\n# Let's summarize and plot the distribution of iscell/probcell values (Suite2p outputs), and the number of detected cells:\n\n# %%\n# ImageSegmentation metadata from Suite2p\nImageSegmentation = ophys.data_interfaces[\"ImageSegmentation\"]\nPlaneSegmentation = ImageSegmentation.plane_segmentations[\"PlaneSegmentation\"]\n\nroi_table = PlaneSegmentation.to_dataframe()\nprint(roi_table.head())\n\n# 'iscell' has two columns: iscell (bool/int), probcell (probability)\niscell, probcell = roi_table['iscell'].apply(lambda x: x[0]), roi_table['iscell'].apply(lambda x: x[1])\nnum_total = len(roi_table)\nnum_cells = iscell.sum()\n\nprint(f\"Total ROIs: {num_total}\")\nprint(f\"ROIs classified as cells: {num_cells}\")\n\nplt.figure(figsize=(7,4))\nplt.hist(probcell, bins=np.linspace(0,1,21), color='C0', edgecolor='k', alpha=0.7)\nplt.xlabel(\"Probability Cell (Suite2p)\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of 'probcell' for ROIs\")\nplt.show()\n\n# %% [markdown]\n# ## 8. Imaging Metadata: Devices and Planes\n#\n# Let's review metadata for imaging device and parameters:\n\n# %%\niplane = PlaneSegmentation.imaging_plane\nprint(\"Imaging plane location:\", iplane.location)\nprint(\"Imaging indicator:\", iplane.indicator)\nprint(\"Excitation wavelength:\", iplane.excitation_lambda)\nprint(\"Imaging rate:\", iplane.imaging_rate)\nprint(\"Grid spacing (microns):\", iplane.grid_spacing[:])\nprint(\"Device (manufacturer):\", iplane.device.manufacturer)\n\n# %% [markdown]\n# ## 9. Example: Average Max-Projection and Mean-Image\n#\n# Raw images (background, mean, max) are available via `Images` in the `ophys` module. Let's visualize these reference images.\n\n# %%\nImages = ophys.data_interfaces[\"Backgrounds_0\"]\nmeanImg = Images.images[\"meanImg\"].data[:]\nmax_proj = Images.images[\"max_proj\"].data[:]\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.imshow(meanImg, cmap='gray')\nplt.title(\"Mean Imaging Frame\")\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(max_proj, cmap='gray')\nplt.title(\"Max Projection Frame\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Summary and Next Steps\n#\n# In this notebook, we introduced Dandiset 001361, accessed NWB files remotely, and visualized representative behavioral and imaging data. This should provide a foundation for:\n#\n# - Exploring how hippocampal population codes adapt to reward relocation\n# - Linking cell activity to behavior or position\n# - Advanced analyses (e.g., identifying place or reward-responsive cells, event-triggered averaging, etc.)\n#\n# For further reanalysis, check out the [custom analysis code](https://github.com/GiocomoLab/Sosa_et_al_2024) provided by the authors.\n#\n# ---  \n# *You are encouraged to adapt and extend this notebook for your own analyses!*\n</notebook>"
  },
  {
    "role": "system",
    "content": "The notebook execution failed with the following error:\n\nNotebook execution failed: [NbClientApp] Executing notebooks/dandisets/001361/0.250406.0045/skip-explore/gpt-4.1/h-2/notebook.ipynb\n[NbClientApp] Executing notebook with kernel: \nTraceback (most recent call last):\n  File \"/home/magland/miniconda3/envs/dev/bin/jupyter-execute\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n    super().launch_instance(argv=argv, **kwargs)\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n    app.initialize(argv)\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n    return method(app, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/nbclient/cli.py\", line 161, in initialize\n    self.run_notebook(path)\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/nbclient/cli.py\", line 215, in run_notebook\n    client.execute()\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n    return loop.run_until_complete(inner)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/asyncio/base_events.py\", line 685, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/nbclient/client.py\", line 709, in async_execute\n    await self.async_execute_cell(\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/nbclient/client.py\", line 1062, in async_execute_cell\n    await self._check_raise_for_error(cell, cell_index, exec_reply)\n  File \"/home/magland/miniconda3/envs/dev/lib/python3.12/site-packages/nbclient/client.py\", line 918, in _check_raise_for_error\n    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\nnbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n------------------\nimport matplotlib.pyplot as plt\n\n# For illustration, let's plot data from the first ~2 minutes (if available)\nfs = int(speed.data.shape[0])\nsampling_rate = speed.rate if hasattr(speed, \"rate\") else 15.5  # (Hz)\nduration_sec = fs / sampling_rate\nmax_time = min(120, duration_sec)  # Max 120 seconds\n\nmask = position.timestamps[:] < max_time\n\n# Extract data for plotting\ntime = position.timestamps[mask]\npos = position.data[mask]\nspd = speed.data[mask]\nlicks = lick.data[mask]\nreward_events = reward.timestamps[:]\nreward_zone_bin = reward_zone.data[mask] > 0.5\n\nfig, axs = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\naxs[0].plot(time, pos, label=\"Position (cm)\", color=\"C0\")\naxs[0].set_ylabel(\"Position (cm)\")\naxs[0].set_title(\"Mouse Trajectory & Behavior (first 2 min)\")\naxs[1].plot(time, spd, label=\"Speed (cm/s)\", color=\"C1\")\naxs[1].set_ylabel(\"Speed (cm/s)\")\naxs[2].plot(time, licks, label=\"Cumulative Licks\", color=\"C2\")\naxs[2].set_ylabel(\"Lick Count\")\naxs[3].plot(time, reward_zone_bin, label=\"In Reward Zone\", color=\"C3\")\naxs[3].set_ylabel(\"Reward Zone\")\naxs[3].set_xlabel(\"Time (s)\")\n\n# Mark reward times\nfor ax in axs:\n    for rt in reward_events:\n        if rt < max_time:\n            ax.axvline(rt, color=\"red\", linestyle=\"--\", linewidth=0.7, alpha=0.7)\n\naxs[0].legend(), axs[1].legend(), axs[2].legend(), axs[3].legend()\nplt.tight_layout()\nplt.show()\n------------------\n\n\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\nCell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(speed\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m sampling_rate \u001b[38;5;241m=\u001b[39m speed\u001b[38;5;241m.\u001b[39mrate \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(speed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m15.5\u001b[39m  \u001b[38;5;66;03m# (Hz)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m duration_sec \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\n\u001b[1;32m      7\u001b[0m max_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m120\u001b[39m, duration_sec)  \u001b[38;5;66;03m# Max 120 seconds\u001b[39;00m\n\u001b[1;32m      9\u001b[0m mask \u001b[38;5;241m=\u001b[39m position\u001b[38;5;241m.\u001b[39mtimestamps[:] \u001b[38;5;241m<\u001b[39m max_time\n\n\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'NoneType'\n\n\n\nPlease provide a corrected version of the notebook."
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 001361: A flexible hippocampal population code for experience relative to reward\n#\n# **Notebook generated with AI assistance. Please interpret code and results with caution and consult original data and documentation as needed.**\n#\n# ---\n#\n# This notebook provides an interactive introduction to [Dandiset 001361, version 0.250406.0045](https://dandiarchive.org/dandiset/001361/0.250406.0045):  \n# **\"A flexible hippocampal population code for experience relative to reward\"** by Sosa, Plitt, & Giocomo (2025, Nature Neuroscience).  \n#\n# ---\n#\n# ## Overview\n#\n# This Dandiset contains two-photon calcium imaging and behavioral datasets from the CA1 area of the mouse hippocampus. Mice navigate virtual linear tracks with changing hidden reward locations, and their neural and behavioral responses are recorded. The data include:\n# - Raw and processed optical physiology (ophys) measurements (e.g., calcium traces)\n# - Behavioral data (licking, position, speed, rewards)\n# - ROI segmentation and metadata\n#\n# Data are provided in NWB format, compatible with the [Neurodata Without Borders](https://www.nwb.org/) ecosystem.\n#\n# ---\n#\n# ## What this notebook covers\n#\n# - How to use the DANDI API to explore Dandiset 001361\n# - How to remotely stream NWB files from the archive\n# - Navigating the structure of a representative NWB file\n# - Visualizing behavioral and calcium imaging data  \n# - Laying groundwork for further reanalysis\n#\n# ---\n#\n# ## Requirements\n#\n# This notebook requires the following packages (assumed installed):\n#\n# - pynwb\n# - h5py\n# - remfile\n# - dandi\n# - numpy\n# - pandas\n# - matplotlib\n#\n# ---\n#\n# Let's get started!\n#\n# %% [markdown]\n# ## 1. Connect to DANDI and List Available Assets\n#\n# We'll use the DANDI Python client library to access the public assets in this Dandiset.  \n# For illustration, we'll focus on NWB files corresponding to mouse `sub-m11`.\n\n# %%\nfrom dandi.dandiapi import DandiAPIClient\n\n# Connect to the DANDI Archive and access the specific Dandiset and version\ndandiset_id = \"001361\"\ndandiset_version = \"0.250406.0045\"\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(dandiset_id, dandiset_version)\n\n# List all NWB files for subject m11 in this dandiset\nnwb_assets = [\n    asset\n    for asset in dandiset.get_assets()\n    if asset.path.startswith(\"sub-m11/\") and asset.path.endswith(\".nwb\")\n]\n\nprint(f\"Found {len(nwb_assets)} NWB files for sub-m11:\")\nfor asset in nwb_assets:\n    print(f\" - {asset.path}\")\n\n# %% [markdown]\n# ## 2. Load a Sample NWB File Remotely\n#\n# We'll pick one session (`sub-m11/sub-m11_ses-05_behavior+ophys.nwb`) to demonstrate data access and visualization.\n#\n# We use `remfile` and `h5py` to stream the NWB file directly from the cloud\u2014no download required!\n\n# %%\nimport remfile\nimport h5py\nimport pynwb\n\n# Choose a session to explore\nasset_path = \"sub-m11/sub-m11_ses-05_behavior+ophys.nwb\"\nasset = next(dandiset.get_assets_by_glob(asset_path))\n\nprint(f\"Streaming remote NWB file: {asset.path}\")\n\n# Open remote NWB file for reading\nremote_file = remfile.File(asset.download_url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\nprint(f\"Loaded NWB session: {nwb.identifier}\")\n\n# %% [markdown]\n# ## 3. Inspecting the NWB File: Session and Subject Information\n\n# %%\nfrom datetime import datetime\n\nprint(\"Session description:\", nwb.session_description)\nprint(\"Session start time:\", nwb.session_start_time)\nprint(\"Session ID:\", nwb.session_id)\nprint(\"---\")\nprint(\"Subject ID:\", nwb.subject.subject_id)\nprint(\"Species:\", nwb.subject.species)\nprint(\"Sex:\", nwb.subject.sex)\nprint(\"Date of birth:\", nwb.subject.date_of_birth.date())\nif hasattr(nwb.subject, \"age__reference\"):\n    print(\"Age at reference (if provided):\", nwb.subject.age__reference)\nprint(\"---\")\nprint(\"Experimenter:\", nwb.experimenter)\n\n# %% [markdown]\n# ## 4. Overview of Acquisition and Processed Data\n#\n# The file stores:\n# - **Acquisition**: Raw two-photon imaging\n# - **Processing modules**: Behavior, ophys (processed), segmentation data\n#\n# Let's see the available acquisition and processing modules:\n\n# %%\nprint(\"Acquisition modules:\", list(nwb.acquisition.keys()))\nprint(\"Processing modules:\", list(nwb.processing.keys()))\n\n# %% [markdown]\n# ## 5. Exploring Behavioral Data: Virtual Navigation, Licking, Rewards\n#\n# All behavioral data are in the `behavior` processing module, under a `BehavioralTimeSeries` interface.  \n# Let's extract key behavioral variables: position, speed, licking, and rewards.\n\n# %%\nimport numpy as np\n\nbehavior = nwb.processing[\"behavior\"]\nbts = behavior.data_interfaces[\"BehavioralTimeSeries\"]\n\n# Grab time series of interest\nposition = bts.time_series[\"position\"]\nspeed = bts.time_series[\"speed\"]\nlick = bts.time_series[\"lick\"]\nreward_zone = bts.time_series[\"reward_zone\"]\nreward = bts.time_series[\"Reward\"]\n\nprint(\"Position data:\", position.data.shape)\nprint(\"Speed data:\", speed.data.shape)\nprint(\"Lick data:\", lick.data.shape)\nprint(\"Reward zone data:\", reward_zone.data.shape)\nprint(\"Reward times:\", reward.timestamps.shape)\n\n# %% [markdown]\n# Let's visualize the mouse's trajectory and behavior around reward delivery events.\n# Note: We're using `.interval` and the length of the timestamps to estimate the time axis.\n\n# %%\nimport matplotlib.pyplot as plt\n\n# Use .timestamps if available, else construct time vector from sample rate (here, .interval=1, 1:1 with frames)\n# Some NWB objects may lack a 'rate' parameter; in that case, rely on available timebase.\n# The `timestamps` arrays are guaranteed by the metadata to be same length as the data arrays.\n\n# Determine how much data to plot (first 2 minutes or less)\nposition_t = position.timestamps[:]\nspeed_t = speed.timestamps[:]\nlick_t = lick.timestamps[:]\nreward_zone_t = reward_zone.timestamps[:]\n\nplot_dur = 120  # seconds\nmax_len = np.sum(position_t < plot_dur)\n\ntime = position_t[:max_len]\npos = position.data[:max_len]\nspd = speed.data[:max_len]\nlicks = lick.data[:max_len]\nreward_zone_bin = reward_zone.data[:max_len] > 0.5\nreward_events = reward.timestamps[:]\n\nfig, axs = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\naxs[0].plot(time, pos, label=\"Position (cm)\", color=\"C0\")\naxs[0].set_ylabel(\"Position (cm)\")\naxs[0].set_title(\"Mouse Trajectory & Behavior (first 2 min)\")\naxs[1].plot(time, spd, label=\"Speed (cm/s)\", color=\"C1\")\naxs[1].set_ylabel(\"Speed (cm/s)\")\naxs[2].plot(time, licks, label=\"Cumulative Licks\", color=\"C2\")\naxs[2].set_ylabel(\"Lick Count\")\naxs[3].plot(time, reward_zone_bin, label=\"In Reward Zone\", color=\"C3\")\naxs[3].set_ylabel(\"Reward Zone\")\naxs[3].set_xlabel(\"Time (s)\")\n\n# Mark reward times\nfor ax in axs:\n    for rt in reward_events:\n        if rt < plot_dur:\n            ax.axvline(rt, color=\"red\", linestyle=\"--\", linewidth=0.7, alpha=0.7)\n\nfor ax in axs:\n    ax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# **Interpretation:**  \n# - Each vertical red dashed line indicates a reward delivery time.  \n# - Observe licking and speeds relative to rewards and reward zone entry.\n#\n# This view provides a quick glance at the behavioral structure of the experiment.\n\n# %% [markdown]\n# ## 6. Exploring Calcium Imaging Data (Processed Fluorescence)\n#\n# The `ophys` processing module contains processed calcium traces (Fluorescence and Deconvolved), segmentation metadata, and neuropil traces.\n#\n# Let's examine the main `Fluorescence` traces for all detected ROIs (cells) and visualize the activity of a random subset across time.\n\n# %%\nophys = nwb.processing[\"ophys\"]\nFluorescence = ophys.data_interfaces[\"Fluorescence\"]\nf_plane0 = Fluorescence.roi_response_series[\"plane0\"]\n\nflu_data = f_plane0.data         # shape (time, cells)\nn_time = flu_data.shape[0]\nn_cells = flu_data.shape[1]\n\n# Construct time axis for calcium traces\n# Use timestamps from behavior, otherwise, use index and reported rate if available\ntry:\n    flu_rate = f_plane0.rate\nexcept AttributeError:\n    flu_rate = None\nif flu_rate is not None:\n    flu_time = np.arange(n_time) / flu_rate\nelse:\n    flu_time = np.arange(n_time)\n\nprint(f\"Fluorescence data shape: {flu_data.shape}  (frames x ROIs)\")\nprint(f\"Calcium imaging sampling rate: {flu_rate if flu_rate else 'unknown'} Hz\")\n\n# Subselect cells for plotting (up to 10 random ROIs)\nn_show = min(10, n_cells)\nnp.random.seed(42)\nsample_cells = np.random.choice(n_cells, n_show, replace=False)\n\nplt.figure(figsize=(10, 6))\noffset = 4\nfor idx, cell in enumerate(sample_cells):\n    plt.plot(flu_time, flu_data[:, cell] + idx * offset, label=f\"Cell {cell}\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Fluorescence (offset per cell)\")\nplt.title(f\"Raw Calcium Traces from {n_show} Random ROIs\")\nplt.yticks([])\nplt.legend(ncol=2, fontsize=8)\nplt.xlim([0, min(120, flu_time[-1])])\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# **Interpretation:**  \n# - Each trace is a cell's raw fluorescence, vertically offset for clarity.  \n# - These signals reflect neural population activity in hippocampal CA1 during virtual navigation.\n\n# %% [markdown]\n# ## 7. ROI Segmentation Metadata: How Many Cells are Recorded?\n#\n# ROIs (\"cells\") are derived from Suite2p and stored as a segmentation table.\n# Let's summarize and plot the distribution of iscell/probcell values (Suite2p outputs), and the number of detected cells:\n\n# %%\n# ImageSegmentation metadata from Suite2p\nImageSegmentation = ophys.data_interfaces[\"ImageSegmentation\"]\nPlaneSegmentation = ImageSegmentation.plane_segmentations[\"PlaneSegmentation\"]\n\nroi_table = PlaneSegmentation.to_dataframe()\nprint(roi_table.head())\n\n# 'iscell' has two columns: iscell (bool/int), probcell (probability)\niscell = roi_table['iscell'].apply(lambda x: x[0])\nprobcell = roi_table['iscell'].apply(lambda x: x[1])\nnum_total = len(roi_table)\nnum_cells = iscell.sum()\n\nprint(f\"Total ROIs: {num_total}\")\nprint(f\"ROIs classified as cells: {num_cells}\")\n\nplt.figure(figsize=(7,4))\nplt.hist(probcell, bins=np.linspace(0,1,21), color='C0', edgecolor='k', alpha=0.7)\nplt.xlabel(\"Probability Cell (Suite2p)\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of 'probcell' for ROIs\")\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## 8. Imaging Metadata: Devices and Planes\n#\n# Let's review metadata for imaging device and parameters:\n\n# %%\niplane = PlaneSegmentation.imaging_plane\nprint(\"Imaging plane location:\", iplane.location)\nprint(\"Imaging indicator:\", iplane.indicator)\nprint(\"Excitation wavelength:\", iplane.excitation_lambda)\nprint(\"Imaging rate:\", iplane.imaging_rate)\nprint(\"Grid spacing (microns):\", iplane.grid_spacing[:])\nprint(\"Device (manufacturer):\", iplane.device.manufacturer)\n\n# %% [markdown]\n# ## 9. Example: Average Max-Projection and Mean-Image\n#\n# Raw images (background, mean, max) are available via `Images` in the `ophys` module. Let's visualize these reference images.\n\n# %%\nImages = ophys.data_interfaces[\"Backgrounds_0\"]\nmeanImg = Images.images[\"meanImg\"].data[:]\nmax_proj = Images.images[\"max_proj\"].data[:]\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.imshow(meanImg, cmap='gray')\nplt.title(\"Mean Imaging Frame\")\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(max_proj, cmap='gray')\nplt.title(\"Max Projection Frame\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Summary and Next Steps\n#\n# In this notebook, we introduced Dandiset 001361, accessed NWB files remotely, and visualized representative behavioral and imaging data. This should provide a foundation for:\n#\n# - Exploring how hippocampal population codes adapt to reward relocation\n# - Linking cell activity to behavior or position\n# - Advanced analyses (e.g., identifying place or reward-responsive cells, event-triggered averaging, etc.)\n#\n# For further reanalysis, check out the [custom analysis code](https://github.com/GiocomoLab/Sosa_et_al_2024) provided by the authors.\n#\n# ---  \n# *You are encouraged to adapt and extend this notebook for your own analyses!*\n</notebook>"
  }
]