[
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata about Dandiset 000690 version 0.250326.0015."
      },
      {
        "type": "text",
        "text": "{\n  \"id\": \"DANDI:000690/0.250326.0015\",\n  \"doi\": \"10.48324/dandi.000690/0.250326.0015\",\n  \"url\": \"https://dandiarchive.org/dandiset/000690/0.250326.0015\",\n  \"name\": \"Allen Institute Openscope - Vision2Hippocampus project\",\n  \"about\": [],\n  \"access\": [\n    {\n      \"status\": \"dandi:OpenAccess\",\n      \"schemaKey\": \"AccessRequirements\"\n    }\n  ],\n  \"license\": [\n    \"spdx:CC-BY-4.0\"\n  ],\n  \"version\": \"0.250326.0015\",\n  \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.4/context.json\",\n  \"citation\": \"Mehta, Mayank R; Purandare, Chinmay; Jha, Siddharth; Lecoq, J\\u00e9r\\u00f4me; Durand, S\\u00e9verine; Gillis, Ryan; Belski, Hannah; Bawany, Ahad; Carlson, Mikayla; Peene, Carter; Wilkes, Josh; Johnson, Tye; Naidoo, Robyn; Suarez, Lucas; Han, Warren; Amaya, Avalon; Nguyen, Katrina; Ouellette, Ben; Swapp, Jackie; Williford, Ali (2025) Allen Institute Openscope - Vision2Hippocampus project (Version 0.250326.0015) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.000690/0.250326.0015\",\n  \"keywords\": [\n    \"mouse\",\n    \"neuropixel\",\n    \"extracellular electrophysiology\",\n    \"neocortex\",\n    \"hippocampus\",\n    \"Entorhinal cortex \",\n    \"excitatory\",\n    \"inhibitory\",\n    \"vision\",\n    \"movies\"\n  ],\n  \"protocol\": [],\n  \"schemaKey\": \"Dandiset\",\n  \"identifier\": \"DANDI:000690\",\n  \"repository\": \"https://dandiarchive.org\",\n  \"contributor\": [\n    {\n      \"name\": \"National Institutes of Health (NIH)\",\n      \"roleName\": [\n        \"dcite:Funder\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"awardNumber\": \"1 U24 NS 113646-03\"\n    },\n    {\n      \"name\": \"Allen Institute\",\n      \"schemaKey\": \"Organization\",\n      \"identifier\": \"https://ror.org/03cpe7c52\",\n      \"includeInCitation\": false\n    },\n    {\n      \"name\": \"Mehta, Mayank R\",\n      \"email\": \"mayankmehta@ucla.edu\",\n      \"roleName\": [\n        \"dcite:Conceptualization\",\n        \"dcite:ContactPerson\",\n        \"dcite:FormalAnalysis\",\n        \"dcite:Investigation\",\n        \"dcite:ProjectManager\",\n        \"dcite:ProjectLeader\",\n        \"dcite:Researcher\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-2005-2468\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Purandare, Chinmay\",\n      \"email\": \"chinmay.purandare@gmail.com\",\n      \"roleName\": [\n        \"dcite:Conceptualization\",\n        \"dcite:FormalAnalysis\",\n        \"dcite:Investigation\",\n        \"dcite:ProjectMember\",\n        \"dcite:ContactPerson\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0001-9225-0186\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Jha, Siddharth\",\n      \"roleName\": [\n        \"dcite:FormalAnalysis\",\n        \"dcite:Investigation\",\n        \"dcite:Researcher\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Lecoq, J\\u00e9r\\u00f4me\",\n      \"email\": \"jeromel@alleninstitute.org\",\n      \"roleName\": [\n        \"dcite:ContactPerson\",\n        \"dcite:FundingAcquisition\",\n        \"dcite:ProjectLeader\",\n        \"dcite:ProjectManager\",\n        \"dcite:ProjectAdministration\",\n        \"dcite:Software\",\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-0131-0938\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Durand, S\\u00e9verine\",\n      \"roleName\": [\n        \"dcite:DataManager\",\n        \"dcite:FormalAnalysis\",\n        \"dcite:Supervision\",\n        \"dcite:DataCurator\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0001-7788-0200\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Gillis, Ryan\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-1879-6519\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Belski, Hannah\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0001-5422-5964\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Bawany, Ahad\",\n      \"roleName\": [\n        \"dcite:DataCurator\",\n        \"dcite:DataManager\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Carlson, Mikayla\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0003-7019-2781\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Peene, Carter\",\n      \"roleName\": [\n        \"dcite:DataCurator\",\n        \"dcite:DataManager\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-6660-2264\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Wilkes, Josh\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0005-4858-2219\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Johnson, Tye\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-7239-7571\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Naidoo, Robyn\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0004-0996-8808\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Suarez, Lucas\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0002-0121-3334\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Han, Warren\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-0104-0380\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Amaya, Avalon\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-2274-2892\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Nguyen, Katrina\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0009-8547-9703\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Ouellette, Ben\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-4913-554X\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Swapp, Jackie\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0008-4965-6242\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Williford, Ali\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-7282-7515\",\n      \"includeInCitation\": true\n    }\n  ],\n  \"dateCreated\": \"2023-10-24T16:52:41.591223+00:00\",\n  \"description\": \"Extensive research shows that visual cortical neurons respond to specific stimuli, e.g. the primary visual cortical neurons respond to bars of light with specific orientation. In contrast, the hippocampal neurons are thought to encode not specific stimuli but instead represent abstract concepts such as space, time and events. How is this abstraction computed in the mouse brain? Specifically, how does the representation of simple visual stimuli evolve from the thalamus, which is a synapse away from the retina, through primary visual cortex, higher order visual areas and all the way to hippocampus, that is farthest removed from the retina?\\n\\nThe current OpenScope project aims to understand how the neural representations of simple and natural stimuli evolve from the LGN through V1, and most hippocampal regions, as well as some of the frontal areas. \\n\\nStimuli presented\\nTwo main categories of visual stimuli were presented\\u2013\\n1.\\tSimple visual motion, elicited by basic stimuli, like bars of light.\\n2.\\tComplex, potentially ethologically valid visual stimuli, elicited by movies involving eagles (and squirrels).\\nTo parametrize the stimulus properties which might be affecting neural responses, mice were shown variants of the vertical bar of light as follows:\\nA(o) \\u2013 The bar of light was white, moving on a black background, 15 degrees wide, and moved at a fixed speed, covered the entire width of the screen in 2 seconds. It covered both movement directions consecutively (naso-temporal, then temporo-nasal).\\nA(i) \\u2013 Similar to A(o), but the bar was now thrice as wide (45o)\\nA(ii) \\u2013 Similar to A(o), but the bar was thrice as slow (covering the width of the screen in 6 seconds).\\nA(iii) \\u2013 Similar to A(o), but the contrast was flipped, i.e. a black bar of light on a white background.\\nA(iv) - Similar to A(o), but instead of a simple white bar, the stimulus was striped, and each stripe changed color as the stimulus moved through the width of the screen. This was called \\u201cdisco\\u201d bar of light\\nA(v) \\u2013 In a subset of mice, A(o) was appended by frames corresponding to the bar of light \\u201cvanishing\\u201d at either of the edges. Two vanishing protocols were attempted, the bar of light is fully absorbed by the boundary, before reemerging. Another protocol had the bar of light vanish for ~1 second in addition to smoothly being absorbed by the boundary, before reemerging.\",\n  \"publishedBy\": {\n    \"id\": \"urn:uuid:fde40c71-b969-4678-a488-336520b8e5b0\",\n    \"name\": \"DANDI publish\",\n    \"endDate\": \"2025-03-26T00:15:32.143954+00:00\",\n    \"schemaKey\": \"PublishActivity\",\n    \"startDate\": \"2025-03-26T00:15:32.143954+00:00\",\n    \"wasAssociatedWith\": [\n      {\n        \"id\": \"urn:uuid:e7f91a82-d8e8-4101-a84c-afb7f518e567\",\n        \"name\": \"DANDI API\",\n        \"version\": \"0.1.0\",\n        \"schemaKey\": \"Software\",\n        \"identifier\": \"RRID:SCR_017571\"\n      }\n    ]\n  },\n  \"studyTarget\": [],\n  \"assetsSummary\": {\n    \"species\": [\n      {\n        \"name\": \"Mus musculus - House mouse\",\n        \"schemaKey\": \"SpeciesType\",\n        \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n      }\n    ],\n    \"approach\": [\n      {\n        \"name\": \"electrophysiological approach\",\n        \"schemaKey\": \"ApproachType\"\n      }\n    ],\n    \"schemaKey\": \"AssetsSummary\",\n    \"dataStandard\": [\n      {\n        \"name\": \"Neurodata Without Borders (NWB)\",\n        \"schemaKey\": \"StandardsType\",\n        \"identifier\": \"RRID:SCR_015242\"\n      }\n    ],\n    \"numberOfBytes\": 600243449691,\n    \"numberOfFiles\": 156,\n    \"numberOfSubjects\": 25,\n    \"variableMeasured\": [\n      \"ElectricalSeries\",\n      \"LFP\",\n      \"ProcessingModule\",\n      \"Units\"\n    ],\n    \"measurementTechnique\": [\n      {\n        \"name\": \"multi electrode extracellular electrophysiology recording technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"signal filtering technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"spike sorting technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"analytical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      }\n    ]\n  },\n  \"datePublished\": \"2025-03-26T00:15:32.143954+00:00\",\n  \"schemaVersion\": \"0.6.4\",\n  \"ethicsApproval\": [\n    {\n      \"schemaKey\": \"EthicsApproval\",\n      \"identifier\": \"\\u200bAllen Institute Institutional Animal Care and Use Committee (IACUC), protocol 2117\",\n      \"contactPoint\": {\n        \"schemaKey\": \"ContactPoint\"\n      }\n    }\n  ],\n  \"wasGeneratedBy\": [],\n  \"acknowledgement\": \"We thank the Allen Institute founder, Paul G. Allen, for his vision, encouragement, and support.\",\n  \"relatedResource\": [],\n  \"manifestLocation\": [\n    \"https://dandiarchive.s3.amazonaws.com/dandisets/000690/0.250326.0015/assets.yaml\"\n  ]\n}"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is a listing of some of the files in the Dandiset."
      },
      {
        "type": "text",
        "text": "sub-692077/sub-692077_ses-1300222049_probe-4_ecephys.nwb\nsub-692077/sub-692077_ses-1300222049_probe-3_ecephys.nwb\nsub-692077/sub-692077_ses-1300222049_probe-2_ecephys.nwb\nsub-692077/sub-692077_ses-1300222049_probe-0_ecephys.nwb\nsub-692077/sub-692077_ses-1300222049.nwb\nsub-692077/sub-692077_ses-1300222049_probe-1_ecephys.nwb\nsub-695435/sub-695435_ses-1309235849_probe-3_ecephys.nwb\nsub-695435/sub-695435_ses-1309235849_probe-1_ecephys.nwb\nsub-695435/sub-695435_ses-1309235849_probe-0_ecephys.nwb\nsub-695435/sub-695435_ses-1309235849_probe-2_ecephys.nwb"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata and usage information about the file sub-692077/sub-692077_ses-1300222049_probe-4_ecephys.nwb."
      },
      {
        "type": "text",
        "text": "\nimport pynwb\nimport h5py\nimport remfile\n\n# Load\nfrom dandi.dandiapi import DandiAPIClient\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000690\", \"0.250326.0015\")\nurl = next(dandiset.get_assets_by_glob(\"sub-692077/sub-692077_ses-1300222049_probe-4_ecephys.nwb\")).download_url\n\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\nnwb # (NWBFile)\nnwb.session_description # (str) LFP data and associated info for one probe\nnwb.identifier # (str) 4\nnwb.session_start_time # (datetime) 2023-09-28T00:00:00-07:00\nnwb.timestamps_reference_time # (datetime) 2023-09-28T00:00:00-07:00\nnwb.file_create_date # (list) [datetime.datetime(2023, 11, 16, 2, 47, 21, 734893, tzinfo=tzoffset(None, -28800))]\nnwb.acquisition # (LabelledDict)\nacquisition = nwb.acquisition\nprobe_4_lfp = acquisition[\"probe_4_lfp\"]\nprobe_4_lfp # (LFP)\nprobe_4_lfp.electrical_series # (LabelledDict)\nelectrical_series = probe_4_lfp.electrical_series\nprobe_4_lfp_data = electrical_series[\"probe_4_lfp_data\"]\nprobe_4_lfp_data # (ElectricalSeries)\nprobe_4_lfp_data.resolution # (float64) -1.0\nprobe_4_lfp_data.comments # (str) no comments\nprobe_4_lfp_data.description # (str) no description\nprobe_4_lfp_data.conversion # (float64) 1.0\nprobe_4_lfp_data.offset # (float64) 0.0\nprobe_4_lfp_data.unit # (str) volts\nprobe_4_lfp_data.data # (Dataset) shape (8962735, 96); dtype float32\n# probe_4_lfp_data.data[:, :] # Access all data\n# probe_4_lfp_data.data[0:n, :] # Access first n rows\n# probe_4_lfp_data.data[:, 0:n] # Access first n columns\nprobe_4_lfp_data.timestamps # (Dataset) shape (8962735,); dtype float64\n# probe_4_lfp_data.timestamps[:] # Access all data\n# probe_4_lfp_data.timestamps[0:n] # Access first n elements\nprobe_4_lfp_data.timestamps_unit # (str) seconds\nprobe_4_lfp_data.interval # (int) 1\nprobe_4_lfp_data.electrodes # (DynamicTableRegion)\nprobe_4_lfp_data.electrodes.description # (str) lfp channels on probe 4\nprobe_4_lfp_data.electrodes.table # (DynamicTable)\nprobe_4_lfp_data.electrodes.table.description # (str) metadata about extracellular electrodes\nprobe_4_lfp_data.electrodes.table.colnames # (tuple)\nprobe_4_lfp_data.electrodes.table.columns # (tuple)\nprobe_4_lfp_data.electrodes.table.id # (ElementIdentifiers)\n# probe_4_lfp_data.electrodes.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 96 rows and 13 columns\n# probe_4_lfp_data.electrodes.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nprobe_4_lfp_data.electrodes.table.location # (VectorData) the location of channel within the subject e.g. brain region\nprobe_4_lfp_data.electrodes.table.group # (VectorData) a reference to the ElectrodeGroup this electrode is a part of\nprobe_4_lfp_data.electrodes.table.group_name # (VectorData) the name of the ElectrodeGroup this electrode is a part of\nprobe_4_lfp_data.electrodes.table.probe_vertical_position # (VectorData) Length-wise position of electrode/channel on device (microns)\nprobe_4_lfp_data.electrodes.table.probe_horizontal_position # (VectorData) Width-wise position of electrode/channel on device (microns)\nprobe_4_lfp_data.electrodes.table.probe_id # (VectorData) The unique id of this electrode's/channel's device\nprobe_4_lfp_data.electrodes.table.local_index # (VectorData) The local index of electrode/channel on device\nprobe_4_lfp_data.electrodes.table.valid_data # (VectorData) Whether data from this electrode/channel is usable\nprobe_4_lfp_data.electrodes.table.x # (VectorData) the x coordinate of the position (+x is posterior)\nprobe_4_lfp_data.electrodes.table.y # (VectorData) the y coordinate of the position (+y is inferior)\nprobe_4_lfp_data.electrodes.table.z # (VectorData) the z coordinate of the position (+z is right)\nprobe_4_lfp_data.electrodes.table.imp # (VectorData) the impedance of the electrode, in ohms\nprobe_4_lfp_data.electrodes.table.filtering # (VectorData) description of hardware filtering, including the filter name and frequency cutoffs\nprobe_4_lfp_data = acquisition[\"probe_4_lfp_data\"]\nprobe_4_lfp_data # (ElectricalSeries)\nprobe_4_lfp_data.resolution # (float64) -1.0\nprobe_4_lfp_data.comments # (str) no comments\nprobe_4_lfp_data.description # (str) no description\nprobe_4_lfp_data.conversion # (float64) 1.0\nprobe_4_lfp_data.offset # (float64) 0.0\nprobe_4_lfp_data.unit # (str) volts\nprobe_4_lfp_data.data # (Dataset) shape (8962735, 96); dtype float32\n# probe_4_lfp_data.data[:, :] # Access all data\n# probe_4_lfp_data.data[0:n, :] # Access first n rows\n# probe_4_lfp_data.data[:, 0:n] # Access first n columns\nprobe_4_lfp_data.timestamps # (Dataset) shape (8962735,); dtype float64\n# probe_4_lfp_data.timestamps[:] # Access all data\n# probe_4_lfp_data.timestamps[0:n] # Access first n elements\nprobe_4_lfp_data.timestamps_unit # (str) seconds\nprobe_4_lfp_data.interval # (int) 1\nprobe_4_lfp_data.electrodes # (DynamicTableRegion)\nprobe_4_lfp_data.electrodes.description # (str) lfp channels on probe 4\nprobe_4_lfp_data.electrodes.table # (DynamicTable)\nprobe_4_lfp_data.electrodes.table.description # (str) metadata about extracellular electrodes\nprobe_4_lfp_data.electrodes.table.colnames # (tuple)\nprobe_4_lfp_data.electrodes.table.columns # (tuple)\nprobe_4_lfp_data.electrodes.table.id # (ElementIdentifiers)\n# probe_4_lfp_data.electrodes.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 96 rows and 13 columns\n# probe_4_lfp_data.electrodes.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nprobe_4_lfp_data.electrodes.table.location # (VectorData) the location of channel within the subject e.g. brain region\nprobe_4_lfp_data.electrodes.table.group # (VectorData) a reference to the ElectrodeGroup this electrode is a part of\nprobe_4_lfp_data.electrodes.table.group_name # (VectorData) the name of the ElectrodeGroup this electrode is a part of\nprobe_4_lfp_data.electrodes.table.probe_vertical_position # (VectorData) Length-wise position of electrode/channel on device (microns)\nprobe_4_lfp_data.electrodes.table.probe_horizontal_position # (VectorData) Width-wise position of electrode/channel on device (microns)\nprobe_4_lfp_data.electrodes.table.probe_id # (VectorData) The unique id of this electrode's/channel's device\nprobe_4_lfp_data.electrodes.table.local_index # (VectorData) The local index of electrode/channel on device\nprobe_4_lfp_data.electrodes.table.valid_data # (VectorData) Whether data from this electrode/channel is usable\nprobe_4_lfp_data.electrodes.table.x # (VectorData) the x coordinate of the position (+x is posterior)\nprobe_4_lfp_data.electrodes.table.y # (VectorData) the y coordinate of the position (+y is inferior)\nprobe_4_lfp_data.electrodes.table.z # (VectorData) the z coordinate of the position (+z is right)\nprobe_4_lfp_data.electrodes.table.imp # (VectorData) the impedance of the electrode, in ohms\nprobe_4_lfp_data.electrodes.table.filtering # (VectorData) description of hardware filtering, including the filter name and frequency cutoffs\nnwb.electrode_groups # (LabelledDict)\nelectrode_groups = nwb.electrode_groups\nprobeF = electrode_groups[\"probeF\"]\nprobeF # (EcephysElectrodeGroup)\nprobeF.description # (str) Ecephys Electrode Group\nprobeF.location # (str) See electrode locations\nprobeF.has_lfp_data # (bool)\nprobeF.probe_id # (int64)\nprobeF.lfp_sampling_rate # (float64) 625.0\nprobeF.device # (EcephysProbe)\nprobeF.device.description # (str) Neuropixels 1.0 Probe\nprobeF.device.manufacturer # (str) imec\nprobeF.device.sampling_rate # (float64) 30000.0\nprobeF.device.probe_id # (int64)\nnwb.devices # (LabelledDict)\ndevices = nwb.devices\nprobeF = devices[\"probeF\"]\nprobeF # (EcephysProbe)\nprobeF.description # (str) Neuropixels 1.0 Probe\nprobeF.manufacturer # (str) imec\nprobeF.sampling_rate # (float64) 30000.0\nprobeF.probe_id # (int64)\nnwb.session_id # (str) 1300222049\nnwb.institution # (str) Allen Institute for Brain Science\nnwb.stimulus_notes # (str) OpenScopeVision2Hippocampus\nnwb.electrodes # (DynamicTable)\nnwb.electrodes.description # (str) metadata about extracellular electrodes\nnwb.electrodes.colnames # (tuple)\nnwb.electrodes.columns # (tuple)\nnwb.electrodes.id # (ElementIdentifiers)\n# nwb.electrodes.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 96 rows and 13 columns\n# nwb.electrodes.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nnwb.electrodes.location # (VectorData) the location of channel within the subject e.g. brain region\nnwb.electrodes.group # (VectorData) a reference to the ElectrodeGroup this electrode is a part of\nnwb.electrodes.group_name # (VectorData) the name of the ElectrodeGroup this electrode is a part of\nnwb.electrodes.probe_vertical_position # (VectorData) Length-wise position of electrode/channel on device (microns)\nnwb.electrodes.probe_horizontal_position # (VectorData) Width-wise position of electrode/channel on device (microns)\nnwb.electrodes.probe_id # (VectorData) The unique id of this electrode's/channel's device\nnwb.electrodes.local_index # (VectorData) The local index of electrode/channel on device\nnwb.electrodes.valid_data # (VectorData) Whether data from this electrode/channel is usable\nnwb.electrodes.x # (VectorData) the x coordinate of the position (+x is posterior)\nnwb.electrodes.y # (VectorData) the y coordinate of the position (+y is inferior)\nnwb.electrodes.z # (VectorData) the z coordinate of the position (+z is right)\nnwb.electrodes.imp # (VectorData) the impedance of the electrode, in ohms\nnwb.electrodes.filtering # (VectorData) description of hardware filtering, including the filter name and frequency cutoffs\nnwb.subject # (EcephysSpecimen)\nnwb.subject.age # (str) P89D\nnwb.subject.age__reference # (str) birth\nnwb.subject.genotype # (str) wt/wt\nnwb.subject.sex # (str) F\nnwb.subject.species # (str) Mus musculus\nnwb.subject.subject_id # (str) 692077\nnwb.subject.strain # (str) Transgenic\nnwb.subject.specimen_name # (str) 692077\nnwb.subject.age_in_days # (float64) 89.0"
      }
    ]
  },
  {
    "role": "user",
    "content": "Please summarize the above information in a jupytext notebook that introduces dandiset 000690 using the format described below.\n\nThe purpose of the notebook is to help researchers explore the dandiset and get started with a reanalysis.\n\nStart with an appropriate title for the notebook, such as \"Exploring Dandiset 000690: ...\" (that should be a markdown heading).\n\nInform the user that the notebook was generated with the assistance of AI, and that they should be cautious when interpreting the code or results.\n\nProvide an overview of the Dandiset. Include a link to the Dandiset of the form https://dandiarchive.org/dandiset/000690/0.250326.0015.\n\nSummarize what the notebook covers.\n\nList the packages that are required to run the notebook. Assume that these are already installed on the user's system. Do not include any pip install commands in the notebook.\n\nShow how to use the DANDI API to load assets.\n\nShow how to load and visualize data from the dandiset based on the above chat.\n\nGenerate all the good quality plots (without being redundant).\n\nLoad NWB data by streaming the remote file (as done in the chat) rather than downloading it.\n\nDo not use any functionality of pynwb or dandi that is not covered in the chat.\n\nYou should stick to material that is covered in the chat and do not hallucinate.\n\nThroughout the notebook, include explanatory markdown cells that guide the user through the process.\n\nThe notebook should be well-documented, and follow best practices Include comments in code cells to explain what each step does.\n\nThe Jupytext should use `# %% [markdown]` for markdown cells and `# %%` delimiters for the code cells.\n\nYour output should be in the format:\n\n<notebook>\nThe text of the jupytext notebook should appear here.\n</notebook>\n\nNo other text should be included in the response.\n"
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 000690: Allen Institute Openscope - Vision2Hippocampus Project\n#\n# *This notebook was generated with the assistance of AI. Please exercise caution when interpreting the code or results and verify them independently as necessary.*\n#\n# ---\n#\n# ## Overview\n#\n# This notebook introduces [Dandiset 000690 (Version 0.250326.0015)](https://dandiarchive.org/dandiset/000690/0.250326.0015), containing extensive extracellular electrophysiology data collected by the Allen Institute's Openscope Vision2Hippocampus project. The dataset comprises Neuropixels probe recordings from multiple brain regions in mice in response to a variety of visual stimuli, such as moving bars and naturalistic movies.\n#\n# Researchers can use this notebook to:\n# - Understand the structure and contents of a typical NWB file in this dandiset.\n# - Stream and examine file metadata and example data (such as LFPs and electrode locations) without downloading large files.\n# - Visualize electrode metadata and a sample of Local Field Potential (LFP) traces.\n#\n# ---\n#\n# ## What Does This Notebook Cover?\n#\n# 1. **Overview of the dataset and key metadata.**\n# 2. **How to use the DANDI API to find and stream data files.**\n# 3. **How to access NWB file contents for a representative probe.**\n# 4. **Visualization of electrode locations and LFP data samples.**\n#\n# ---\n#\n# ## Required Packages\n#\n# This notebook requires the following packages (assumed to be installed):\n#\n# - `dandi`\n# - `remfile`\n# - `h5py`\n# - `pynwb`\n# - `pandas`\n# - `numpy`\n# - `matplotlib`\n#\n\n# %% [markdown]\n# ## 1. Set Up and Load Required Libraries\n\n# %%\n# Import required libraries\nfrom dandi.dandiapi import DandiAPIClient\nimport remfile\nimport h5py\nimport pynwb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %% [markdown]\n# ## 2. Browse and Access Data Files in the Dandiset\n#\n# We'll connect to the DANDI Archive, select a sample NWB file, and explore its content.\n\n# %%\n# Connect to the DANDI Archive using the API client and list relevant assets\ndandiset_id = \"000690\"\nversion = \"0.250326.0015\"\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(dandiset_id, version)\n\n# List a few files in the Dandiset\nassets = [a for a in dandiset.get_assets()]\nprint(\"Number of files in this version:\", len(assets))\n\n# Print the first five NWB files in the Dandiset\nprint(\"\\nExample files:\")\nfor a in assets[:5]:\n    print(\"-\", a.path)\n\n# %% [markdown]\n# ## 3. Select a Representative NWB File and Stream It Remotely\n#\n# We'll focus on a representative probe LFP data file:\n# - `sub-692077/sub-692077_ses-1300222049_probe-4_ecephys.nwb`\n#\n# We access the file by streaming with `remfile`, avoiding the need to fully download the potentially large file.\n\n# %%\n# Select a representative file for detailed examination\ntarget_path = \"sub-692077/sub-692077_ses-1300222049_probe-4_ecephys.nwb\"\ntarget_asset = next(dandiset.get_assets_by_glob(target_path))\ndownload_url = target_asset.download_url\n\n# Open the NWB file via remote HDF5 streaming\nremote_file = remfile.File(download_url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file, load_namespaces=True)\nnwb = io.read()\n\nprint(\"Session description:\", nwb.session_description)\nprint(\"Session start time:\", nwb.session_start_time)\nprint(\"Subject:\", nwb.subject.subject_id, nwb.subject.species)\nprint(\"Institution:\", nwb.institution)\nprint(\"Recording probe groups:\", list(nwb.electrode_groups.keys()))\n\n# %% [markdown]\n# ## 4. Examine Electrode Metadata\n#\n# Each probe recording includes detailed electrode metadata. We'll convert the electrode table to a pandas DataFrame and display key information, such as channel locations within the brain and probe, impedance, and validation status.\n\n# %%\n# Access the electrode metadata table and convert it to a pandas DataFrame\nelectrodes_df = nwb.electrodes.to_dataframe()\nprint(\"Electrode metadata columns:\\n\", electrodes_df.columns.tolist())\n\n# Display the first few electrodes\nelectrodes_df.head()\n\n# %% [markdown]\n# ### Visualize Spatial Distribution of Electrodes on the Probe\n#\n# Here, we plot the vertical and horizontal positions of the electrodes to visualize their arrangement on the Neuropixels probe.\n\n# %%\nplt.figure(figsize=(6, 12))\nplt.scatter(\n    electrodes_df[\"probe_horizontal_position\"], \n    electrodes_df[\"probe_vertical_position\"], \n    c=electrodes_df[\"valid_data\"].astype(int), \n    cmap=\"coolwarm\",\n    label=\"Electrode\"\n)\nplt.xlabel(\"Horizontal Position (\u03bcm)\")\nplt.ylabel(\"Vertical Position (\u03bcm)\")\nplt.title(\"Probe Electrode Layout (Color = Valid Data)\")\nplt.colorbar(label=\"Valid (1) / Invalid (0)\")\nplt.gca().invert_yaxis()\nplt.show()\n\n# %% [markdown]\n# ## 5. Examine and Plot LFP Data\n#\n# The NWB file contains local field potential (LFP) data for 96 channels (electrodes). The dataset includes both voltage data and timestamp arrays.\n#\n# Let's plot a sample of the LFP traces for a short time window for the first few channels.\n\n# %%\n# Get LFP ElectricalSeries\nacquisition = nwb.acquisition\nprobe_4_lfp = acquisition[\"probe_4_lfp\"]  # LFP container\nlfp_series = probe_4_lfp.electrical_series[\"probe_4_lfp_data\"]\n\n# Read a small segment of the LFP data (first 5 seconds, first 8 channels)\nn_samples = 5 * 625   # 5 seconds at 625 Hz sampling rate\nn_channels = 8\n\nlfp_data = lfp_series.data[:n_samples, :n_channels]\nlfp_time = lfp_series.timestamps[:n_samples]\n\nplt.figure(figsize=(12, 6))\nfor ch in range(n_channels):\n    plt.plot(lfp_time, lfp_data[:, ch] * 1e3 + ch*2, label=f\"Ch {ch}\")  # Offset for visualization, convert V to mV\n\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"LFP (mV, vertically offset)\")\nplt.title(\"Sample LFP Traces from Probe 4 (First 8 Channels, First 5s)\")\nplt.yticks([])\nplt.legend(loc=\"upper right\")\nplt.show()\n\n# %% [markdown]\n# ## 6. Summary\n#\n# In this notebook, we explored Dandiset 000690 from the DANDI Archive, focusing on streaming and analyzing NWB files related to the Allen Institute Vision2Hippocampus project. We examined metadata, visualized electrode arrangements, and plotted example LFP data, providing a foundation for further, more detailed analyses.\n#\n# **For more information and to access the entire dataset, visit**: [https://dandiarchive.org/dandiset/000690/0.250326.0015](https://dandiarchive.org/dandiset/000690/0.250326.0015)\n#\n# ---\n#\n# *End of notebook. Please consult the official dataset documentation and code APIs for more advanced analyses and for accessing additional modalities (spike times, stimuli presentations, etc.).*\n</notebook>"
  }
]