[
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata about Dandiset 000617 version 0.250311.1615."
      },
      {
        "type": "text",
        "text": "{\n  \"id\": \"DANDI:000617/0.250311.1615\",\n  \"doi\": \"10.48324/dandi.000617/0.250311.1615\",\n  \"url\": \"https://dandiarchive.org/dandiset/000617/0.250311.1615\",\n  \"name\": \"Allen Institute Openscope - Sequence Learning Project\",\n  \"about\": [],\n  \"access\": [\n    {\n      \"status\": \"dandi:OpenAccess\",\n      \"schemaKey\": \"AccessRequirements\"\n    }\n  ],\n  \"license\": [\n    \"spdx:CC-BY-4.0\"\n  ],\n  \"version\": \"0.250311.1615\",\n  \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.4/context.json\",\n  \"citation\": \"Berry, Michael; Lecoq, J\\u00e9r\\u00f4me; Amaya, Avalon; Wilkes, Josh; Nguyen, Katrina; Naidoo, Robyn; Johnson, Tye; Han, Warren Han; Williford, Ali; Swapp, Jackie (2025) Allen Institute Openscope - Sequence Learning Project (Version 0.250311.1615) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.000617/0.250311.1615\",\n  \"keywords\": [\n    \"L2/3\",\n    \"L4\",\n    \"two-photon calcium imaging\",\n    \"prediction\",\n    \"pyramidal neurons\",\n    \"somata\",\n    \"neocortex\",\n    \"predictive coding\",\n    \"predictive processing\",\n    \"sequence learning\",\n    \"movies\"\n  ],\n  \"protocol\": [],\n  \"schemaKey\": \"Dandiset\",\n  \"identifier\": \"DANDI:000617\",\n  \"repository\": \"https://dandiarchive.org\",\n  \"contributor\": [\n    {\n      \"name\": \"National Institutes of Health (NIH)\",\n      \"roleName\": [\n        \"dcite:Funder\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"awardNumber\": \"1 U24 NS 113646-02\"\n    },\n    {\n      \"url\": \"https://alleninstitute.org/\",\n      \"name\": \"Allen Institute\",\n      \"roleName\": [\n        \"dcite:Producer\",\n        \"dcite:Funder\",\n        \"dcite:Affiliation\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"identifier\": \"https://ror.org/03cpe7c52\",\n      \"includeInCitation\": false\n    },\n    {\n      \"name\": \"Berry, Michael\",\n      \"email\": \"berry@princeton.edu\",\n      \"roleName\": [\n        \"dcite:ContactPerson\",\n        \"dcite:Conceptualization\",\n        \"dcite:Author\",\n        \"dcite:Investigation\",\n        \"dcite:ProjectLeader\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-4133-7999\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Lecoq, J\\u00e9r\\u00f4me\",\n      \"email\": \"jeromel@alleninstitute.org\",\n      \"roleName\": [\n        \"dcite:ContactPerson\",\n        \"dcite:DataManager\",\n        \"dcite:FundingAcquisition\",\n        \"dcite:Maintainer\",\n        \"dcite:ProjectManager\",\n        \"dcite:ProjectAdministration\",\n        \"dcite:Software\",\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-0131-0938\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Amaya, Avalon\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-2274-2892\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Wilkes, Josh\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0005-4858-2219\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Nguyen, Katrina\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0009-8547-9703\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Naidoo, Robyn\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0004-0996-8808\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Johnson, Tye\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-7239-7571\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Han, Warren Han\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-0104-0380\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Williford, Ali\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-7282-7515\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Swapp, Jackie\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0008-4965-6242\",\n      \"includeInCitation\": true\n    }\n  ],\n  \"dateCreated\": \"2023-07-28T21:00:57.366907+00:00\",\n  \"description\": \"Adaptive and coordinated behavior requires that an animal be able to make predictions about the near and even far future. This intuition that some neural computations should be \\u2018predictive\\u2019 in their character has a long history, starting with ideas about how the receptive field structure of retinal ganglion cells relate to the statistics of natural visual scenes. Ideas about predictive computation have been most influential in thinking about the function of the neocortex. Here, the relatively stereotyped local circuitry of the neocortex has long led to speculation that each local circuit might be carrying out a somewhat similar, fundamental computation on its specific inputs. In addition, the organization of sensory-motor pathways into hierarchies (e.g., V1, V2, V4, IT in the ventral visual stream) with stereotyped feedforward and feedback connections has motivated ideas about hierarchical predictive codes, where higher levels of the hierarchy send predictions down to the lower level that then compares its inputs against the predictions and only send the surprises up the hierarchy (such as in the work of Mumford, Rao & Ballard, and Friston). Despite the wide influence of ideas about predictive coding, there is relatively little experimental evidence that such computations occur in multiple cortical areas, perhaps serving as a \\u2018canonical computation\\u2019 of the neocortical microcircuit. Our experimental design is based on a Sequence Learning Experiment, in which head-fixed mice passively view sequences of three different natural movie clips (labeled \\u2018A\\u2019, \\u2018B\\u2019, \\u2018C\\u2019), each having a duration of 2 seconds. We begin with one recording session (day #0), where the movie clips are presented in random order along with a 2 second grey screen (labeled \\u2018X\\u2019). Each stimulus occurs a total of 525 times, allowing a thorough characterization of neural responses before any sequence learning has occurred. Next, there are 3 recording sessions where the three movie clips are presented in a repeating temporal sequence, ABCABC\\u2026, for 500 times, in order to train the mouse\\u2019s brain. This training allows the mouse to potentially use the identity of the current movie clip predict the next movie clip. In addition, each sequence training session includes a period of random-order presentation, in order to assess changes in neural tuning during sequence learning. Finally, our last session (day #4) had stimuli presented in random order, allowing us to test more thoroughly how responses changed due to sequence learning.\\n\\nOur design uses 2-photon microscopy with eight simultaneously recorded fields-of-view. The fields-of-view will include both layer 2/3 and layer 4 as well as from multiple cortical areas: V1 (VISp), LM (VISl), AM (VISam), and PM (VISpm). The experiment used the Cux2-CreERTS2:Camk2a-tTa; Ai93(TITL-GCaMP6f) mouse line, which has expression in excitatory neurons of both layer 4 and 2/3.\",\n  \"publishedBy\": {\n    \"id\": \"urn:uuid:d9375de5-739f-4da0-bfbf-176fc6695b08\",\n    \"name\": \"DANDI publish\",\n    \"endDate\": \"2025-03-11T16:15:16.112223+00:00\",\n    \"schemaKey\": \"PublishActivity\",\n    \"startDate\": \"2025-03-11T16:15:16.112223+00:00\",\n    \"wasAssociatedWith\": [\n      {\n        \"id\": \"urn:uuid:57b72d14-f227-4486-bbd6-9c2ae3663b85\",\n        \"name\": \"DANDI API\",\n        \"version\": \"0.1.0\",\n        \"schemaKey\": \"Software\",\n        \"identifier\": \"RRID:SCR_017571\"\n      }\n    ]\n  },\n  \"studyTarget\": [],\n  \"assetsSummary\": {\n    \"species\": [\n      {\n        \"name\": \"Mus musculus - House mouse\",\n        \"schemaKey\": \"SpeciesType\",\n        \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n      }\n    ],\n    \"approach\": [\n      {\n        \"name\": \"microscopy approach; cell population imaging\",\n        \"schemaKey\": \"ApproachType\"\n      }\n    ],\n    \"schemaKey\": \"AssetsSummary\",\n    \"dataStandard\": [\n      {\n        \"name\": \"Neurodata Without Borders (NWB)\",\n        \"schemaKey\": \"StandardsType\",\n        \"identifier\": \"RRID:SCR_015242\"\n      }\n    ],\n    \"numberOfBytes\": 13197788224949,\n    \"numberOfFiles\": 1197,\n    \"numberOfSubjects\": 13,\n    \"variableMeasured\": [\n      \"OpticalChannel\",\n      \"PlaneSegmentation\",\n      \"ProcessingModule\",\n      \"ImagingPlane\",\n      \"TwoPhotonSeries\"\n    ],\n    \"measurementTechnique\": [\n      {\n        \"name\": \"analytical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"two-photon microscopy technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"surgical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      }\n    ]\n  },\n  \"datePublished\": \"2025-03-11T16:15:16.112223+00:00\",\n  \"schemaVersion\": \"0.6.4\",\n  \"ethicsApproval\": [\n    {\n      \"schemaKey\": \"EthicsApproval\",\n      \"identifier\": \"\\u200bAllen Institute Institutional Animal Care and Use Committee (IACUC), protocol 2117\",\n      \"contactPoint\": {\n        \"schemaKey\": \"ContactPoint\"\n      }\n    }\n  ],\n  \"wasGeneratedBy\": [],\n  \"acknowledgement\": \"We thank the Allen Institute founder, Paul G. Allen, for his vision, encouragement, and support.\",\n  \"relatedResource\": [],\n  \"manifestLocation\": [\n    \"https://dandiarchive.s3.amazonaws.com/dandisets/000617/0.250311.1615/assets.yaml\"\n  ]\n}"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is a listing of some of the files in the Dandiset."
      },
      {
        "type": "text",
        "text": "sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294084428-acq-1294179943-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914859-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294084428-acq-1294179942-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370666-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370664-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914856-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370663-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914853-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914860-raw-movies_ophys.nwb"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata and usage information about the file sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb."
      },
      {
        "type": "text",
        "text": "\nimport pynwb\nimport h5py\nimport remfile\n\n# Load\nfrom dandi.dandiapi import DandiAPIClient\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250311.1615\")\nurl = next(dandiset.get_assets_by_glob(\"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\")).download_url\n\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\nnwb # (NWBFile)\nnwb.session_description # (str) Ophys Session\nnwb.identifier # (str) 1294179945\nnwb.session_start_time # (datetime) 2023-08-31T20:49:56.751000+00:00\nnwb.timestamps_reference_time # (datetime) 2023-08-31T20:49:56.751000+00:00\nnwb.file_create_date # (list) [datetime.datetime(2024, 2, 8, 18, 6, 8, 800829, tzinfo=tzlocal())]\nnwb.acquisition # (LabelledDict)\nacquisition = nwb.acquisition\nEyeTracking = acquisition[\"EyeTracking\"]\nEyeTracking # (EllipseEyeTracking)\nEyeTracking.spatial_series # (LabelledDict)\nspatial_series = EyeTracking.spatial_series\ncorneal_reflection_tracking = spatial_series[\"corneal_reflection_tracking\"]\ncorneal_reflection_tracking # (EllipseSeries)\ncorneal_reflection_tracking.resolution # (float64) -1.0\ncorneal_reflection_tracking.comments # (str) no comments\ncorneal_reflection_tracking.description # (str) no description\ncorneal_reflection_tracking.conversion # (float64) 1.0\ncorneal_reflection_tracking.offset # (float) 0.0\ncorneal_reflection_tracking.unit # (str) meters\ncorneal_reflection_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# corneal_reflection_tracking.data[:, :] # Access all data\n# corneal_reflection_tracking.data[0:n, :] # Access first n rows\n# corneal_reflection_tracking.data[:, 0:n] # Access first n columns\ncorneal_reflection_tracking.timestamps_unit # (str) seconds\ncorneal_reflection_tracking.interval # (int) 1\ncorneal_reflection_tracking.reference_frame # (str) nose\ncorneal_reflection_tracking.area # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.area[:] # Access all data\n# corneal_reflection_tracking.area[0:n] # Access first n elements\ncorneal_reflection_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.area_raw[:] # Access all data\n# corneal_reflection_tracking.area_raw[0:n] # Access first n elements\ncorneal_reflection_tracking.width # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.width[:] # Access all data\n# corneal_reflection_tracking.width[0:n] # Access first n elements\ncorneal_reflection_tracking.height # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.height[:] # Access all data\n# corneal_reflection_tracking.height[0:n] # Access first n elements\ncorneal_reflection_tracking.angle # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.angle[:] # Access all data\n# corneal_reflection_tracking.angle[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps # (EllipseSeries)\ncorneal_reflection_tracking.timestamps.resolution # (float64) -1.0\ncorneal_reflection_tracking.timestamps.comments # (str) no comments\ncorneal_reflection_tracking.timestamps.description # (str) no description\ncorneal_reflection_tracking.timestamps.conversion # (float64) 1.0\ncorneal_reflection_tracking.timestamps.offset # (float) 0.0\ncorneal_reflection_tracking.timestamps.unit # (str) meters\ncorneal_reflection_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# corneal_reflection_tracking.timestamps.data[:, :] # Access all data\n# corneal_reflection_tracking.timestamps.data[0:n, :] # Access first n rows\n# corneal_reflection_tracking.timestamps.data[:, 0:n] # Access first n columns\ncorneal_reflection_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.timestamps[:] # Access all data\n# corneal_reflection_tracking.timestamps.timestamps[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.timestamps_unit # (str) seconds\ncorneal_reflection_tracking.timestamps.interval # (int) 1\ncorneal_reflection_tracking.timestamps.reference_frame # (str) nose\ncorneal_reflection_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.area[:] # Access all data\n# corneal_reflection_tracking.timestamps.area[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.area_raw[:] # Access all data\n# corneal_reflection_tracking.timestamps.area_raw[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.width[:] # Access all data\n# corneal_reflection_tracking.timestamps.width[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.height[:] # Access all data\n# corneal_reflection_tracking.timestamps.height[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.angle[:] # Access all data\n# corneal_reflection_tracking.timestamps.angle[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.timestamp_link # (list)\neye_tracking = spatial_series[\"eye_tracking\"]\neye_tracking # (EllipseSeries)\neye_tracking.resolution # (float64) -1.0\neye_tracking.comments # (str) no comments\neye_tracking.description # (str) no description\neye_tracking.conversion # (float64) 1.0\neye_tracking.offset # (float) 0.0\neye_tracking.unit # (str) meters\neye_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# eye_tracking.data[:, :] # Access all data\n# eye_tracking.data[0:n, :] # Access first n rows\n# eye_tracking.data[:, 0:n] # Access first n columns\neye_tracking.timestamps # (Dataset) shape (254226,); dtype float64\n# eye_tracking.timestamps[:] # Access all data\n# eye_tracking.timestamps[0:n] # Access first n elements\neye_tracking.timestamps_unit # (str) seconds\neye_tracking.interval # (int) 1\neye_tracking.reference_frame # (str) nose\neye_tracking.area # (Dataset) shape (254226,); dtype float64\n# eye_tracking.area[:] # Access all data\n# eye_tracking.area[0:n] # Access first n elements\neye_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# eye_tracking.area_raw[:] # Access all data\n# eye_tracking.area_raw[0:n] # Access first n elements\neye_tracking.width # (Dataset) shape (254226,); dtype float64\n# eye_tracking.width[:] # Access all data\n# eye_tracking.width[0:n] # Access first n elements\neye_tracking.height # (Dataset) shape (254226,); dtype float64\n# eye_tracking.height[:] # Access all data\n# eye_tracking.height[0:n] # Access first n elements\neye_tracking.angle # (Dataset) shape (254226,); dtype float64\n# eye_tracking.angle[:] # Access all data\n# eye_tracking.angle[0:n] # Access first n elements\neye_tracking.timestamp_link # (list)\npupil_tracking = spatial_series[\"pupil_tracking\"]\npupil_tracking # (EllipseSeries)\npupil_tracking.resolution # (float64) -1.0\npupil_tracking.comments # (str) no comments\npupil_tracking.description # (str) no description\npupil_tracking.conversion # (float64) 1.0\npupil_tracking.offset # (float) 0.0\npupil_tracking.unit # (str) meters\npupil_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# pupil_tracking.data[:, :] # Access all data\n# pupil_tracking.data[0:n, :] # Access first n rows\n# pupil_tracking.data[:, 0:n] # Access first n columns\npupil_tracking.timestamps_unit # (str) seconds\npupil_tracking.interval # (int) 1\npupil_tracking.reference_frame # (str) nose\npupil_tracking.area # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.area[:] # Access all data\n# pupil_tracking.area[0:n] # Access first n elements\npupil_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.area_raw[:] # Access all data\n# pupil_tracking.area_raw[0:n] # Access first n elements\npupil_tracking.width # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.width[:] # Access all data\n# pupil_tracking.width[0:n] # Access first n elements\npupil_tracking.height # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.height[:] # Access all data\n# pupil_tracking.height[0:n] # Access first n elements\npupil_tracking.angle # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.angle[:] # Access all data\n# pupil_tracking.angle[0:n] # Access first n elements\npupil_tracking.timestamps # (EllipseSeries)\npupil_tracking.timestamps.resolution # (float64) -1.0\npupil_tracking.timestamps.comments # (str) no comments\npupil_tracking.timestamps.description # (str) no description\npupil_tracking.timestamps.conversion # (float64) 1.0\npupil_tracking.timestamps.offset # (float) 0.0\npupil_tracking.timestamps.unit # (str) meters\npupil_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# pupil_tracking.timestamps.data[:, :] # Access all data\n# pupil_tracking.timestamps.data[0:n, :] # Access first n rows\n# pupil_tracking.timestamps.data[:, 0:n] # Access first n columns\npupil_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.timestamps[:] # Access all data\n# pupil_tracking.timestamps.timestamps[0:n] # Access first n elements\npupil_tracking.timestamps.timestamps_unit # (str) seconds\npupil_tracking.timestamps.interval # (int) 1\npupil_tracking.timestamps.reference_frame # (str) nose\npupil_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.area[:] # Access all data\n# pupil_tracking.timestamps.area[0:n] # Access first n elements\npupil_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.area_raw[:] # Access all data\n# pupil_tracking.timestamps.area_raw[0:n] # Access first n elements\npupil_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.width[:] # Access all data\n# pupil_tracking.timestamps.width[0:n] # Access first n elements\npupil_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.height[:] # Access all data\n# pupil_tracking.timestamps.height[0:n] # Access first n elements\npupil_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.angle[:] # Access all data\n# pupil_tracking.timestamps.angle[0:n] # Access first n elements\npupil_tracking.timestamps.timestamp_link # (list)\nEyeTracking.eye_tracking # (EllipseSeries)\nEyeTracking.eye_tracking.resolution # (float64) -1.0\nEyeTracking.eye_tracking.comments # (str) no comments\nEyeTracking.eye_tracking.description # (str) no description\nEyeTracking.eye_tracking.conversion # (float64) 1.0\nEyeTracking.eye_tracking.offset # (float) 0.0\nEyeTracking.eye_tracking.unit # (str) meters\nEyeTracking.eye_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.eye_tracking.data[:, :] # Access all data\n# EyeTracking.eye_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.eye_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.eye_tracking.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.timestamps[:] # Access all data\n# EyeTracking.eye_tracking.timestamps[0:n] # Access first n elements\nEyeTracking.eye_tracking.timestamps_unit # (str) seconds\nEyeTracking.eye_tracking.interval # (int) 1\nEyeTracking.eye_tracking.reference_frame # (str) nose\nEyeTracking.eye_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.area[:] # Access all data\n# EyeTracking.eye_tracking.area[0:n] # Access first n elements\nEyeTracking.eye_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.area_raw[:] # Access all data\n# EyeTracking.eye_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.eye_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.width[:] # Access all data\n# EyeTracking.eye_tracking.width[0:n] # Access first n elements\nEyeTracking.eye_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.height[:] # Access all data\n# EyeTracking.eye_tracking.height[0:n] # Access first n elements\nEyeTracking.eye_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.angle[:] # Access all data\n# EyeTracking.eye_tracking.angle[0:n] # Access first n elements\nEyeTracking.eye_tracking.timestamp_link # (list)\nEyeTracking.pupil_tracking # (EllipseSeries)\nEyeTracking.pupil_tracking.resolution # (float64) -1.0\nEyeTracking.pupil_tracking.comments # (str) no comments\nEyeTracking.pupil_tracking.description # (str) no description\nEyeTracking.pupil_tracking.conversion # (float64) 1.0\nEyeTracking.pupil_tracking.offset # (float) 0.0\nEyeTracking.pupil_tracking.unit # (str) meters\nEyeTracking.pupil_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.pupil_tracking.data[:, :] # Access all data\n# EyeTracking.pupil_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.pupil_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.pupil_tracking.timestamps_unit # (str) seconds\nEyeTracking.pupil_tracking.interval # (int) 1\nEyeTracking.pupil_tracking.reference_frame # (str) nose\nEyeTracking.pupil_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.area[:] # Access all data\n# EyeTracking.pupil_tracking.area[0:n] # Access first n elements\nEyeTracking.pupil_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.area_raw[:] # Access all data\n# EyeTracking.pupil_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.pupil_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.width[:] # Access all data\n# EyeTracking.pupil_tracking.width[0:n] # Access first n elements\nEyeTracking.pupil_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.height[:] # Access all data\n# EyeTracking.pupil_tracking.height[0:n] # Access first n elements\nEyeTracking.pupil_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.angle[:] # Access all data\n# EyeTracking.pupil_tracking.angle[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps # (EllipseSeries)\nEyeTracking.pupil_tracking.timestamps.resolution # (float64) -1.0\nEyeTracking.pupil_tracking.timestamps.comments # (str) no comments\nEyeTracking.pupil_tracking.timestamps.description # (str) no description\nEyeTracking.pupil_tracking.timestamps.conversion # (float64) 1.0\nEyeTracking.pupil_tracking.timestamps.offset # (float) 0.0\nEyeTracking.pupil_tracking.timestamps.unit # (str) meters\nEyeTracking.pupil_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.pupil_tracking.timestamps.data[:, :] # Access all data\n# EyeTracking.pupil_tracking.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.pupil_tracking.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.pupil_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.timestamps[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.timestamps_unit # (str) seconds\nEyeTracking.pupil_tracking.timestamps.interval # (int) 1\nEyeTracking.pupil_tracking.timestamps.reference_frame # (str) nose\nEyeTracking.pupil_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.area[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.area[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.area_raw[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.width[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.width[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.height[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.height[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.angle[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.angle[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.timestamp_link # (list)\nEyeTracking.corneal_reflection_tracking # (EllipseSeries)\nEyeTracking.corneal_reflection_tracking.resolution # (float64) -1.0\nEyeTracking.corneal_reflection_tracking.comments # (str) no comments\nEyeTracking.corneal_reflection_tracking.description # (str) no description\nEyeTracking.corneal_reflection_tracking.conversion # (float64) 1.0\nEyeTracking.corneal_reflection_tracking.offset # (float) 0.0\nEyeTracking.corneal_reflection_tracking.unit # (str) meters\nEyeTracking.corneal_reflection_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.corneal_reflection_tracking.data[:, :] # Access all data\n# EyeTracking.corneal_reflection_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.corneal_reflection_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.corneal_reflection_tracking.timestamps_unit # (str) seconds\nEyeTracking.corneal_reflection_tracking.interval # (int) 1\nEyeTracking.corneal_reflection_tracking.reference_frame # (str) nose\nEyeTracking.corneal_reflection_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.area[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.area[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.area_raw[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.width[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.width[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.height[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.height[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.angle[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.angle[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps # (EllipseSeries)\nEyeTracking.corneal_reflection_tracking.timestamps.resolution # (float64) -1.0\nEyeTracking.corneal_reflection_tracking.timestamps.comments # (str) no comments\nEyeTracking.corneal_reflection_tracking.timestamps.description # (str) no description\nEyeTracking.corneal_reflection_tracking.timestamps.conversion # (float64) 1.0\nEyeTracking.corneal_reflection_tracking.timestamps.offset # (float) 0.0\nEyeTracking.corneal_reflection_tracking.timestamps.unit # (str) meters\nEyeTracking.corneal_reflection_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.data[:, :] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.corneal_reflection_tracking.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.corneal_reflection_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.timestamps[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.timestamps_unit # (str) seconds\nEyeTracking.corneal_reflection_tracking.timestamps.interval # (int) 1\nEyeTracking.corneal_reflection_tracking.timestamps.reference_frame # (str) nose\nEyeTracking.corneal_reflection_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.area[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.area[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.area_raw[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.width[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.width[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.height[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.height[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.angle[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.angle[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.timestamp_link # (list)\nEyeTracking.likely_blink # (TimeSeries)\nEyeTracking.likely_blink.resolution # (float64) -1.0\nEyeTracking.likely_blink.comments # (str) no comments\nEyeTracking.likely_blink.description # (str) blinks\nEyeTracking.likely_blink.conversion # (float64) 1.0\nEyeTracking.likely_blink.offset # (float) 0.0\nEyeTracking.likely_blink.unit # (str) N/A\nEyeTracking.likely_blink.data # (Dataset) shape (254226,); dtype bool\n# EyeTracking.likely_blink.data[:] # Access all data\n# EyeTracking.likely_blink.data[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps_unit # (str) seconds\nEyeTracking.likely_blink.interval # (int) 1\nEyeTracking.likely_blink.timestamps # (EllipseSeries)\nEyeTracking.likely_blink.timestamps.resolution # (float64) -1.0\nEyeTracking.likely_blink.timestamps.comments # (str) no comments\nEyeTracking.likely_blink.timestamps.description # (str) no description\nEyeTracking.likely_blink.timestamps.conversion # (float64) 1.0\nEyeTracking.likely_blink.timestamps.offset # (float) 0.0\nEyeTracking.likely_blink.timestamps.unit # (str) meters\nEyeTracking.likely_blink.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.likely_blink.timestamps.data[:, :] # Access all data\n# EyeTracking.likely_blink.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.likely_blink.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.likely_blink.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.timestamps[:] # Access all data\n# EyeTracking.likely_blink.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.timestamps_unit # (str) seconds\nEyeTracking.likely_blink.timestamps.interval # (int) 1\nEyeTracking.likely_blink.timestamps.reference_frame # (str) nose\nEyeTracking.likely_blink.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.area[:] # Access all data\n# EyeTracking.likely_blink.timestamps.area[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.area_raw[:] # Access all data\n# EyeTracking.likely_blink.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.width[:] # Access all data\n# EyeTracking.likely_blink.timestamps.width[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.height[:] # Access all data\n# EyeTracking.likely_blink.timestamps.height[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.angle[:] # Access all data\n# EyeTracking.likely_blink.timestamps.angle[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.timestamp_link # (list)\nraw_suite2p_motion_corrected = acquisition[\"raw_suite2p_motion_corrected\"]\nraw_suite2p_motion_corrected # (TwoPhotonSeries)\nraw_suite2p_motion_corrected.starting_time # (float64) 0.0\nraw_suite2p_motion_corrected.rate # (float64) 10.71\nraw_suite2p_motion_corrected.resolution # (float64) -1.0\nraw_suite2p_motion_corrected.comments # (str) no comments\nraw_suite2p_motion_corrected.description # (str) no description\nraw_suite2p_motion_corrected.conversion # (float64) 1.0\nraw_suite2p_motion_corrected.offset # (float) 0.0\nraw_suite2p_motion_corrected.unit # (str) SIunit\nraw_suite2p_motion_corrected.data # (Dataset) shape (40018, 512, 512); dtype int16\n# raw_suite2p_motion_corrected.data[:, :, :] # Access all data\n# raw_suite2p_motion_corrected.data[0, :, :] # Access first plane\nraw_suite2p_motion_corrected.starting_time_unit # (str) seconds\nraw_suite2p_motion_corrected.format # (str) raw\nraw_suite2p_motion_corrected.imaging_plane # (ImagingPlane)\nraw_suite2p_motion_corrected.imaging_plane.optical_channel # (list)\nraw_suite2p_motion_corrected.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\nraw_suite2p_motion_corrected.imaging_plane.excitation_lambda # (float64) 910.0\nraw_suite2p_motion_corrected.imaging_plane.imaging_rate # (float64) 10.0\nraw_suite2p_motion_corrected.imaging_plane.indicator # (str) GCaMP6f\nraw_suite2p_motion_corrected.imaging_plane.location # (str) VISpm\nraw_suite2p_motion_corrected.imaging_plane.conversion # (float) 1.0\nraw_suite2p_motion_corrected.imaging_plane.unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.origin_coords_unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.grid_spacing_unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.device # (Device)\nraw_suite2p_motion_corrected.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nv_in = acquisition[\"v_in\"]\nv_in # (TimeSeries)\nv_in.resolution # (float64) -1.0\nv_in.comments # (str) no comments\nv_in.description # (str) The theoretical maximum voltage that the running wheel encoder will reach prior to \"wrapping\". Th...\nv_in.conversion # (float64) 1.0\nv_in.offset # (float) 0.0\nv_in.unit # (str) V\nv_in.data # (Dataset) shape (252000,); dtype float64\n# v_in.data[:] # Access all data\n# v_in.data[0:n] # Access first n elements\nv_in.timestamps # (Dataset) shape (252000,); dtype float64\n# v_in.timestamps[:] # Access all data\n# v_in.timestamps[0:n] # Access first n elements\nv_in.timestamps_unit # (str) seconds\nv_in.interval # (int) 1\nv_sig = acquisition[\"v_sig\"]\nv_sig # (TimeSeries)\nv_sig.resolution # (float64) -1.0\nv_sig.comments # (str) no comments\nv_sig.description # (str) Voltage signal from the running wheel encoder\nv_sig.conversion # (float64) 1.0\nv_sig.offset # (float) 0.0\nv_sig.unit # (str) V\nv_sig.data # (Dataset) shape (252000,); dtype float64\n# v_sig.data[:] # Access all data\n# v_sig.data[0:n] # Access first n elements\nv_sig.timestamps # (Dataset) shape (252000,); dtype float64\n# v_sig.timestamps[:] # Access all data\n# v_sig.timestamps[0:n] # Access first n elements\nv_sig.timestamps_unit # (str) seconds\nv_sig.interval # (int) 1\nnwb.keywords # (StrDataset) shape (5,); dtype object\n# nwb.keywords[:] # Access all data\n# nwb.keywords[0:n] # Access first n elements\n# First few values of nwb.keywords: ['2-photon' 'calcium imaging' 'visual cortex' 'behavior' 'task']\nnwb.processing # (LabelledDict)\nprocessing = nwb.processing\nophys = processing[\"ophys\"]\nophys # (ProcessingModule)\nophys.description # (str) Ophys processing module\nophys.data_interfaces # (LabelledDict)\ndata_interfaces = ophys.data_interfaces\ncorrected_fluorescence = data_interfaces[\"corrected_fluorescence\"]\ncorrected_fluorescence # (Fluorescence)\ncorrected_fluorescence.roi_response_series # (LabelledDict)\nroi_response_series = corrected_fluorescence.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\ndff = data_interfaces[\"dff\"]\ndff # (DfOverF)\ndff.roi_response_series # (LabelledDict)\nroi_response_series = dff.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\nevent_detection = data_interfaces[\"event_detection\"]\nevent_detection # (OphysEventDetection)\nevent_detection.resolution # (float64) -1.0\nevent_detection.comments # (str) no comments\nevent_detection.description # (str) no description\nevent_detection.conversion # (float64) 1.0\nevent_detection.offset # (float) 0.0\nevent_detection.unit # (str) N/A\nevent_detection.data # (Dataset) shape (40018, 18); dtype float64\n# event_detection.data[:, :] # Access all data\n# event_detection.data[0:n, :] # Access first n rows\n# event_detection.data[:, 0:n] # Access first n columns\nevent_detection.timestamps # (Dataset) shape (40018,); dtype float64\n# event_detection.timestamps[:] # Access all data\n# event_detection.timestamps[0:n] # Access first n elements\nevent_detection.timestamps_unit # (str) seconds\nevent_detection.interval # (int) 1\nevent_detection.lambdas # (Dataset) shape (18,); dtype float64\n# event_detection.lambdas[:] # Access all data\n# event_detection.lambdas[0:n] # Access first n elements\n# First few values of event_detection.lambdas: [0.0287 0.0128 0.0086 0.0289 0.0335 0.0079 0.0203 0.0221 0.0168 0.0097]\nevent_detection.noise_stds # (Dataset) shape (18,); dtype float64\n# event_detection.noise_stds[:] # Access all data\n# event_detection.noise_stds[0:n] # Access first n elements\n# First few values of event_detection.noise_stds: [0.06418523 0.04296032 0.03497644 0.06435245 0.06978224 0.03365562  0.05395723 0.0563328  0.04906347 0.03729971]\nevent_detection.rois # (DynamicTableRegion)\nevent_detection.rois.description # (str) Cells with detected events\nevent_detection.rois.table # (PlaneSegmentation)\nevent_detection.rois.table.description # (str) Segmented rois\nevent_detection.rois.table.colnames # (tuple)\nevent_detection.rois.table.columns # (tuple)\nevent_detection.rois.table.id # (ElementIdentifiers)\nevent_detection.rois.table.imaging_plane # (ImagingPlane)\nevent_detection.rois.table.imaging_plane.optical_channel # (list)\nevent_detection.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\nevent_detection.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\nevent_detection.rois.table.imaging_plane.imaging_rate # (float64) 10.0\nevent_detection.rois.table.imaging_plane.indicator # (str) GCaMP6f\nevent_detection.rois.table.imaging_plane.location # (str) VISpm\nevent_detection.rois.table.imaging_plane.conversion # (float) 1.0\nevent_detection.rois.table.imaging_plane.unit # (str) meters\nevent_detection.rois.table.imaging_plane.origin_coords_unit # (str) meters\nevent_detection.rois.table.imaging_plane.grid_spacing_unit # (str) meters\nevent_detection.rois.table.imaging_plane.device # (Device)\nevent_detection.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# event_detection.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# event_detection.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nevent_detection.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\nevent_detection.rois.table.height # (VectorData) Height of ROI in pixels\nevent_detection.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\nevent_detection.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\nevent_detection.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\nevent_detection.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\nevent_detection.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\nevent_detection.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\nevent_detection.rois.table.width # (VectorData) Width of ROI in pixels\nevent_detection.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\nevent_detection.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\nevent_detection.rois.table.image_mask # (VectorData) Image masks for each ROI\nimage_segmentation = data_interfaces[\"image_segmentation\"]\nimage_segmentation # (ImageSegmentation)\nimage_segmentation.plane_segmentations # (LabelledDict)\nplane_segmentations = image_segmentation.plane_segmentations\ncell_specimen_table = plane_segmentations[\"cell_specimen_table\"]\ncell_specimen_table # (PlaneSegmentation)\ncell_specimen_table.description # (str) Segmented rois\ncell_specimen_table.colnames # (tuple)\ncell_specimen_table.columns # (tuple)\ncell_specimen_table.id # (ElementIdentifiers)\ncell_specimen_table.imaging_plane # (ImagingPlane)\ncell_specimen_table.imaging_plane.optical_channel # (list)\ncell_specimen_table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ncell_specimen_table.imaging_plane.excitation_lambda # (float64) 910.0\ncell_specimen_table.imaging_plane.imaging_rate # (float64) 10.0\ncell_specimen_table.imaging_plane.indicator # (str) GCaMP6f\ncell_specimen_table.imaging_plane.location # (str) VISpm\ncell_specimen_table.imaging_plane.conversion # (float) 1.0\ncell_specimen_table.imaging_plane.unit # (str) meters\ncell_specimen_table.imaging_plane.origin_coords_unit # (str) meters\ncell_specimen_table.imaging_plane.grid_spacing_unit # (str) meters\ncell_specimen_table.imaging_plane.device # (Device)\ncell_specimen_table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# cell_specimen_table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# cell_specimen_table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ncell_specimen_table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ncell_specimen_table.height # (VectorData) Height of ROI in pixels\ncell_specimen_table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ncell_specimen_table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ncell_specimen_table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ncell_specimen_table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ncell_specimen_table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ncell_specimen_table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ncell_specimen_table.width # (VectorData) Width of ROI in pixels\ncell_specimen_table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ncell_specimen_table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ncell_specimen_table.image_mask # (VectorData) Image masks for each ROI\nimages = data_interfaces[\"images\"]\nimages # (Images)\nimages.description # (str) no description\nimages.images # (LabelledDict)\nimages_1 = images.images\naverage_image = images_1[\"average_image\"]\naverage_image # (GrayscaleImage)\nmax_projection = images_1[\"max_projection\"]\nmax_projection # (GrayscaleImage)\nsegmentation_mask_image = images_1[\"segmentation_mask_image\"]\nsegmentation_mask_image # (GrayscaleImage)\nneuropil_trace = data_interfaces[\"neuropil_trace\"]\nneuropil_trace # (Fluorescence)\nneuropil_trace.roi_response_series # (LabelledDict)\nroi_response_series = neuropil_trace.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\nophys_motion_correction_x = data_interfaces[\"ophys_motion_correction_x\"]\nophys_motion_correction_x # (TimeSeries)\nophys_motion_correction_x.resolution # (float64) -1.0\nophys_motion_correction_x.comments # (str) no comments\nophys_motion_correction_x.description # (str) no description\nophys_motion_correction_x.conversion # (float64) 1.0\nophys_motion_correction_x.offset # (float) 0.0\nophys_motion_correction_x.unit # (str) pixels\nophys_motion_correction_x.data # (Dataset) shape (40018,); dtype int64\n# ophys_motion_correction_x.data[:] # Access all data\n# ophys_motion_correction_x.data[0:n] # Access first n elements\nophys_motion_correction_x.timestamps # (Dataset) shape (40018,); dtype float64\n# ophys_motion_correction_x.timestamps[:] # Access all data\n# ophys_motion_correction_x.timestamps[0:n] # Access first n elements\nophys_motion_correction_x.timestamps_unit # (str) seconds\nophys_motion_correction_x.interval # (int) 1\nophys_motion_correction_y = data_interfaces[\"ophys_motion_correction_y\"]\nophys_motion_correction_y # (TimeSeries)\nophys_motion_correction_y.resolution # (float64) -1.0\nophys_motion_correction_y.comments # (str) no comments\nophys_motion_correction_y.description # (str) no description\nophys_motion_correction_y.conversion # (float64) 1.0\nophys_motion_correction_y.offset # (float) 0.0\nophys_motion_correction_y.unit # (str) pixels\nophys_motion_correction_y.data # (Dataset) shape (40018,); dtype int64\n# ophys_motion_correction_y.data[:] # Access all data\n# ophys_motion_correction_y.data[0:n] # Access first n elements\nophys_motion_correction_y.timestamps # (Dataset) shape (40018,); dtype float64\n# ophys_motion_correction_y.timestamps[:] # Access all data\n# ophys_motion_correction_y.timestamps[0:n] # Access first n elements\nophys_motion_correction_y.timestamps_unit # (str) seconds\nophys_motion_correction_y.interval # (int) 1\nrunning = processing[\"running\"]\nrunning # (ProcessingModule)\nrunning.description # (str) Running speed processing module\nrunning.data_interfaces # (LabelledDict)\ndata_interfaces = running.data_interfaces\ndx = data_interfaces[\"dx\"]\ndx # (TimeSeries)\ndx.resolution # (float64) -1.0\ndx.comments # (str) no comments\ndx.description # (str) Running wheel angular change, computed during data collection\ndx.conversion # (float64) 1.0\ndx.offset # (float) 0.0\ndx.unit # (str) cm\ndx.data # (Dataset) shape (252000,); dtype float32\n# dx.data[:] # Access all data\n# dx.data[0:n] # Access first n elements\ndx.timestamps # (Dataset) shape (252000,); dtype float64\n# dx.timestamps[:] # Access all data\n# dx.timestamps[0:n] # Access first n elements\ndx.timestamps_unit # (str) seconds\ndx.interval # (int) 1\nspeed = data_interfaces[\"speed\"]\nspeed # (TimeSeries)\nspeed.resolution # (float64) -1.0\nspeed.comments # (str) no comments\nspeed.description # (str) no description\nspeed.conversion # (float64) 1.0\nspeed.offset # (float) 0.0\nspeed.unit # (str) cm/s\nspeed.data # (Dataset) shape (252000,); dtype float64\n# speed.data[:] # Access all data\n# speed.data[0:n] # Access first n elements\nspeed.timestamps # (Dataset) shape (252000,); dtype float64\n# speed.timestamps[:] # Access all data\n# speed.timestamps[0:n] # Access first n elements\nspeed.timestamps_unit # (str) seconds\nspeed.interval # (int) 1\nspeed_unfiltered = data_interfaces[\"speed_unfiltered\"]\nspeed_unfiltered # (TimeSeries)\nspeed_unfiltered.resolution # (float64) -1.0\nspeed_unfiltered.comments # (str) no comments\nspeed_unfiltered.description # (str) no description\nspeed_unfiltered.conversion # (float64) 1.0\nspeed_unfiltered.offset # (float) 0.0\nspeed_unfiltered.unit # (str) cm/s\nspeed_unfiltered.data # (Dataset) shape (252000,); dtype float64\n# speed_unfiltered.data[:] # Access all data\n# speed_unfiltered.data[0:n] # Access first n elements\nspeed_unfiltered.timestamps # (Dataset) shape (252000,); dtype float64\n# speed_unfiltered.timestamps[:] # Access all data\n# speed_unfiltered.timestamps[0:n] # Access first n elements\nspeed_unfiltered.timestamps_unit # (str) seconds\nspeed_unfiltered.interval # (int) 1\nstimulus = processing[\"stimulus\"]\nstimulus # (ProcessingModule)\nstimulus.description # (str) Stimulus Times processing\nstimulus.data_interfaces # (LabelledDict)\ndata_interfaces = stimulus.data_interfaces\ntimestamps = data_interfaces[\"timestamps\"]\ntimestamps # (TimeSeries)\ntimestamps.resolution # (float64) -1.0\ntimestamps.comments # (str) no comments\ntimestamps.description # (str) no description\ntimestamps.conversion # (float64) 1.0\ntimestamps.offset # (float) 0.0\ntimestamps.unit # (str) s\ntimestamps.data # (Dataset) shape (252000,); dtype float64\n# timestamps.data[:] # Access all data\n# timestamps.data[0:n] # Access first n elements\ntimestamps.timestamps # (Dataset) shape (252000,); dtype float64\n# timestamps.timestamps[:] # Access all data\n# timestamps.timestamps[0:n] # Access first n elements\ntimestamps.timestamps_unit # (str) seconds\ntimestamps.interval # (int) 1\nstimulus_ophys = processing[\"stimulus_ophys\"]\nstimulus_ophys # (ProcessingModule)\nstimulus_ophys.description # (str) Stimulus Times processing\nstimulus_ophys.data_interfaces # (LabelledDict)\ndata_interfaces = stimulus_ophys.data_interfaces\ntimestamps = data_interfaces[\"timestamps\"]\ntimestamps # (TimeSeries)\ntimestamps.resolution # (float64) -1.0\ntimestamps.comments # (str) no comments\ntimestamps.description # (str) no description\ntimestamps.conversion # (float64) 1.0\ntimestamps.offset # (float) 0.0\ntimestamps.unit # (str) s\ntimestamps.data # (Dataset) shape (252000,); dtype float64\n# timestamps.data[:] # Access all data\n# timestamps.data[0:n] # Access first n elements\ntimestamps.timestamps # (Dataset) shape (252000,); dtype float64\n# timestamps.timestamps[:] # Access all data\n# timestamps.timestamps[0:n] # Access first n elements\ntimestamps.timestamps_unit # (str) seconds\ntimestamps.interval # (int) 1\nnwb.devices # (LabelledDict)\ndevices = nwb.devices\nMESO_2 = devices[\"MESO.2\"]\nMESO_2 # (Device)\nMESO_2.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nnwb.imaging_planes # (LabelledDict)\nimaging_planes = nwb.imaging_planes\nimaging_plane_1 = imaging_planes[\"imaging_plane_1\"]\nimaging_plane_1 # (ImagingPlane)\nimaging_plane_1.optical_channel # (list)\nimaging_plane_1.description # (str) (512, 512) field of view in VISpm at depth 271 um\nimaging_plane_1.excitation_lambda # (float64) 910.0\nimaging_plane_1.imaging_rate # (float64) 10.0\nimaging_plane_1.indicator # (str) GCaMP6f\nimaging_plane_1.location # (str) VISpm\nimaging_plane_1.conversion # (float) 1.0\nimaging_plane_1.unit # (str) meters\nimaging_plane_1.origin_coords_unit # (str) meters\nimaging_plane_1.grid_spacing_unit # (str) meters\nimaging_plane_1.device # (Device)\nimaging_plane_1.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nnwb.intervals # (LabelledDict)\nintervals = nwb.intervals\ngray_presentations = intervals[\"gray_presentations\"]\ngray_presentations # (TimeIntervals)\ngray_presentations.description # (str) Presentation times and stimuli details for 'gray' stimuli. \\nNote: image_name references control_...\ngray_presentations.colnames # (tuple)\ngray_presentations.columns # (tuple)\ngray_presentations.id # (ElementIdentifiers)\n# gray_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# gray_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ngray_presentations.start_time # (VectorData) Start time of epoch, in seconds\ngray_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\ngray_presentations.stimulus_name # (VectorData) Name of stimulus\ngray_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\ngray_presentations.frame # (VectorData) Frame of movie stimulus\ngray_presentations.color # (VectorData) No description\ngray_presentations.contrast # (VectorData) Contrast of stimulus\ngray_presentations.opacity # (VectorData) Opacity of stimulus\ngray_presentations.orientation # (VectorData) Orientation of stimulus\ngray_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\ngray_presentations.units # (VectorData) Units of stimulus size\ngray_presentations.stimulus_index # (VectorData) Index of stimulus type\ngray_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# gray_presentations.tags_index[0] # (ndarray)\n# gray_presentations.tags_index[1] # (ndarray)\n# gray_presentations.tags_index[2] # (ndarray)\n# gray_presentations.tags_index[3] # (ndarray)\n# ...\ngray_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# gray_presentations.timeseries_index[0] # (list)\n# gray_presentations.timeseries_index[1] # (list)\n# gray_presentations.timeseries_index[2] # (list)\n# gray_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_A_presentations = intervals[\"movie_clip_A_presentations\"]\nmovie_clip_A_presentations # (TimeIntervals)\nmovie_clip_A_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_A' stimuli. \\nNote: image_name references ...\nmovie_clip_A_presentations.colnames # (tuple)\nmovie_clip_A_presentations.columns # (tuple)\nmovie_clip_A_presentations.id # (ElementIdentifiers)\n# movie_clip_A_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_A_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_A_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_A_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_A_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_A_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_A_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_A_presentations.color # (VectorData) No description\nmovie_clip_A_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_A_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_A_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_A_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_A_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_A_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_A_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_A_presentations.tags_index[0] # (ndarray)\n# movie_clip_A_presentations.tags_index[1] # (ndarray)\n# movie_clip_A_presentations.tags_index[2] # (ndarray)\n# movie_clip_A_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_A_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_A_presentations.timeseries_index[0] # (list)\n# movie_clip_A_presentations.timeseries_index[1] # (list)\n# movie_clip_A_presentations.timeseries_index[2] # (list)\n# movie_clip_A_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_B_presentations = intervals[\"movie_clip_B_presentations\"]\nmovie_clip_B_presentations # (TimeIntervals)\nmovie_clip_B_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_B' stimuli. \\nNote: image_name references ...\nmovie_clip_B_presentations.colnames # (tuple)\nmovie_clip_B_presentations.columns # (tuple)\nmovie_clip_B_presentations.id # (ElementIdentifiers)\n# movie_clip_B_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_B_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_B_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_B_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_B_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_B_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_B_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_B_presentations.color # (VectorData) No description\nmovie_clip_B_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_B_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_B_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_B_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_B_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_B_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_B_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_B_presentations.tags_index[0] # (ndarray)\n# movie_clip_B_presentations.tags_index[1] # (ndarray)\n# movie_clip_B_presentations.tags_index[2] # (ndarray)\n# movie_clip_B_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_B_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_B_presentations.timeseries_index[0] # (list)\n# movie_clip_B_presentations.timeseries_index[1] # (list)\n# movie_clip_B_presentations.timeseries_index[2] # (list)\n# movie_clip_B_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_C_presentations = intervals[\"movie_clip_C_presentations\"]\nmovie_clip_C_presentations # (TimeIntervals)\nmovie_clip_C_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_C' stimuli. \\nNote: image_name references ...\nmovie_clip_C_presentations.colnames # (tuple)\nmovie_clip_C_presentations.columns # (tuple)\nmovie_clip_C_presentations.id # (ElementIdentifiers)\n# movie_clip_C_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_C_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_C_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_C_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_C_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_C_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_C_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_C_presentations.color # (VectorData) No description\nmovie_clip_C_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_C_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_C_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_C_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_C_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_C_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_C_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_C_presentations.tags_index[0] # (ndarray)\n# movie_clip_C_presentations.tags_index[1] # (ndarray)\n# movie_clip_C_presentations.tags_index[2] # (ndarray)\n# movie_clip_C_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_C_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_C_presentations.timeseries_index[0] # (list)\n# movie_clip_C_presentations.timeseries_index[1] # (list)\n# movie_clip_C_presentations.timeseries_index[2] # (list)\n# movie_clip_C_presentations.timeseries_index[3] # (list)\n# ...\nnwb.lab_meta_data # (LabelledDict)\nlab_meta_data = nwb.lab_meta_data\nmetadata = lab_meta_data[\"metadata\"]\nmetadata # (OphysMetadata)\nmetadata.ophys_experiment_id # (int64)\nmetadata.ophys_session_id # (int64)\nmetadata.experiment_container_id # (int64)\nmetadata.imaging_depth # (int64)\nmetadata.field_of_view_width # (int64)\nmetadata.field_of_view_height # (int64)\nmetadata.imaging_plane_group # (int64)\nmetadata.imaging_plane_group_count # (int64)\nnwb.experiment_description # (str) ophys session\nnwb.institution # (str) Allen Institute for Brain Science\nnwb.surgery # (str)  Structure: VISpm\nnwb.subject # (Subject)\nnwb.subject.age # (str) P108.0D\nnwb.subject.description # (str) external: 684475 donor_id: (1281520997,) specimen_id: ['684475']\nnwb.subject.genotype # (str) Cux2-CreERT2/wt;Camk2a-tTA/wt;Ai93(TITL-GCaMP6f)/wt\nnwb.subject.sex # (str) M\nnwb.subject.species # (str) Mus musculus\nnwb.subject.subject_id # (str) 684475"
      }
    ]
  },
  {
    "role": "user",
    "content": "On the basis of the above information, please create a jupytext notebook that introduces dandiset 000617 using the format described below.\n\nThe purpose of the notebook is to help researchers explore the dandiset and get started with a reanalysis.\n\nStart with an appropriate title for the notebook, such as \"Exploring Dandiset 000617: ...\" (that should be a markdown heading).\n\nInform the user that the notebook was generated with the assistance of AI, and that they should be cautious when interpreting the code or results.\n\nProvide an overview of the Dandiset. Include a link to the Dandiset of the form https://dandiarchive.org/dandiset/000617/0.250311.1615.\n\nSummarize what the notebook covers.\n\nList the packages that are required to run the notebook. Assume that these are already installed on the user's system. Do not include any pip install commands in the notebook.\n\nShow how to use the DANDI API to load assets.\n\nShow how to load and visualize data from the dandiset based on the above information. Feel free to organize things differently from how they are in the above information, but do not make up new information.\n\nGenerate good quality plots (without being redundant).\n\nLoad NWB data by streaming the remote file (as done in the above information) rather than downloading it.\n\nDo not use any functionality of pynwb or dandi that is not covered in the above information.\n\nYou should stick to material that is covered in the above information and do not hallucinate.\n\nThroughout the notebook, include explanatory markdown cells that guide the user through the process.\n\nThe notebook should be well-documented, and follow best practices Include comments in code cells to explain what each step does.\n\nThe Jupytext should use `# %% [markdown]` for markdown cells and `# %%` delimiters for the code cells.\n\nIf any NWB have units objects, you should know the following:\nunits.spike_times_index[i] provides the vector of spike times for the i^th unit. It is actually not an index. Do not use units.spike_times.\n\nDo not render or display the nwb object directly in the notebook as the output could be very large.\n\nUse concise scientific language.\n\nYour notebook will be evaluated based on the following criteria:\n1. How well did the notebook help you understand the purpose and content of the Dandiset?\n2. After reviewing the notebook, do you feel confident in how to access the different types of data from this Dandiset?\n3. Did the notebook help you understand the structure of the NWB file(s) and how to work with them?\n4. Did the visualizations in the notebook generally help you understand key aspects of the data?\n5. Did any of the visualizations make it harder to understand the data (e.g., due to poor formatting, unclear axes, or misleading displays)?\n6. Do you feel more confident creating your own visualizations of the data after seeing the examples in the notebook?\n7. How well did the visualizations show the structure or complexity of the data?\n8. Were there any interpretations or conclusions in the notebook that felt unclear or not well supported by the data shown?\n9. Did any of the plots or examples feel unnecessarily repetitive or redundant?\n10. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n11. How clear and easy was the notebook to follow?\n12. Did the notebook provide code you could easily reuse or adapt to explore the Dandiset yourself?\n13. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n14. Overall, how helpful was this notebook for getting started with this Dandiset?\n\nYour output should be in the format:\n\n<notebook>\nThe text of the jupytext notebook should appear here.\n</notebook>\n\nNo other text should be included in the response.\n"
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 000617: Sequence Learning in Mouse Visual Cortex\n\n# %% [markdown]\n# **Note**: This notebook was generated with AI assistance. Please verify all code and results independently.\n\n# %% [markdown]\n# ## Overview\n# This notebook explores [Dandiset 000617](https://dandiarchive.org/dandiset/000617/0.250311.1615), which investigates sequence learning in mouse visual cortex using two-photon calcium imaging. The experiment examines how neural responses in different visual cortical areas change as mice learn temporal sequences of natural movie clips.\n\n# %% [markdown]\n# The experiment design involves:\n# - Head-fixed mice passively viewing sequences of 2-second natural movie clips (A, B, C)\n# - Initial random presentation (Day 0)\n# - Three training sessions with repeating sequences (ABCABC...)\n# - Final random presentation (Day 4)\n# - Simultaneous recording from multiple visual areas (V1, LM, AM, PM) and layers (L2/3, L4)\n\n# %% [markdown]\n# This notebook demonstrates:\n# 1. Loading data using the DANDI API\n# 2. Accessing calcium imaging traces and ROI information\n# 3. Examining eye tracking and running speed data\n# 4. Visualizing stimulus presentations\n# 5. Exploring motion correction data\n\n# %% [markdown]\n# ## Required Packages\n\n# %%\nimport pynwb\nimport h5py\nimport remfile\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dandi.dandiapi import DandiAPIClient\nimport pandas as pd\n\n# %% [markdown]\n# ## Loading Data from DANDI\n\n# %%\n# Initialize DANDI API client\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250311.1615\")\n\n# Get a specific NWB file\nasset_path = \"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\"\nurl = next(dandiset.get_assets_by_glob(asset_path)).download_url\n\n# Load the NWB file\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\n# %% [markdown]\n# ## Exploring Neural Activity Data\n\n# %%\n# Access DFF (\u0394F/F) traces\ndff_data = nwb.processing['ophys']['dff']['roi_response_series']['traces'].data\ntimestamps = nwb.processing['ophys']['dff']['roi_response_series']['traces'].timestamps\n\n# Plot DFF traces for first few neurons\nplt.figure(figsize=(12, 6))\nfor i in range(min(5, dff_data.shape[1])):\n    plt.plot(timestamps[:1000], dff_data[:1000, i], label=f'Neuron {i+1}')\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F')\nplt.title('Calcium Activity Traces')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Analyzing Running Behavior\n\n# %%\n# Get running speed data\nspeed = nwb.processing['running']['speed'].data\nspeed_timestamps = nwb.processing['running']['speed'].timestamps\n\n# Plot running speed\nplt.figure(figsize=(12, 4))\nplt.plot(speed_timestamps[:1000], speed[:1000])\nplt.xlabel('Time (s)')\nplt.ylabel('Speed (cm/s)')\nplt.title('Mouse Running Speed')\nplt.show()\n\n# %% [markdown]\n# ## Eye Tracking Analysis\n\n# %%\n# Get pupil tracking data\npupil_x = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 0]\npupil_y = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 1]\npupil_area = nwb.acquisition['EyeTracking'].pupil_tracking.area\n\n# Create scatter plot of pupil position\nplt.figure(figsize=(10, 10))\nplt.scatter(pupil_x[:1000], pupil_y[:1000], c=pupil_area[:1000], \n            cmap='viridis', alpha=0.5, s=1)\nplt.colorbar(label='Pupil Area')\nplt.xlabel('X Position')\nplt.ylabel('Y Position')\nplt.title('Pupil Position and Area')\nplt.show()\n\n# %% [markdown]\n# ## Stimulus Presentation Analysis\n\n# %%\n# Convert stimulus presentations to pandas DataFrames\nmovie_A_df = nwb.intervals['movie_clip_A_presentations'].to_dataframe()\nmovie_B_df = nwb.intervals['movie_clip_B_presentations'].to_dataframe()\nmovie_C_df = nwb.intervals['movie_clip_C_presentations'].to_dataframe()\n\n# Plot stimulus presentation times\nplt.figure(figsize=(12, 4))\nfor i, (df, label) in enumerate([\n    (movie_A_df, 'Movie A'),\n    (movie_B_df, 'Movie B'),\n    (movie_C_df, 'Movie C')\n]):\n    plt.eventplot(df['start_time'], lineoffsets=i, linelengths=0.5, \n                 label=label)\nplt.xlabel('Time (s)')\nplt.ylabel('Stimulus Type')\nplt.title('Stimulus Presentation Times')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Motion Correction Analysis\n\n# %%\n# Get motion correction data\nmotion_x = nwb.processing['ophys']['ophys_motion_correction_x'].data\nmotion_y = nwb.processing['ophys']['ophys_motion_correction_y'].data\nmotion_timestamps = nwb.processing['ophys']['ophys_motion_correction_x'].timestamps\n\n# Plot motion correction\nplt.figure(figsize=(12, 6))\nplt.plot(motion_timestamps[:1000], motion_x[:1000], label='X correction')\nplt.plot(motion_timestamps[:1000], motion_y[:1000], label='Y correction')\nplt.xlabel('Time (s)')\nplt.ylabel('Correction (pixels)')\nplt.title('Motion Correction')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Next Steps\n# \n# This dataset enables various analyses:\n# - Comparing neural responses between random and sequential presentations\n# - Investigating predictive coding across visual areas\n# - Examining relationships between running behavior and neural activity\n# - Studying eye movements during sequence learning\n# \n# The NWB files contain additional data not explored here, including:\n# - ROI masks and cell classification\n# - Neuropil fluorescence\n# - Event detection results\n# - Multiple imaging planes\n</notebook>"
  },
  {
    "role": "user",
    "content": "The notebook execution failed with the following error:\n\nNotebook execution failed: [NbClientApp] Executing notebooks/dandisets/000617/0.250311.1615/skip-explore/claude-3.5-sonnet/h-3/notebook.ipynb\n[NbClientApp] Executing notebook with kernel: \nTraceback (most recent call last):\n  File \"/opt/conda/bin/jupyter-execute\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n    super().launch_instance(argv=argv, **kwargs)\n  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n    app.initialize(argv)\n  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 118, in inner\n    return method(app, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/cli.py\", line 161, in initialize\n    self.run_notebook(path)\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/cli.py\", line 215, in run_notebook\n    client.execute()\n  File \"/opt/conda/lib/python3.11/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n    return loop.run_until_complete(inner)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 709, in async_execute\n    await self.async_execute_cell(\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 1062, in async_execute_cell\n    await self._check_raise_for_error(cell, cell_index, exec_reply)\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 918, in _check_raise_for_error\n    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\nnbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n------------------\n# Access DFF (\u0394F/F) traces\ndff_data = nwb.processing['ophys']['dff']['roi_response_series']['traces'].data\ntimestamps = nwb.processing['ophys']['dff']['roi_response_series']['traces'].timestamps\n\n# Plot DFF traces for first few neurons\nplt.figure(figsize=(12, 6))\nfor i in range(min(5, dff_data.shape[1])):\n    plt.plot(timestamps[:1000], dff_data[:1000, i], label=f'Neuron {i+1}')\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F')\nplt.title('Calcium Activity Traces')\nplt.legend()\nplt.show()\n------------------\n\n\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\nCell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Access DFF (\u0394F/F) traces\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dff_data \u001b[38;5;241m=\u001b[39m \u001b[43mnwb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mophys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroi_response_series\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraces\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m      3\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m nwb\u001b[38;5;241m.\u001b[39mprocessing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mophys\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdff\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroi_response_series\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraces\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtimestamps\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot DFF traces for first few neurons\u001b[39;00m\n\nFile \u001b[0;32m/opt/conda/lib/python3.11/site-packages/hdmf/utils.py:668\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    667\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/opt/conda/lib/python3.11/site-packages/hdmf/container.py:1150\u001b[0m, in \u001b[0;36mMultiContainerInterface.__make_getitem.<locals>._func\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(msg)\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\n\u001b[0;31mKeyError\u001b[0m: \"'roi_response_series' not found in DfOverF 'dff'.\"\n\n\n\nPlease provide a corrected version of the notebook."
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 000617: Sequence Learning in Mouse Visual Cortex\n\n# %% [markdown]\n# **Note**: This notebook was generated with AI assistance. Please verify all code and results independently.\n\n# %% [markdown]\n# ## Overview\n# This notebook explores [Dandiset 000617](https://dandiarchive.org/dandiset/000617/0.250311.1615), which investigates sequence learning in mouse visual cortex using two-photon calcium imaging. The experiment examines how neural responses in different visual areas change as mice learn temporal sequences of natural movie clips.\n\n# %% [markdown]\n# The experiment design involves:\n# - Head-fixed mice passively viewing sequences of 2-second natural movie clips (A, B, C)\n# - Initial random presentation (Day 0)\n# - Three training sessions with repeating sequences (ABCABC...)\n# - Final random presentation (Day 4)\n# - Simultaneous recording from multiple visual areas (V1, LM, AM, PM) and layers (L2/3, L4)\n\n# %% [markdown]\n# This notebook demonstrates:\n# 1. Loading data using the DANDI API\n# 2. Accessing calcium imaging traces and ROI information\n# 3. Examining eye tracking and running speed data\n# 4. Visualizing stimulus presentations\n# 5. Exploring motion correction data\n\n# %% [markdown]\n# ## Required Packages\n\n# %%\nimport pynwb\nimport h5py\nimport remfile\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dandi.dandiapi import DandiAPIClient\nimport pandas as pd\n\n# %% [markdown]\n# ## Loading Data from DANDI\n\n# %%\n# Initialize DANDI API client\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250311.1615\")\n\n# Get a specific NWB file\nasset_path = \"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\"\nurl = next(dandiset.get_assets_by_glob(asset_path)).download_url\n\n# Load the NWB file\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\n# %% [markdown]\n# ## Exploring Neural Activity Data\n\n# %%\n# Access DFF (\u0394F/F) traces from the correct path\ndff_data = nwb.processing['ophys']['dff'].data\ntimestamps = nwb.processing['ophys']['dff'].timestamps\n\n# Plot DFF traces for first few neurons\nplt.figure(figsize=(12, 6))\nfor i in range(min(5, dff_data.shape[1])):\n    plt.plot(timestamps[:1000], dff_data[:1000, i], label=f'Neuron {i+1}')\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F')\nplt.title('Calcium Activity Traces')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Analyzing Running Behavior\n\n# %%\n# Get running speed data\nspeed = nwb.processing['running']['speed'].data\nspeed_timestamps = nwb.processing['running']['speed'].timestamps\n\n# Plot running speed\nplt.figure(figsize=(12, 4))\nplt.plot(speed_timestamps[:1000], speed[:1000])\nplt.xlabel('Time (s)')\nplt.ylabel('Speed (cm/s)')\nplt.title('Mouse Running Speed')\nplt.show()\n\n# %% [markdown]\n# ## Eye Tracking Analysis\n\n# %%\n# Get pupil tracking data\npupil_x = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 0]\npupil_y = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 1]\npupil_area = nwb.acquisition['EyeTracking'].pupil_tracking.area\n\n# Create scatter plot of pupil position\nplt.figure(figsize=(10, 10))\nplt.scatter(pupil_x[:1000], pupil_y[:1000], c=pupil_area[:1000], \n            cmap='viridis', alpha=0.5, s=1)\nplt.colorbar(label='Pupil Area')\nplt.xlabel('X Position')\nplt.ylabel('Y Position')\nplt.title('Pupil Position and Area')\nplt.show()\n\n# %% [markdown]\n# ## Stimulus Presentation Analysis\n\n# %%\n# Convert stimulus presentations to pandas DataFrames\nmovie_A_df = nwb.intervals['movie_clip_A_presentations'].to_dataframe()\nmovie_B_df = nwb.intervals['movie_clip_B_presentations'].to_dataframe()\nmovie_C_df = nwb.intervals['movie_clip_C_presentations'].to_dataframe()\n\n# Plot stimulus presentation times\nplt.figure(figsize=(12, 4))\nfor i, (df, label) in enumerate([\n    (movie_A_df, 'Movie A'),\n    (movie_B_df, 'Movie B'),\n    (movie_C_df, 'Movie C')\n]):\n    plt.eventplot(df['start_time'], lineoffsets=i, linelengths=0.5, \n                 label=label)\nplt.xlabel('Time (s)')\nplt.ylabel('Stimulus Type')\nplt.title('Stimulus Presentation Times')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Motion Correction Analysis\n\n# %%\n# Get motion correction data\nmotion_x = nwb.processing['ophys']['ophys_motion_correction_x'].data\nmotion_y = nwb.processing['ophys']['ophys_motion_correction_y'].data\nmotion_timestamps = nwb.processing['ophys']['ophys_motion_correction_x'].timestamps\n\n# Plot motion correction\nplt.figure(figsize=(12, 6))\nplt.plot(motion_timestamps[:1000], motion_x[:1000], label='X correction')\nplt.plot(motion_timestamps[:1000], motion_y[:1000], label='Y correction')\nplt.xlabel('Time (s)')\nplt.ylabel('Correction (pixels)')\nplt.title('Motion Correction')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Next Steps\n# \n# This dataset enables various analyses:\n# - Comparing neural responses between random and sequential presentations\n# - Investigating predictive coding across visual areas\n# - Examining relationships between running behavior and neural activity\n# - Studying eye movements during sequence learning\n# \n# The NWB files contain additional data not explored here, including:\n# - ROI masks and cell classification\n# - Neuropil fluorescence\n# - Event detection results\n# - Multiple imaging planes\n</notebook>"
  },
  {
    "role": "user",
    "content": "The notebook execution failed with the following error:\n\nNotebook execution failed: [NbClientApp] Executing notebooks/dandisets/000617/0.250311.1615/skip-explore/claude-3.5-sonnet/h-3/notebook.ipynb\n[NbClientApp] Executing notebook with kernel: \nTraceback (most recent call last):\n  File \"/opt/conda/bin/jupyter-execute\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n    super().launch_instance(argv=argv, **kwargs)\n  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n    app.initialize(argv)\n  File \"/opt/conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 118, in inner\n    return method(app, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/cli.py\", line 161, in initialize\n    self.run_notebook(path)\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/cli.py\", line 215, in run_notebook\n    client.execute()\n  File \"/opt/conda/lib/python3.11/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n    return loop.run_until_complete(inner)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 709, in async_execute\n    await self.async_execute_cell(\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 1062, in async_execute_cell\n    await self._check_raise_for_error(cell, cell_index, exec_reply)\n  File \"/opt/conda/lib/python3.11/site-packages/nbclient/client.py\", line 918, in _check_raise_for_error\n    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\nnbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n------------------\n# Access DFF (\u0394F/F) traces from the correct path\ndff_data = nwb.processing['ophys']['dff'].data\ntimestamps = nwb.processing['ophys']['dff'].timestamps\n\n# Plot DFF traces for first few neurons\nplt.figure(figsize=(12, 6))\nfor i in range(min(5, dff_data.shape[1])):\n    plt.plot(timestamps[:1000], dff_data[:1000, i], label=f'Neuron {i+1}')\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F')\nplt.title('Calcium Activity Traces')\nplt.legend()\nplt.show()\n------------------\n\n\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\nCell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Access DFF (\u0394F/F) traces from the correct path\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dff_data \u001b[38;5;241m=\u001b[39m \u001b[43mnwb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mophys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m      3\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m nwb\u001b[38;5;241m.\u001b[39mprocessing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mophys\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtimestamps\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot DFF traces for first few neurons\u001b[39;00m\n\n\u001b[0;31mAttributeError\u001b[0m: 'DfOverF' object has no attribute 'data'\n\n\n\nPlease provide a corrected version of the notebook."
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 000617: Sequence Learning in Mouse Visual Cortex\n\n# %% [markdown]\n# **Note**: This notebook was generated with AI assistance. Please verify all code and results independently.\n\n# %% [markdown]\n# ## Overview\n# This notebook explores [Dandiset 000617](https://dandiarchive.org/dandiset/000617/0.250311.1615), which investigates sequence learning in mouse visual cortex using two-photon calcium imaging. The experiment examines how neural responses in different visual areas change as mice learn temporal sequences of natural movie clips.\n\n# %% [markdown]\n# The experiment design involves:\n# - Head-fixed mice passively viewing sequences of 2-second natural movie clips (A, B, C)\n# - Initial random presentation (Day 0)\n# - Three training sessions with repeating sequences (ABCABC...)\n# - Final random presentation (Day 4)\n# - Simultaneous recording from multiple visual areas (V1, LM, AM, PM) and layers (L2/3, L4)\n\n# %% [markdown]\n# This notebook demonstrates:\n# 1. Loading data using the DANDI API\n# 2. Accessing calcium imaging traces and ROI information\n# 3. Examining eye tracking and running speed data\n# 4. Visualizing stimulus presentations\n# 5. Exploring motion correction data\n\n# %% [markdown]\n# ## Required Packages\n\n# %%\nimport pynwb\nimport h5py\nimport remfile\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dandi.dandiapi import DandiAPIClient\nimport pandas as pd\n\n# %% [markdown]\n# ## Loading Data from DANDI\n\n# %%\n# Initialize DANDI API client\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250311.1615\")\n\n# Get a specific NWB file\nasset_path = \"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\"\nurl = next(dandiset.get_assets_by_glob(asset_path)).download_url\n\n# Load the NWB file\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\n# %% [markdown]\n# ## Exploring Neural Activity Data\n\n# %%\n# Access DFF (\u0394F/F) traces from the correct path\ndff_data = nwb.processing['ophys']['dff'].roi_response_series['traces'].data\ntimestamps = nwb.processing['ophys']['dff'].roi_response_series['traces'].timestamps\n\n# Plot DFF traces for first few neurons\nplt.figure(figsize=(12, 6))\nfor i in range(min(5, dff_data.shape[1])):\n    plt.plot(timestamps[:1000], dff_data[:1000, i], label=f'Neuron {i+1}')\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F')\nplt.title('Calcium Activity Traces')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Analyzing Running Behavior\n\n# %%\n# Get running speed data\nspeed = nwb.processing['running']['speed'].data\nspeed_timestamps = nwb.processing['running']['speed'].timestamps\n\n# Plot running speed\nplt.figure(figsize=(12, 4))\nplt.plot(speed_timestamps[:1000], speed[:1000])\nplt.xlabel('Time (s)')\nplt.ylabel('Speed (cm/s)')\nplt.title('Mouse Running Speed')\nplt.show()\n\n# %% [markdown]\n# ## Eye Tracking Analysis\n\n# %%\n# Get pupil tracking data\npupil_x = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 0]\npupil_y = nwb.acquisition['EyeTracking'].pupil_tracking.data[:, 1]\npupil_area = nwb.acquisition['EyeTracking'].pupil_tracking.area\n\n# Create scatter plot of pupil position\nplt.figure(figsize=(10, 10))\nplt.scatter(pupil_x[:1000], pupil_y[:1000], c=pupil_area[:1000], \n            cmap='viridis', alpha=0.5, s=1)\nplt.colorbar(label='Pupil Area')\nplt.xlabel('X Position')\nplt.ylabel('Y Position')\nplt.title('Pupil Position and Area')\nplt.show()\n\n# %% [markdown]\n# ## Stimulus Presentation Analysis\n\n# %%\n# Convert stimulus presentations to pandas DataFrames\nmovie_A_df = nwb.intervals['movie_clip_A_presentations'].to_dataframe()\nmovie_B_df = nwb.intervals['movie_clip_B_presentations'].to_dataframe()\nmovie_C_df = nwb.intervals['movie_clip_C_presentations'].to_dataframe()\n\n# Plot stimulus presentation times\nplt.figure(figsize=(12, 4))\nfor i, (df, label) in enumerate([\n    (movie_A_df, 'Movie A'),\n    (movie_B_df, 'Movie B'),\n    (movie_C_df, 'Movie C')\n]):\n    plt.eventplot(df['start_time'], lineoffsets=i, linelengths=0.5, \n                 label=label)\nplt.xlabel('Time (s)')\nplt.ylabel('Stimulus Type')\nplt.title('Stimulus Presentation Times')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Motion Correction Analysis\n\n# %%\n# Get motion correction data\nmotion_x = nwb.processing['ophys']['ophys_motion_correction_x'].data\nmotion_y = nwb.processing['ophys']['ophys_motion_correction_y'].data\nmotion_timestamps = nwb.processing['ophys']['ophys_motion_correction_x'].timestamps\n\n# Plot motion correction\nplt.figure(figsize=(12, 6))\nplt.plot(motion_timestamps[:1000], motion_x[:1000], label='X correction')\nplt.plot(motion_timestamps[:1000], motion_y[:1000], label='Y correction')\nplt.xlabel('Time (s)')\nplt.ylabel('Correction (pixels)')\nplt.title('Motion Correction')\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# ## Next Steps\n# \n# This dataset enables various analyses:\n# - Comparing neural responses between random and sequential presentations\n# - Investigating predictive coding across visual areas\n# - Examining relationships between running behavior and neural activity\n# - Studying eye movements during sequence learning\n# \n# The NWB files contain additional data not explored here, including:\n# - ROI masks and cell classification\n# - Neuropil fluorescence\n# - Event detection results\n# - Multiple imaging planes\n</notebook>"
  }
]