[
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata about Dandiset 000617 version 0.250312.0130."
      },
      {
        "type": "text",
        "text": "{\n  \"id\": \"DANDI:000617/0.250312.0130\",\n  \"doi\": \"10.48324/dandi.000617/0.250312.0130\",\n  \"url\": \"https://dandiarchive.org/dandiset/000617/0.250312.0130\",\n  \"name\": \"Allen Institute Openscope - Sequence Learning Project\",\n  \"about\": [],\n  \"access\": [\n    {\n      \"status\": \"dandi:OpenAccess\",\n      \"schemaKey\": \"AccessRequirements\"\n    }\n  ],\n  \"license\": [\n    \"spdx:CC-BY-4.0\"\n  ],\n  \"version\": \"0.250312.0130\",\n  \"@context\": \"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.4/context.json\",\n  \"citation\": \"Berry, Michael; Lecoq, J\\u00e9r\\u00f4me; Amaya, Avalon; Wilkes, Josh; Nguyen, Katrina; Peene, Carter; Bawany, Ahad; Han, Warren Han; Seid,  Samuel; Young, Ahrial; Naidoo, Robyn; Ha, Vivian; Johnson, Tye; Williford, Ali; Swapp, Jackie; Caldejon, Shiella (2025) Allen Institute Openscope - Sequence Learning Project (Version 0.250312.0130) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.000617/0.250312.0130\",\n  \"keywords\": [\n    \"L2/3\",\n    \"L4\",\n    \"two-photon calcium imaging\",\n    \"prediction\",\n    \"pyramidal neurons\",\n    \"somata\",\n    \"neocortex\",\n    \"predictive coding\",\n    \"predictive processing\",\n    \"sequence learning\",\n    \"movies\"\n  ],\n  \"protocol\": [],\n  \"schemaKey\": \"Dandiset\",\n  \"identifier\": \"DANDI:000617\",\n  \"repository\": \"https://dandiarchive.org\",\n  \"contributor\": [\n    {\n      \"name\": \"National Institutes of Health (NIH)\",\n      \"roleName\": [\n        \"dcite:Funder\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"awardNumber\": \"1 U24 NS 113646-02\"\n    },\n    {\n      \"url\": \"https://alleninstitute.org/\",\n      \"name\": \"Allen Institute\",\n      \"roleName\": [\n        \"dcite:Producer\",\n        \"dcite:Funder\",\n        \"dcite:Affiliation\"\n      ],\n      \"schemaKey\": \"Organization\",\n      \"identifier\": \"https://ror.org/03cpe7c52\",\n      \"includeInCitation\": false\n    },\n    {\n      \"name\": \"Berry, Michael\",\n      \"email\": \"berry@princeton.edu\",\n      \"roleName\": [\n        \"dcite:ContactPerson\",\n        \"dcite:Conceptualization\",\n        \"dcite:Author\",\n        \"dcite:Investigation\",\n        \"dcite:ProjectLeader\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-4133-7999\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Lecoq, J\\u00e9r\\u00f4me\",\n      \"email\": \"jeromel@alleninstitute.org\",\n      \"roleName\": [\n        \"dcite:ContactPerson\",\n        \"dcite:DataManager\",\n        \"dcite:FundingAcquisition\",\n        \"dcite:Maintainer\",\n        \"dcite:ProjectManager\",\n        \"dcite:ProjectAdministration\",\n        \"dcite:Software\",\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-0131-0938\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Amaya, Avalon\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0003-2274-2892\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Wilkes, Josh\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0005-4858-2219\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Nguyen, Katrina\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0009-8547-9703\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Peene, Carter\",\n      \"roleName\": [\n        \"dcite:DataCurator\",\n        \"dcite:DataManager\",\n        \"dcite:Software\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-6660-2264\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Bawany, Ahad\",\n      \"roleName\": [\n        \"dcite:DataCurator\",\n        \"dcite:DataManager\",\n        \"dcite:Software\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0002-5495-5411\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Han, Warren Han\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-0104-0380\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Seid,  Samuel\",\n      \"roleName\": [\n        \"dcite:DataCollector\",\n        \"dcite:DataCurator\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0000-0002-5403-7816\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Young, Ahrial\",\n      \"roleName\": [\n        \"dcite:DataCollector\",\n        \"dcite:DataCurator\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Naidoo, Robyn\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0004-0996-8808\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Ha, Vivian\",\n      \"roleName\": [\n        \"dcite:DataCollector\",\n        \"dcite:DataCurator\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0005-0093-3825\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Johnson, Tye\",\n      \"roleName\": [\n        \"dcite:DataCollector\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0006-7239-7571\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Williford, Ali\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0000-7282-7515\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Swapp, Jackie\",\n      \"roleName\": [\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"identifier\": \"0009-0008-4965-6242\",\n      \"includeInCitation\": true\n    },\n    {\n      \"name\": \"Caldejon, Shiella\",\n      \"roleName\": [\n        \"dcite:DataCollector\",\n        \"dcite:DataCurator\",\n        \"dcite:DataManager\",\n        \"dcite:Supervision\"\n      ],\n      \"schemaKey\": \"Person\",\n      \"includeInCitation\": true\n    }\n  ],\n  \"dateCreated\": \"2023-07-28T21:00:57.366907+00:00\",\n  \"description\": \"Adaptive and coordinated behavior requires that an animal be able to make predictions about the near and even far future. This intuition that some neural computations should be \\u2018predictive\\u2019 in their character has a long history, starting with ideas about how the receptive field structure of retinal ganglion cells relate to the statistics of natural visual scenes. Ideas about predictive computation have been most influential in thinking about the function of the neocortex. Here, the relatively stereotyped local circuitry of the neocortex has long led to speculation that each local circuit might be carrying out a somewhat similar, fundamental computation on its specific inputs. In addition, the organization of sensory-motor pathways into hierarchies (e.g., V1, V2, V4, IT in the ventral visual stream) with stereotyped feedforward and feedback connections has motivated ideas about hierarchical predictive codes, where higher levels of the hierarchy send predictions down to the lower level that then compares its inputs against the predictions and only send the surprises up the hierarchy (such as in the work of Mumford, Rao & Ballard, and Friston). Despite the wide influence of ideas about predictive coding, there is relatively little experimental evidence that such computations occur in multiple cortical areas, perhaps serving as a \\u2018canonical computation\\u2019 of the neocortical microcircuit. Our experimental design is based on a Sequence Learning Experiment, in which head-fixed mice passively view sequences of three different natural movie clips (labeled \\u2018A\\u2019, \\u2018B\\u2019, \\u2018C\\u2019), each having a duration of 2 seconds. We begin with one recording session (day #0), where the movie clips are presented in random order along with a 2 second grey screen (labeled \\u2018X\\u2019). Each stimulus occurs a total of 525 times, allowing a thorough characterization of neural responses before any sequence learning has occurred. Next, there are 3 recording sessions where the three movie clips are presented in a repeating temporal sequence, ABCABC\\u2026, for 500 times, in order to train the mouse\\u2019s brain. This training allows the mouse to potentially use the identity of the current movie clip predict the next movie clip. In addition, each sequence training session includes a period of random-order presentation, in order to assess changes in neural tuning during sequence learning. Finally, our last session (day #4) had stimuli presented in random order, allowing us to test more thoroughly how responses changed due to sequence learning.\\n\\nOur design uses 2-photon microscopy with eight simultaneously recorded fields-of-view. The fields-of-view will include both layer 2/3 and layer 4 as well as from multiple cortical areas: V1 (VISp), LM (VISl), AM (VISam), and PM (VISpm). The experiment used the Cux2-CreERTS2:Camk2a-tTa; Ai93(TITL-GCaMP6f) mouse line, which has expression in excitatory neurons of both layer 4 and 2/3.\",\n  \"publishedBy\": {\n    \"id\": \"urn:uuid:c661ecb4-5e78-479e-9f63-e2ab5d6040e2\",\n    \"name\": \"DANDI publish\",\n    \"endDate\": \"2025-03-12T01:30:40.025190+00:00\",\n    \"schemaKey\": \"PublishActivity\",\n    \"startDate\": \"2025-03-12T01:30:40.025190+00:00\",\n    \"wasAssociatedWith\": [\n      {\n        \"id\": \"urn:uuid:b7b71906-2791-4624-b429-aa5c1877f3b8\",\n        \"name\": \"DANDI API\",\n        \"version\": \"0.1.0\",\n        \"schemaKey\": \"Software\",\n        \"identifier\": \"RRID:SCR_017571\"\n      }\n    ]\n  },\n  \"studyTarget\": [],\n  \"assetsSummary\": {\n    \"species\": [\n      {\n        \"name\": \"Mus musculus - House mouse\",\n        \"schemaKey\": \"SpeciesType\",\n        \"identifier\": \"http://purl.obolibrary.org/obo/NCBITaxon_10090\"\n      }\n    ],\n    \"approach\": [\n      {\n        \"name\": \"microscopy approach; cell population imaging\",\n        \"schemaKey\": \"ApproachType\"\n      }\n    ],\n    \"schemaKey\": \"AssetsSummary\",\n    \"dataStandard\": [\n      {\n        \"name\": \"Neurodata Without Borders (NWB)\",\n        \"schemaKey\": \"StandardsType\",\n        \"identifier\": \"RRID:SCR_015242\"\n      }\n    ],\n    \"numberOfBytes\": 13197788224949,\n    \"numberOfFiles\": 1197,\n    \"numberOfSubjects\": 13,\n    \"variableMeasured\": [\n      \"OpticalChannel\",\n      \"PlaneSegmentation\",\n      \"ProcessingModule\",\n      \"ImagingPlane\",\n      \"TwoPhotonSeries\"\n    ],\n    \"measurementTechnique\": [\n      {\n        \"name\": \"analytical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"two-photon microscopy technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      },\n      {\n        \"name\": \"surgical technique\",\n        \"schemaKey\": \"MeasurementTechniqueType\"\n      }\n    ]\n  },\n  \"datePublished\": \"2025-03-12T01:30:40.025190+00:00\",\n  \"schemaVersion\": \"0.6.4\",\n  \"ethicsApproval\": [\n    {\n      \"schemaKey\": \"EthicsApproval\",\n      \"identifier\": \"\\u200bAllen Institute Institutional Animal Care and Use Committee (IACUC), protocol 2117\",\n      \"contactPoint\": {\n        \"schemaKey\": \"ContactPoint\"\n      }\n    }\n  ],\n  \"wasGeneratedBy\": [],\n  \"acknowledgement\": \"We thank the Allen Institute founder, Paul G. Allen, for his vision, encouragement, and support.\",\n  \"relatedResource\": [],\n  \"manifestLocation\": [\n    \"https://dandiarchive.s3.amazonaws.com/dandisets/000617/0.250312.0130/assets.yaml\"\n  ]\n}"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is a listing of some of the files in the Dandiset."
      },
      {
        "type": "text",
        "text": "sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294084428-acq-1294179943-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914859-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294084428-acq-1294179942-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370666-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370664-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914856-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1294259213-acq-1294370663-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914853-raw-movies_ophys.nwb\nsub-684475/sub-684475_ses-1293785946-acq-1293914860-raw-movies_ophys.nwb"
      }
    ]
  },
  {
    "role": "system",
    "content": [
      {
        "type": "text",
        "text": "The following is metadata and usage information about the file sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb."
      },
      {
        "type": "text",
        "text": "\nimport pynwb\nimport h5py\nimport remfile\n\n# Load\nfrom dandi.dandiapi import DandiAPIClient\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250312.0130\")\nurl = next(dandiset.get_assets_by_glob(\"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\")).download_url\n\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\nnwb # (NWBFile)\nnwb.session_description # (str) Ophys Session\nnwb.identifier # (str) 1294179945\nnwb.session_start_time # (datetime) 2023-08-31T20:49:56.751000+00:00\nnwb.timestamps_reference_time # (datetime) 2023-08-31T20:49:56.751000+00:00\nnwb.file_create_date # (list) [datetime.datetime(2024, 2, 8, 18, 6, 8, 800829, tzinfo=tzlocal())]\nnwb.acquisition # (LabelledDict)\nacquisition = nwb.acquisition\nEyeTracking = acquisition[\"EyeTracking\"]\nEyeTracking # (EllipseEyeTracking)\nEyeTracking.spatial_series # (LabelledDict)\nspatial_series = EyeTracking.spatial_series\ncorneal_reflection_tracking = spatial_series[\"corneal_reflection_tracking\"]\ncorneal_reflection_tracking # (EllipseSeries)\ncorneal_reflection_tracking.resolution # (float64) -1.0\ncorneal_reflection_tracking.comments # (str) no comments\ncorneal_reflection_tracking.description # (str) no description\ncorneal_reflection_tracking.conversion # (float64) 1.0\ncorneal_reflection_tracking.offset # (float) 0.0\ncorneal_reflection_tracking.unit # (str) meters\ncorneal_reflection_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# corneal_reflection_tracking.data[:, :] # Access all data\n# corneal_reflection_tracking.data[0:n, :] # Access first n rows\n# corneal_reflection_tracking.data[:, 0:n] # Access first n columns\ncorneal_reflection_tracking.timestamps_unit # (str) seconds\ncorneal_reflection_tracking.interval # (int) 1\ncorneal_reflection_tracking.reference_frame # (str) nose\ncorneal_reflection_tracking.area # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.area[:] # Access all data\n# corneal_reflection_tracking.area[0:n] # Access first n elements\ncorneal_reflection_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.area_raw[:] # Access all data\n# corneal_reflection_tracking.area_raw[0:n] # Access first n elements\ncorneal_reflection_tracking.width # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.width[:] # Access all data\n# corneal_reflection_tracking.width[0:n] # Access first n elements\ncorneal_reflection_tracking.height # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.height[:] # Access all data\n# corneal_reflection_tracking.height[0:n] # Access first n elements\ncorneal_reflection_tracking.angle # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.angle[:] # Access all data\n# corneal_reflection_tracking.angle[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps # (EllipseSeries)\ncorneal_reflection_tracking.timestamps.resolution # (float64) -1.0\ncorneal_reflection_tracking.timestamps.comments # (str) no comments\ncorneal_reflection_tracking.timestamps.description # (str) no description\ncorneal_reflection_tracking.timestamps.conversion # (float64) 1.0\ncorneal_reflection_tracking.timestamps.offset # (float) 0.0\ncorneal_reflection_tracking.timestamps.unit # (str) meters\ncorneal_reflection_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# corneal_reflection_tracking.timestamps.data[:, :] # Access all data\n# corneal_reflection_tracking.timestamps.data[0:n, :] # Access first n rows\n# corneal_reflection_tracking.timestamps.data[:, 0:n] # Access first n columns\ncorneal_reflection_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.timestamps[:] # Access all data\n# corneal_reflection_tracking.timestamps.timestamps[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.timestamps_unit # (str) seconds\ncorneal_reflection_tracking.timestamps.interval # (int) 1\ncorneal_reflection_tracking.timestamps.reference_frame # (str) nose\ncorneal_reflection_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.area[:] # Access all data\n# corneal_reflection_tracking.timestamps.area[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.area_raw[:] # Access all data\n# corneal_reflection_tracking.timestamps.area_raw[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.width[:] # Access all data\n# corneal_reflection_tracking.timestamps.width[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.height[:] # Access all data\n# corneal_reflection_tracking.timestamps.height[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# corneal_reflection_tracking.timestamps.angle[:] # Access all data\n# corneal_reflection_tracking.timestamps.angle[0:n] # Access first n elements\ncorneal_reflection_tracking.timestamps.timestamp_link # (list)\neye_tracking = spatial_series[\"eye_tracking\"]\neye_tracking # (EllipseSeries)\neye_tracking.resolution # (float64) -1.0\neye_tracking.comments # (str) no comments\neye_tracking.description # (str) no description\neye_tracking.conversion # (float64) 1.0\neye_tracking.offset # (float) 0.0\neye_tracking.unit # (str) meters\neye_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# eye_tracking.data[:, :] # Access all data\n# eye_tracking.data[0:n, :] # Access first n rows\n# eye_tracking.data[:, 0:n] # Access first n columns\neye_tracking.timestamps # (Dataset) shape (254226,); dtype float64\n# eye_tracking.timestamps[:] # Access all data\n# eye_tracking.timestamps[0:n] # Access first n elements\neye_tracking.timestamps_unit # (str) seconds\neye_tracking.interval # (int) 1\neye_tracking.reference_frame # (str) nose\neye_tracking.area # (Dataset) shape (254226,); dtype float64\n# eye_tracking.area[:] # Access all data\n# eye_tracking.area[0:n] # Access first n elements\neye_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# eye_tracking.area_raw[:] # Access all data\n# eye_tracking.area_raw[0:n] # Access first n elements\neye_tracking.width # (Dataset) shape (254226,); dtype float64\n# eye_tracking.width[:] # Access all data\n# eye_tracking.width[0:n] # Access first n elements\neye_tracking.height # (Dataset) shape (254226,); dtype float64\n# eye_tracking.height[:] # Access all data\n# eye_tracking.height[0:n] # Access first n elements\neye_tracking.angle # (Dataset) shape (254226,); dtype float64\n# eye_tracking.angle[:] # Access all data\n# eye_tracking.angle[0:n] # Access first n elements\neye_tracking.timestamp_link # (list)\npupil_tracking = spatial_series[\"pupil_tracking\"]\npupil_tracking # (EllipseSeries)\npupil_tracking.resolution # (float64) -1.0\npupil_tracking.comments # (str) no comments\npupil_tracking.description # (str) no description\npupil_tracking.conversion # (float64) 1.0\npupil_tracking.offset # (float) 0.0\npupil_tracking.unit # (str) meters\npupil_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# pupil_tracking.data[:, :] # Access all data\n# pupil_tracking.data[0:n, :] # Access first n rows\n# pupil_tracking.data[:, 0:n] # Access first n columns\npupil_tracking.timestamps_unit # (str) seconds\npupil_tracking.interval # (int) 1\npupil_tracking.reference_frame # (str) nose\npupil_tracking.area # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.area[:] # Access all data\n# pupil_tracking.area[0:n] # Access first n elements\npupil_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.area_raw[:] # Access all data\n# pupil_tracking.area_raw[0:n] # Access first n elements\npupil_tracking.width # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.width[:] # Access all data\n# pupil_tracking.width[0:n] # Access first n elements\npupil_tracking.height # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.height[:] # Access all data\n# pupil_tracking.height[0:n] # Access first n elements\npupil_tracking.angle # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.angle[:] # Access all data\n# pupil_tracking.angle[0:n] # Access first n elements\npupil_tracking.timestamps # (EllipseSeries)\npupil_tracking.timestamps.resolution # (float64) -1.0\npupil_tracking.timestamps.comments # (str) no comments\npupil_tracking.timestamps.description # (str) no description\npupil_tracking.timestamps.conversion # (float64) 1.0\npupil_tracking.timestamps.offset # (float) 0.0\npupil_tracking.timestamps.unit # (str) meters\npupil_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# pupil_tracking.timestamps.data[:, :] # Access all data\n# pupil_tracking.timestamps.data[0:n, :] # Access first n rows\n# pupil_tracking.timestamps.data[:, 0:n] # Access first n columns\npupil_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.timestamps[:] # Access all data\n# pupil_tracking.timestamps.timestamps[0:n] # Access first n elements\npupil_tracking.timestamps.timestamps_unit # (str) seconds\npupil_tracking.timestamps.interval # (int) 1\npupil_tracking.timestamps.reference_frame # (str) nose\npupil_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.area[:] # Access all data\n# pupil_tracking.timestamps.area[0:n] # Access first n elements\npupil_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.area_raw[:] # Access all data\n# pupil_tracking.timestamps.area_raw[0:n] # Access first n elements\npupil_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.width[:] # Access all data\n# pupil_tracking.timestamps.width[0:n] # Access first n elements\npupil_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.height[:] # Access all data\n# pupil_tracking.timestamps.height[0:n] # Access first n elements\npupil_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# pupil_tracking.timestamps.angle[:] # Access all data\n# pupil_tracking.timestamps.angle[0:n] # Access first n elements\npupil_tracking.timestamps.timestamp_link # (list)\nEyeTracking.eye_tracking # (EllipseSeries)\nEyeTracking.eye_tracking.resolution # (float64) -1.0\nEyeTracking.eye_tracking.comments # (str) no comments\nEyeTracking.eye_tracking.description # (str) no description\nEyeTracking.eye_tracking.conversion # (float64) 1.0\nEyeTracking.eye_tracking.offset # (float) 0.0\nEyeTracking.eye_tracking.unit # (str) meters\nEyeTracking.eye_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.eye_tracking.data[:, :] # Access all data\n# EyeTracking.eye_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.eye_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.eye_tracking.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.timestamps[:] # Access all data\n# EyeTracking.eye_tracking.timestamps[0:n] # Access first n elements\nEyeTracking.eye_tracking.timestamps_unit # (str) seconds\nEyeTracking.eye_tracking.interval # (int) 1\nEyeTracking.eye_tracking.reference_frame # (str) nose\nEyeTracking.eye_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.area[:] # Access all data\n# EyeTracking.eye_tracking.area[0:n] # Access first n elements\nEyeTracking.eye_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.area_raw[:] # Access all data\n# EyeTracking.eye_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.eye_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.width[:] # Access all data\n# EyeTracking.eye_tracking.width[0:n] # Access first n elements\nEyeTracking.eye_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.height[:] # Access all data\n# EyeTracking.eye_tracking.height[0:n] # Access first n elements\nEyeTracking.eye_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.eye_tracking.angle[:] # Access all data\n# EyeTracking.eye_tracking.angle[0:n] # Access first n elements\nEyeTracking.eye_tracking.timestamp_link # (list)\nEyeTracking.pupil_tracking # (EllipseSeries)\nEyeTracking.pupil_tracking.resolution # (float64) -1.0\nEyeTracking.pupil_tracking.comments # (str) no comments\nEyeTracking.pupil_tracking.description # (str) no description\nEyeTracking.pupil_tracking.conversion # (float64) 1.0\nEyeTracking.pupil_tracking.offset # (float) 0.0\nEyeTracking.pupil_tracking.unit # (str) meters\nEyeTracking.pupil_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.pupil_tracking.data[:, :] # Access all data\n# EyeTracking.pupil_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.pupil_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.pupil_tracking.timestamps_unit # (str) seconds\nEyeTracking.pupil_tracking.interval # (int) 1\nEyeTracking.pupil_tracking.reference_frame # (str) nose\nEyeTracking.pupil_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.area[:] # Access all data\n# EyeTracking.pupil_tracking.area[0:n] # Access first n elements\nEyeTracking.pupil_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.area_raw[:] # Access all data\n# EyeTracking.pupil_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.pupil_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.width[:] # Access all data\n# EyeTracking.pupil_tracking.width[0:n] # Access first n elements\nEyeTracking.pupil_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.height[:] # Access all data\n# EyeTracking.pupil_tracking.height[0:n] # Access first n elements\nEyeTracking.pupil_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.angle[:] # Access all data\n# EyeTracking.pupil_tracking.angle[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps # (EllipseSeries)\nEyeTracking.pupil_tracking.timestamps.resolution # (float64) -1.0\nEyeTracking.pupil_tracking.timestamps.comments # (str) no comments\nEyeTracking.pupil_tracking.timestamps.description # (str) no description\nEyeTracking.pupil_tracking.timestamps.conversion # (float64) 1.0\nEyeTracking.pupil_tracking.timestamps.offset # (float) 0.0\nEyeTracking.pupil_tracking.timestamps.unit # (str) meters\nEyeTracking.pupil_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.pupil_tracking.timestamps.data[:, :] # Access all data\n# EyeTracking.pupil_tracking.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.pupil_tracking.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.pupil_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.timestamps[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.timestamps_unit # (str) seconds\nEyeTracking.pupil_tracking.timestamps.interval # (int) 1\nEyeTracking.pupil_tracking.timestamps.reference_frame # (str) nose\nEyeTracking.pupil_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.area[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.area[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.area_raw[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.width[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.width[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.height[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.height[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.pupil_tracking.timestamps.angle[:] # Access all data\n# EyeTracking.pupil_tracking.timestamps.angle[0:n] # Access first n elements\nEyeTracking.pupil_tracking.timestamps.timestamp_link # (list)\nEyeTracking.corneal_reflection_tracking # (EllipseSeries)\nEyeTracking.corneal_reflection_tracking.resolution # (float64) -1.0\nEyeTracking.corneal_reflection_tracking.comments # (str) no comments\nEyeTracking.corneal_reflection_tracking.description # (str) no description\nEyeTracking.corneal_reflection_tracking.conversion # (float64) 1.0\nEyeTracking.corneal_reflection_tracking.offset # (float) 0.0\nEyeTracking.corneal_reflection_tracking.unit # (str) meters\nEyeTracking.corneal_reflection_tracking.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.corneal_reflection_tracking.data[:, :] # Access all data\n# EyeTracking.corneal_reflection_tracking.data[0:n, :] # Access first n rows\n# EyeTracking.corneal_reflection_tracking.data[:, 0:n] # Access first n columns\nEyeTracking.corneal_reflection_tracking.timestamps_unit # (str) seconds\nEyeTracking.corneal_reflection_tracking.interval # (int) 1\nEyeTracking.corneal_reflection_tracking.reference_frame # (str) nose\nEyeTracking.corneal_reflection_tracking.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.area[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.area[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.area_raw[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.area_raw[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.width[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.width[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.height[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.height[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.angle[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.angle[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps # (EllipseSeries)\nEyeTracking.corneal_reflection_tracking.timestamps.resolution # (float64) -1.0\nEyeTracking.corneal_reflection_tracking.timestamps.comments # (str) no comments\nEyeTracking.corneal_reflection_tracking.timestamps.description # (str) no description\nEyeTracking.corneal_reflection_tracking.timestamps.conversion # (float64) 1.0\nEyeTracking.corneal_reflection_tracking.timestamps.offset # (float) 0.0\nEyeTracking.corneal_reflection_tracking.timestamps.unit # (str) meters\nEyeTracking.corneal_reflection_tracking.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.data[:, :] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.corneal_reflection_tracking.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.corneal_reflection_tracking.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.timestamps[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.timestamps_unit # (str) seconds\nEyeTracking.corneal_reflection_tracking.timestamps.interval # (int) 1\nEyeTracking.corneal_reflection_tracking.timestamps.reference_frame # (str) nose\nEyeTracking.corneal_reflection_tracking.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.area[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.area[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.area_raw[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.width[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.width[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.height[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.height[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.corneal_reflection_tracking.timestamps.angle[:] # Access all data\n# EyeTracking.corneal_reflection_tracking.timestamps.angle[0:n] # Access first n elements\nEyeTracking.corneal_reflection_tracking.timestamps.timestamp_link # (list)\nEyeTracking.likely_blink # (TimeSeries)\nEyeTracking.likely_blink.resolution # (float64) -1.0\nEyeTracking.likely_blink.comments # (str) no comments\nEyeTracking.likely_blink.description # (str) blinks\nEyeTracking.likely_blink.conversion # (float64) 1.0\nEyeTracking.likely_blink.offset # (float) 0.0\nEyeTracking.likely_blink.unit # (str) N/A\nEyeTracking.likely_blink.data # (Dataset) shape (254226,); dtype bool\n# EyeTracking.likely_blink.data[:] # Access all data\n# EyeTracking.likely_blink.data[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps_unit # (str) seconds\nEyeTracking.likely_blink.interval # (int) 1\nEyeTracking.likely_blink.timestamps # (EllipseSeries)\nEyeTracking.likely_blink.timestamps.resolution # (float64) -1.0\nEyeTracking.likely_blink.timestamps.comments # (str) no comments\nEyeTracking.likely_blink.timestamps.description # (str) no description\nEyeTracking.likely_blink.timestamps.conversion # (float64) 1.0\nEyeTracking.likely_blink.timestamps.offset # (float) 0.0\nEyeTracking.likely_blink.timestamps.unit # (str) meters\nEyeTracking.likely_blink.timestamps.data # (Dataset) shape (254226, 2); dtype float64\n# EyeTracking.likely_blink.timestamps.data[:, :] # Access all data\n# EyeTracking.likely_blink.timestamps.data[0:n, :] # Access first n rows\n# EyeTracking.likely_blink.timestamps.data[:, 0:n] # Access first n columns\nEyeTracking.likely_blink.timestamps.timestamps # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.timestamps[:] # Access all data\n# EyeTracking.likely_blink.timestamps.timestamps[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.timestamps_unit # (str) seconds\nEyeTracking.likely_blink.timestamps.interval # (int) 1\nEyeTracking.likely_blink.timestamps.reference_frame # (str) nose\nEyeTracking.likely_blink.timestamps.area # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.area[:] # Access all data\n# EyeTracking.likely_blink.timestamps.area[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.area_raw # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.area_raw[:] # Access all data\n# EyeTracking.likely_blink.timestamps.area_raw[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.width # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.width[:] # Access all data\n# EyeTracking.likely_blink.timestamps.width[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.height # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.height[:] # Access all data\n# EyeTracking.likely_blink.timestamps.height[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.angle # (Dataset) shape (254226,); dtype float64\n# EyeTracking.likely_blink.timestamps.angle[:] # Access all data\n# EyeTracking.likely_blink.timestamps.angle[0:n] # Access first n elements\nEyeTracking.likely_blink.timestamps.timestamp_link # (list)\nraw_suite2p_motion_corrected = acquisition[\"raw_suite2p_motion_corrected\"]\nraw_suite2p_motion_corrected # (TwoPhotonSeries)\nraw_suite2p_motion_corrected.starting_time # (float64) 0.0\nraw_suite2p_motion_corrected.rate # (float64) 10.71\nraw_suite2p_motion_corrected.resolution # (float64) -1.0\nraw_suite2p_motion_corrected.comments # (str) no comments\nraw_suite2p_motion_corrected.description # (str) no description\nraw_suite2p_motion_corrected.conversion # (float64) 1.0\nraw_suite2p_motion_corrected.offset # (float) 0.0\nraw_suite2p_motion_corrected.unit # (str) SIunit\nraw_suite2p_motion_corrected.data # (Dataset) shape (40018, 512, 512); dtype int16\n# raw_suite2p_motion_corrected.data[:, :, :] # Access all data\n# raw_suite2p_motion_corrected.data[0, :, :] # Access first plane\nraw_suite2p_motion_corrected.starting_time_unit # (str) seconds\nraw_suite2p_motion_corrected.format # (str) raw\nraw_suite2p_motion_corrected.imaging_plane # (ImagingPlane)\nraw_suite2p_motion_corrected.imaging_plane.optical_channel # (list)\nraw_suite2p_motion_corrected.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\nraw_suite2p_motion_corrected.imaging_plane.excitation_lambda # (float64) 910.0\nraw_suite2p_motion_corrected.imaging_plane.imaging_rate # (float64) 10.0\nraw_suite2p_motion_corrected.imaging_plane.indicator # (str) GCaMP6f\nraw_suite2p_motion_corrected.imaging_plane.location # (str) VISpm\nraw_suite2p_motion_corrected.imaging_plane.conversion # (float) 1.0\nraw_suite2p_motion_corrected.imaging_plane.unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.origin_coords_unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.grid_spacing_unit # (str) meters\nraw_suite2p_motion_corrected.imaging_plane.device # (Device)\nraw_suite2p_motion_corrected.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nv_in = acquisition[\"v_in\"]\nv_in # (TimeSeries)\nv_in.resolution # (float64) -1.0\nv_in.comments # (str) no comments\nv_in.description # (str) The theoretical maximum voltage that the running wheel encoder will reach prior to \"wrapping\". Th...\nv_in.conversion # (float64) 1.0\nv_in.offset # (float) 0.0\nv_in.unit # (str) V\nv_in.data # (Dataset) shape (252000,); dtype float64\n# v_in.data[:] # Access all data\n# v_in.data[0:n] # Access first n elements\nv_in.timestamps # (Dataset) shape (252000,); dtype float64\n# v_in.timestamps[:] # Access all data\n# v_in.timestamps[0:n] # Access first n elements\nv_in.timestamps_unit # (str) seconds\nv_in.interval # (int) 1\nv_sig = acquisition[\"v_sig\"]\nv_sig # (TimeSeries)\nv_sig.resolution # (float64) -1.0\nv_sig.comments # (str) no comments\nv_sig.description # (str) Voltage signal from the running wheel encoder\nv_sig.conversion # (float64) 1.0\nv_sig.offset # (float) 0.0\nv_sig.unit # (str) V\nv_sig.data # (Dataset) shape (252000,); dtype float64\n# v_sig.data[:] # Access all data\n# v_sig.data[0:n] # Access first n elements\nv_sig.timestamps # (Dataset) shape (252000,); dtype float64\n# v_sig.timestamps[:] # Access all data\n# v_sig.timestamps[0:n] # Access first n elements\nv_sig.timestamps_unit # (str) seconds\nv_sig.interval # (int) 1\nnwb.keywords # (StrDataset) shape (5,); dtype object\n# nwb.keywords[:] # Access all data\n# nwb.keywords[0:n] # Access first n elements\n# First few values of nwb.keywords: ['2-photon' 'calcium imaging' 'visual cortex' 'behavior' 'task']\nnwb.processing # (LabelledDict)\nprocessing = nwb.processing\nophys = processing[\"ophys\"]\nophys # (ProcessingModule)\nophys.description # (str) Ophys processing module\nophys.data_interfaces # (LabelledDict)\ndata_interfaces = ophys.data_interfaces\ncorrected_fluorescence = data_interfaces[\"corrected_fluorescence\"]\ncorrected_fluorescence # (Fluorescence)\ncorrected_fluorescence.roi_response_series # (LabelledDict)\nroi_response_series = corrected_fluorescence.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\ndff = data_interfaces[\"dff\"]\ndff # (DfOverF)\ndff.roi_response_series # (LabelledDict)\nroi_response_series = dff.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\nevent_detection = data_interfaces[\"event_detection\"]\nevent_detection # (OphysEventDetection)\nevent_detection.resolution # (float64) -1.0\nevent_detection.comments # (str) no comments\nevent_detection.description # (str) no description\nevent_detection.conversion # (float64) 1.0\nevent_detection.offset # (float) 0.0\nevent_detection.unit # (str) N/A\nevent_detection.data # (Dataset) shape (40018, 18); dtype float64\n# event_detection.data[:, :] # Access all data\n# event_detection.data[0:n, :] # Access first n rows\n# event_detection.data[:, 0:n] # Access first n columns\nevent_detection.timestamps # (Dataset) shape (40018,); dtype float64\n# event_detection.timestamps[:] # Access all data\n# event_detection.timestamps[0:n] # Access first n elements\nevent_detection.timestamps_unit # (str) seconds\nevent_detection.interval # (int) 1\nevent_detection.lambdas # (Dataset) shape (18,); dtype float64\n# event_detection.lambdas[:] # Access all data\n# event_detection.lambdas[0:n] # Access first n elements\n# First few values of event_detection.lambdas: [0.0287 0.0128 0.0086 0.0289 0.0335 0.0079 0.0203 0.0221 0.0168 0.0097]\nevent_detection.noise_stds # (Dataset) shape (18,); dtype float64\n# event_detection.noise_stds[:] # Access all data\n# event_detection.noise_stds[0:n] # Access first n elements\n# First few values of event_detection.noise_stds: [0.06418523 0.04296032 0.03497644 0.06435245 0.06978224 0.03365562  0.05395723 0.0563328  0.04906347 0.03729971]\nevent_detection.rois # (DynamicTableRegion)\nevent_detection.rois.description # (str) Cells with detected events\nevent_detection.rois.table # (PlaneSegmentation)\nevent_detection.rois.table.description # (str) Segmented rois\nevent_detection.rois.table.colnames # (tuple)\nevent_detection.rois.table.columns # (tuple)\nevent_detection.rois.table.id # (ElementIdentifiers)\nevent_detection.rois.table.imaging_plane # (ImagingPlane)\nevent_detection.rois.table.imaging_plane.optical_channel # (list)\nevent_detection.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\nevent_detection.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\nevent_detection.rois.table.imaging_plane.imaging_rate # (float64) 10.0\nevent_detection.rois.table.imaging_plane.indicator # (str) GCaMP6f\nevent_detection.rois.table.imaging_plane.location # (str) VISpm\nevent_detection.rois.table.imaging_plane.conversion # (float) 1.0\nevent_detection.rois.table.imaging_plane.unit # (str) meters\nevent_detection.rois.table.imaging_plane.origin_coords_unit # (str) meters\nevent_detection.rois.table.imaging_plane.grid_spacing_unit # (str) meters\nevent_detection.rois.table.imaging_plane.device # (Device)\nevent_detection.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# event_detection.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# event_detection.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nevent_detection.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\nevent_detection.rois.table.height # (VectorData) Height of ROI in pixels\nevent_detection.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\nevent_detection.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\nevent_detection.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\nevent_detection.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\nevent_detection.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\nevent_detection.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\nevent_detection.rois.table.width # (VectorData) Width of ROI in pixels\nevent_detection.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\nevent_detection.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\nevent_detection.rois.table.image_mask # (VectorData) Image masks for each ROI\nimage_segmentation = data_interfaces[\"image_segmentation\"]\nimage_segmentation # (ImageSegmentation)\nimage_segmentation.plane_segmentations # (LabelledDict)\nplane_segmentations = image_segmentation.plane_segmentations\ncell_specimen_table = plane_segmentations[\"cell_specimen_table\"]\ncell_specimen_table # (PlaneSegmentation)\ncell_specimen_table.description # (str) Segmented rois\ncell_specimen_table.colnames # (tuple)\ncell_specimen_table.columns # (tuple)\ncell_specimen_table.id # (ElementIdentifiers)\ncell_specimen_table.imaging_plane # (ImagingPlane)\ncell_specimen_table.imaging_plane.optical_channel # (list)\ncell_specimen_table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ncell_specimen_table.imaging_plane.excitation_lambda # (float64) 910.0\ncell_specimen_table.imaging_plane.imaging_rate # (float64) 10.0\ncell_specimen_table.imaging_plane.indicator # (str) GCaMP6f\ncell_specimen_table.imaging_plane.location # (str) VISpm\ncell_specimen_table.imaging_plane.conversion # (float) 1.0\ncell_specimen_table.imaging_plane.unit # (str) meters\ncell_specimen_table.imaging_plane.origin_coords_unit # (str) meters\ncell_specimen_table.imaging_plane.grid_spacing_unit # (str) meters\ncell_specimen_table.imaging_plane.device # (Device)\ncell_specimen_table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# cell_specimen_table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# cell_specimen_table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ncell_specimen_table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ncell_specimen_table.height # (VectorData) Height of ROI in pixels\ncell_specimen_table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ncell_specimen_table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ncell_specimen_table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ncell_specimen_table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ncell_specimen_table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ncell_specimen_table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ncell_specimen_table.width # (VectorData) Width of ROI in pixels\ncell_specimen_table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ncell_specimen_table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ncell_specimen_table.image_mask # (VectorData) Image masks for each ROI\nimages = data_interfaces[\"images\"]\nimages # (Images)\nimages.description # (str) no description\nimages.images # (LabelledDict)\nimages_1 = images.images\naverage_image = images_1[\"average_image\"]\naverage_image # (GrayscaleImage)\nmax_projection = images_1[\"max_projection\"]\nmax_projection # (GrayscaleImage)\nsegmentation_mask_image = images_1[\"segmentation_mask_image\"]\nsegmentation_mask_image # (GrayscaleImage)\nneuropil_trace = data_interfaces[\"neuropil_trace\"]\nneuropil_trace # (Fluorescence)\nneuropil_trace.roi_response_series # (LabelledDict)\nroi_response_series = neuropil_trace.roi_response_series\ntraces = roi_response_series[\"traces\"]\ntraces # (RoiResponseSeries)\ntraces.resolution # (float64) -1.0\ntraces.comments # (str) no comments\ntraces.description # (str) no description\ntraces.conversion # (float64) 1.0\ntraces.offset # (float) 0.0\ntraces.unit # (str) NA\ntraces.data # (Dataset) shape (40018, 18); dtype float64\n# traces.data[:, :] # Access all data\n# traces.data[0:n, :] # Access first n rows\n# traces.data[:, 0:n] # Access first n columns\ntraces.timestamps # (Dataset) shape (40018,); dtype float64\n# traces.timestamps[:] # Access all data\n# traces.timestamps[0:n] # Access first n elements\ntraces.timestamps_unit # (str) seconds\ntraces.interval # (int) 1\ntraces.rois # (DynamicTableRegion)\ntraces.rois.description # (str) segmented cells labeled by cell_specimen_id\ntraces.rois.table # (PlaneSegmentation)\ntraces.rois.table.description # (str) Segmented rois\ntraces.rois.table.colnames # (tuple)\ntraces.rois.table.columns # (tuple)\ntraces.rois.table.id # (ElementIdentifiers)\ntraces.rois.table.imaging_plane # (ImagingPlane)\ntraces.rois.table.imaging_plane.optical_channel # (list)\ntraces.rois.table.imaging_plane.description # (str) (512, 512) field of view in VISpm at depth 271 um\ntraces.rois.table.imaging_plane.excitation_lambda # (float64) 910.0\ntraces.rois.table.imaging_plane.imaging_rate # (float64) 10.0\ntraces.rois.table.imaging_plane.indicator # (str) GCaMP6f\ntraces.rois.table.imaging_plane.location # (str) VISpm\ntraces.rois.table.imaging_plane.conversion # (float) 1.0\ntraces.rois.table.imaging_plane.unit # (str) meters\ntraces.rois.table.imaging_plane.origin_coords_unit # (str) meters\ntraces.rois.table.imaging_plane.grid_spacing_unit # (str) meters\ntraces.rois.table.imaging_plane.device # (Device)\ntraces.rois.table.imaging_plane.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\n# traces.rois.table.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 18 rows and 12 columns\n# traces.rois.table.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ntraces.rois.table.cell_specimen_id # (VectorData) Unified id of segmented cell across experiments (after cell matching)\ntraces.rois.table.height # (VectorData) Height of ROI in pixels\ntraces.rois.table.mask_image_plane # (VectorData) Which image plane an ROI resides on. Overlapping ROIs are stored on different mask image planes.\ntraces.rois.table.max_correction_down # (VectorData) Max motion correction in down direction in pixels\ntraces.rois.table.max_correction_left # (VectorData) Max motion correction in left direction in pixels\ntraces.rois.table.max_correction_right # (VectorData) Max motion correction in right direction in pixels\ntraces.rois.table.max_correction_up # (VectorData) Max motion correction in up direction in pixels\ntraces.rois.table.valid_roi # (VectorData) Indicates if cell classification found the ROI to be a cell or not\ntraces.rois.table.width # (VectorData) Width of ROI in pixels\ntraces.rois.table.x # (VectorData) x position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.y # (VectorData) y position of ROI in Image Plane in pixels (top left corner)\ntraces.rois.table.image_mask # (VectorData) Image masks for each ROI\nophys_motion_correction_x = data_interfaces[\"ophys_motion_correction_x\"]\nophys_motion_correction_x # (TimeSeries)\nophys_motion_correction_x.resolution # (float64) -1.0\nophys_motion_correction_x.comments # (str) no comments\nophys_motion_correction_x.description # (str) no description\nophys_motion_correction_x.conversion # (float64) 1.0\nophys_motion_correction_x.offset # (float) 0.0\nophys_motion_correction_x.unit # (str) pixels\nophys_motion_correction_x.data # (Dataset) shape (40018,); dtype int64\n# ophys_motion_correction_x.data[:] # Access all data\n# ophys_motion_correction_x.data[0:n] # Access first n elements\nophys_motion_correction_x.timestamps # (Dataset) shape (40018,); dtype float64\n# ophys_motion_correction_x.timestamps[:] # Access all data\n# ophys_motion_correction_x.timestamps[0:n] # Access first n elements\nophys_motion_correction_x.timestamps_unit # (str) seconds\nophys_motion_correction_x.interval # (int) 1\nophys_motion_correction_y = data_interfaces[\"ophys_motion_correction_y\"]\nophys_motion_correction_y # (TimeSeries)\nophys_motion_correction_y.resolution # (float64) -1.0\nophys_motion_correction_y.comments # (str) no comments\nophys_motion_correction_y.description # (str) no description\nophys_motion_correction_y.conversion # (float64) 1.0\nophys_motion_correction_y.offset # (float) 0.0\nophys_motion_correction_y.unit # (str) pixels\nophys_motion_correction_y.data # (Dataset) shape (40018,); dtype int64\n# ophys_motion_correction_y.data[:] # Access all data\n# ophys_motion_correction_y.data[0:n] # Access first n elements\nophys_motion_correction_y.timestamps # (Dataset) shape (40018,); dtype float64\n# ophys_motion_correction_y.timestamps[:] # Access all data\n# ophys_motion_correction_y.timestamps[0:n] # Access first n elements\nophys_motion_correction_y.timestamps_unit # (str) seconds\nophys_motion_correction_y.interval # (int) 1\nrunning = processing[\"running\"]\nrunning # (ProcessingModule)\nrunning.description # (str) Running speed processing module\nrunning.data_interfaces # (LabelledDict)\ndata_interfaces = running.data_interfaces\ndx = data_interfaces[\"dx\"]\ndx # (TimeSeries)\ndx.resolution # (float64) -1.0\ndx.comments # (str) no comments\ndx.description # (str) Running wheel angular change, computed during data collection\ndx.conversion # (float64) 1.0\ndx.offset # (float) 0.0\ndx.unit # (str) cm\ndx.data # (Dataset) shape (252000,); dtype float32\n# dx.data[:] # Access all data\n# dx.data[0:n] # Access first n elements\ndx.timestamps # (Dataset) shape (252000,); dtype float64\n# dx.timestamps[:] # Access all data\n# dx.timestamps[0:n] # Access first n elements\ndx.timestamps_unit # (str) seconds\ndx.interval # (int) 1\nspeed = data_interfaces[\"speed\"]\nspeed # (TimeSeries)\nspeed.resolution # (float64) -1.0\nspeed.comments # (str) no comments\nspeed.description # (str) no description\nspeed.conversion # (float64) 1.0\nspeed.offset # (float) 0.0\nspeed.unit # (str) cm/s\nspeed.data # (Dataset) shape (252000,); dtype float64\n# speed.data[:] # Access all data\n# speed.data[0:n] # Access first n elements\nspeed.timestamps # (Dataset) shape (252000,); dtype float64\n# speed.timestamps[:] # Access all data\n# speed.timestamps[0:n] # Access first n elements\nspeed.timestamps_unit # (str) seconds\nspeed.interval # (int) 1\nspeed_unfiltered = data_interfaces[\"speed_unfiltered\"]\nspeed_unfiltered # (TimeSeries)\nspeed_unfiltered.resolution # (float64) -1.0\nspeed_unfiltered.comments # (str) no comments\nspeed_unfiltered.description # (str) no description\nspeed_unfiltered.conversion # (float64) 1.0\nspeed_unfiltered.offset # (float) 0.0\nspeed_unfiltered.unit # (str) cm/s\nspeed_unfiltered.data # (Dataset) shape (252000,); dtype float64\n# speed_unfiltered.data[:] # Access all data\n# speed_unfiltered.data[0:n] # Access first n elements\nspeed_unfiltered.timestamps # (Dataset) shape (252000,); dtype float64\n# speed_unfiltered.timestamps[:] # Access all data\n# speed_unfiltered.timestamps[0:n] # Access first n elements\nspeed_unfiltered.timestamps_unit # (str) seconds\nspeed_unfiltered.interval # (int) 1\nstimulus = processing[\"stimulus\"]\nstimulus # (ProcessingModule)\nstimulus.description # (str) Stimulus Times processing\nstimulus.data_interfaces # (LabelledDict)\ndata_interfaces = stimulus.data_interfaces\ntimestamps = data_interfaces[\"timestamps\"]\ntimestamps # (TimeSeries)\ntimestamps.resolution # (float64) -1.0\ntimestamps.comments # (str) no comments\ntimestamps.description # (str) no description\ntimestamps.conversion # (float64) 1.0\ntimestamps.offset # (float) 0.0\ntimestamps.unit # (str) s\ntimestamps.data # (Dataset) shape (252000,); dtype float64\n# timestamps.data[:] # Access all data\n# timestamps.data[0:n] # Access first n elements\ntimestamps.timestamps # (Dataset) shape (252000,); dtype float64\n# timestamps.timestamps[:] # Access all data\n# timestamps.timestamps[0:n] # Access first n elements\ntimestamps.timestamps_unit # (str) seconds\ntimestamps.interval # (int) 1\nstimulus_ophys = processing[\"stimulus_ophys\"]\nstimulus_ophys # (ProcessingModule)\nstimulus_ophys.description # (str) Stimulus Times processing\nstimulus_ophys.data_interfaces # (LabelledDict)\ndata_interfaces = stimulus_ophys.data_interfaces\ntimestamps = data_interfaces[\"timestamps\"]\ntimestamps # (TimeSeries)\ntimestamps.resolution # (float64) -1.0\ntimestamps.comments # (str) no comments\ntimestamps.description # (str) no description\ntimestamps.conversion # (float64) 1.0\ntimestamps.offset # (float) 0.0\ntimestamps.unit # (str) s\ntimestamps.data # (Dataset) shape (252000,); dtype float64\n# timestamps.data[:] # Access all data\n# timestamps.data[0:n] # Access first n elements\ntimestamps.timestamps # (Dataset) shape (252000,); dtype float64\n# timestamps.timestamps[:] # Access all data\n# timestamps.timestamps[0:n] # Access first n elements\ntimestamps.timestamps_unit # (str) seconds\ntimestamps.interval # (int) 1\nnwb.devices # (LabelledDict)\ndevices = nwb.devices\nMESO_2 = devices[\"MESO.2\"]\nMESO_2 # (Device)\nMESO_2.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nnwb.imaging_planes # (LabelledDict)\nimaging_planes = nwb.imaging_planes\nimaging_plane_1 = imaging_planes[\"imaging_plane_1\"]\nimaging_plane_1 # (ImagingPlane)\nimaging_plane_1.optical_channel # (list)\nimaging_plane_1.description # (str) (512, 512) field of view in VISpm at depth 271 um\nimaging_plane_1.excitation_lambda # (float64) 910.0\nimaging_plane_1.imaging_rate # (float64) 10.0\nimaging_plane_1.indicator # (str) GCaMP6f\nimaging_plane_1.location # (str) VISpm\nimaging_plane_1.conversion # (float) 1.0\nimaging_plane_1.unit # (str) meters\nimaging_plane_1.origin_coords_unit # (str) meters\nimaging_plane_1.grid_spacing_unit # (str) meters\nimaging_plane_1.device # (Device)\nimaging_plane_1.device.description # (str) Allen Brain Observatory - Mesoscope 2P Rig\nnwb.intervals # (LabelledDict)\nintervals = nwb.intervals\ngray_presentations = intervals[\"gray_presentations\"]\ngray_presentations # (TimeIntervals)\ngray_presentations.description # (str) Presentation times and stimuli details for 'gray' stimuli. \\nNote: image_name references control_...\ngray_presentations.colnames # (tuple)\ngray_presentations.columns # (tuple)\ngray_presentations.id # (ElementIdentifiers)\n# gray_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# gray_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\ngray_presentations.start_time # (VectorData) Start time of epoch, in seconds\ngray_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\ngray_presentations.stimulus_name # (VectorData) Name of stimulus\ngray_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\ngray_presentations.frame # (VectorData) Frame of movie stimulus\ngray_presentations.color # (VectorData) No description\ngray_presentations.contrast # (VectorData) Contrast of stimulus\ngray_presentations.opacity # (VectorData) Opacity of stimulus\ngray_presentations.orientation # (VectorData) Orientation of stimulus\ngray_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\ngray_presentations.units # (VectorData) Units of stimulus size\ngray_presentations.stimulus_index # (VectorData) Index of stimulus type\ngray_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# gray_presentations.tags_index[0] # (ndarray)\n# gray_presentations.tags_index[1] # (ndarray)\n# gray_presentations.tags_index[2] # (ndarray)\n# gray_presentations.tags_index[3] # (ndarray)\n# ...\ngray_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# gray_presentations.timeseries_index[0] # (list)\n# gray_presentations.timeseries_index[1] # (list)\n# gray_presentations.timeseries_index[2] # (list)\n# gray_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_A_presentations = intervals[\"movie_clip_A_presentations\"]\nmovie_clip_A_presentations # (TimeIntervals)\nmovie_clip_A_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_A' stimuli. \\nNote: image_name references ...\nmovie_clip_A_presentations.colnames # (tuple)\nmovie_clip_A_presentations.columns # (tuple)\nmovie_clip_A_presentations.id # (ElementIdentifiers)\n# movie_clip_A_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_A_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_A_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_A_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_A_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_A_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_A_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_A_presentations.color # (VectorData) No description\nmovie_clip_A_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_A_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_A_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_A_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_A_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_A_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_A_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_A_presentations.tags_index[0] # (ndarray)\n# movie_clip_A_presentations.tags_index[1] # (ndarray)\n# movie_clip_A_presentations.tags_index[2] # (ndarray)\n# movie_clip_A_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_A_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_A_presentations.timeseries_index[0] # (list)\n# movie_clip_A_presentations.timeseries_index[1] # (list)\n# movie_clip_A_presentations.timeseries_index[2] # (list)\n# movie_clip_A_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_B_presentations = intervals[\"movie_clip_B_presentations\"]\nmovie_clip_B_presentations # (TimeIntervals)\nmovie_clip_B_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_B' stimuli. \\nNote: image_name references ...\nmovie_clip_B_presentations.colnames # (tuple)\nmovie_clip_B_presentations.columns # (tuple)\nmovie_clip_B_presentations.id # (ElementIdentifiers)\n# movie_clip_B_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_B_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_B_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_B_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_B_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_B_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_B_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_B_presentations.color # (VectorData) No description\nmovie_clip_B_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_B_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_B_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_B_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_B_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_B_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_B_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_B_presentations.tags_index[0] # (ndarray)\n# movie_clip_B_presentations.tags_index[1] # (ndarray)\n# movie_clip_B_presentations.tags_index[2] # (ndarray)\n# movie_clip_B_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_B_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_B_presentations.timeseries_index[0] # (list)\n# movie_clip_B_presentations.timeseries_index[1] # (list)\n# movie_clip_B_presentations.timeseries_index[2] # (list)\n# movie_clip_B_presentations.timeseries_index[3] # (list)\n# ...\nmovie_clip_C_presentations = intervals[\"movie_clip_C_presentations\"]\nmovie_clip_C_presentations # (TimeIntervals)\nmovie_clip_C_presentations.description # (str) Presentation times and stimuli details for 'movie_clip_C' stimuli. \\nNote: image_name references ...\nmovie_clip_C_presentations.colnames # (tuple)\nmovie_clip_C_presentations.columns # (tuple)\nmovie_clip_C_presentations.id # (ElementIdentifiers)\n# movie_clip_C_presentations.to_dataframe() # (DataFrame) Convert to a pandas DataFrame with 63000 rows and 16 columns\n# movie_clip_C_presentations.to_dataframe().head() # (DataFrame) Show the first few rows of the pandas DataFrame\nmovie_clip_C_presentations.start_time # (VectorData) Start time of epoch, in seconds\nmovie_clip_C_presentations.stop_time # (VectorData) Stop time of epoch, in seconds\nmovie_clip_C_presentations.stimulus_name # (VectorData) Name of stimulus\nmovie_clip_C_presentations.stimulus_block # (VectorData) Index of contiguous presentations of one stimulus type\nmovie_clip_C_presentations.frame # (VectorData) Frame of movie stimulus\nmovie_clip_C_presentations.color # (VectorData) No description\nmovie_clip_C_presentations.contrast # (VectorData) Contrast of stimulus\nmovie_clip_C_presentations.opacity # (VectorData) Opacity of stimulus\nmovie_clip_C_presentations.orientation # (VectorData) Orientation of stimulus\nmovie_clip_C_presentations.size # (VectorData) Size of stimulus (see \u2018units\u2019 field for units)\nmovie_clip_C_presentations.units # (VectorData) Units of stimulus size\nmovie_clip_C_presentations.stimulus_index # (VectorData) Index of stimulus type\nmovie_clip_C_presentations.tags # (VectorIndex) Index for VectorData 'tags'\n# movie_clip_C_presentations.tags_index[0] # (ndarray)\n# movie_clip_C_presentations.tags_index[1] # (ndarray)\n# movie_clip_C_presentations.tags_index[2] # (ndarray)\n# movie_clip_C_presentations.tags_index[3] # (ndarray)\n# ...\nmovie_clip_C_presentations.timeseries # (VectorIndex) Index for VectorData 'timeseries'\n# movie_clip_C_presentations.timeseries_index[0] # (list)\n# movie_clip_C_presentations.timeseries_index[1] # (list)\n# movie_clip_C_presentations.timeseries_index[2] # (list)\n# movie_clip_C_presentations.timeseries_index[3] # (list)\n# ...\nnwb.lab_meta_data # (LabelledDict)\nlab_meta_data = nwb.lab_meta_data\nmetadata = lab_meta_data[\"metadata\"]\nmetadata # (OphysMetadata)\nmetadata.ophys_experiment_id # (int64)\nmetadata.ophys_session_id # (int64)\nmetadata.experiment_container_id # (int64)\nmetadata.imaging_depth # (int64)\nmetadata.field_of_view_width # (int64)\nmetadata.field_of_view_height # (int64)\nmetadata.imaging_plane_group # (int64)\nmetadata.imaging_plane_group_count # (int64)\nnwb.experiment_description # (str) ophys session\nnwb.institution # (str) Allen Institute for Brain Science\nnwb.surgery # (str)  Structure: VISpm\nnwb.subject # (Subject)\nnwb.subject.age # (str) P108.0D\nnwb.subject.description # (str) external: 684475 donor_id: (1281520997,) specimen_id: ['684475']\nnwb.subject.genotype # (str) Cux2-CreERT2/wt;Camk2a-tTA/wt;Ai93(TITL-GCaMP6f)/wt\nnwb.subject.sex # (str) M\nnwb.subject.species # (str) Mus musculus\nnwb.subject.subject_id # (str) 684475"
      }
    ]
  },
  {
    "role": "user",
    "content": "On the basis of the above information, please create a jupytext notebook that introduces dandiset 000617 using the format described below.\n\nThe purpose of the notebook is to help researchers explore the dandiset and get started with a reanalysis.\n\nStart with an appropriate title for the notebook, such as \"Exploring Dandiset 000617: ...\" (that should be a markdown heading).\n\nInform the user that the notebook was generated with the assistance of AI, and that they should be cautious when interpreting the code or results.\n\nProvide an overview of the Dandiset. Include a link to the Dandiset of the form https://dandiarchive.org/dandiset/000617/0.250312.0130.\n\nSummarize what the notebook covers. This should be internally consistent with the notebook you are creating, even if it differs from what is specified in the above information.\n\nList the packages that are required to run the notebook. Assume that these are already installed on the user's system. Do not include any pip install commands in the notebook.\n\nShow how to use the DANDI Python API to information about the dandiset using code similar to the following:\n\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"001333\", \"0.250327.2220\")\nmetadata = dandiset.get_raw_metadata()\nprint(f\"Dandiset name: {metadata['name']}\")\nprint(f\"Dandiset URL: {metadata['url']}\")\n\nShow how to use the DANDI Python API to explore the .nwb files in the dandiset.\n\nShow how to load and visualize data from the dandiset based on the above information. Feel free to organize things differently from how they are in the above information, but do not make up new information.\n\nGenerate good quality plots (without being redundant).\n\nLoad NWB data by streaming the remote file (as done in the above information) rather than downloading it.\n\nDo not use any functionality of pynwb or dandi that is not covered in the above information.\n\nYou should stick to material that is covered in the above information and do not hallucinate.\n\nThroughout the notebook, include explanatory markdown cells that guide the user through the process.\n\nThe notebook should be well-documented, and follow best practices Include comments in code cells to explain what each step does.\n\nThe Jupytext should use `# %% [markdown]` for markdown cells and `# %%` delimiters for the code cells.\n\nIf any NWB files have units objects, you should know the following:\nunits.spike_times_index[i] provides the vector of spike times for the i^th unit. It is actually not an index. Do not use units.spike_times.\n\nDo not render or display the nwb object obtained from NWBHDF5IO directly in the notebook as the output could be very large.\n\nUse concise scientific language.\n\nPrioritize correctness and accuracy over verbosity and complexity.\n\nDo not speculate about things that are not supported by the above information.\n\nSTRICT RULE: If a chat conversation is provided above, do not include new analyses that are not covered in that conversation.\n\nSTRICT RULE: If a chat conversation is provided above, you should learn from any feedback in the conversation to avoid pitfalls in plots.\n\nDo not be verbose in your summary or wrap up of the notebook, although you may briefly suggest some general ideas for future exploration.\n\nThere is no need to close files or clean up resources at the end of the notebook.\n\nYour notebook will be evaluated based on the following criteria:\n1. How well did the notebook help you understand the purpose and content of the Dandiset?\n2. After reviewing the notebook, do you feel confident in how to access the different types of data from this Dandiset?\n3. Did the notebook help you understand the structure of the NWB file(s) and how to work with them?\n4. Did the visualizations in the notebook generally help you understand key aspects of the data?\n5. Did any of the visualizations make it harder to understand the data (e.g., due to poor formatting, unclear axes, or misleading displays)?\n6. Do you feel more confident creating your own visualizations of the data after seeing the examples in the notebook?\n7. How well did the visualizations show the structure or complexity of the data?\n8. Were there any interpretations or conclusions in the notebook that felt unclear or not well supported by the data shown?\n9. Did any of the plots or examples feel unnecessarily repetitive or redundant?\n10. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n11. How clear and easy was the notebook to follow?\n12. Did the notebook provide code you could easily reuse or adapt to explore the Dandiset yourself?\n13. Did the notebook help you understand what kinds of questions or analyses you could do next with this Dandiset?\n14. Overall, how helpful was this notebook for getting started with this Dandiset?\n\nYour output must be in the format:\n\n<notebook>\nThe text of the jupytext notebook should appear here.\n</notebook>\n\nNo other text should be included in the response.\n"
  },
  {
    "role": "assistant",
    "content": "<notebook>\n# %% [markdown]\n# # Exploring Dandiset 000617: Allen Institute Openscope - Sequence Learning Project\n\n# %% [markdown]\n# > **Note**: This notebook was generated with the assistance of AI. Please be cautious when interpreting the code or results, and validate any findings with your own analysis.\n\n# %% [markdown]\n# ## Overview of the Dandiset\n# \n# This dandiset (DANDI:000617) contains two-photon calcium imaging data from the [Allen Institute Openscope - Sequence Learning Project](https://dandiarchive.org/dandiset/000617/0.250312.0130).\n# \n# The experiment investigates how neural circuits in the mouse visual cortex learn to predict sequences of visual stimuli. In this experiment, head-fixed mice passively viewed sequences of three different natural movie clips (labeled 'A', 'B', 'C'), each having a duration of 2 seconds. The experiment begins with a baseline recording where the movie clips are presented in random order along with a 2-second grey screen (labeled 'X'). Subsequently, mice undergo training sessions where the movie clips are presented in a repeating temporal sequence (ABCABC...) to encourage sequence learning.\n# \n# The experiment uses two-photon microscopy with simultaneous recordings from eight fields-of-view, including both layer 2/3 and layer 4 neurons from multiple visual cortical areas: V1 (VISp), LM (VISl), AM (VISam), and PM (VISpm). The mouse line used in the experiment is Cux2-CreERTS2:Camk2a-tTa; Ai93(TITL-GCaMP6f), which has GCaMP6f expression in excitatory neurons of both layer 4 and layer 2/3.\n\n# %% [markdown]\n# ## What This Notebook Covers\n# \n# In this notebook, we will:\n# \n# 1. Connect to the DANDI Archive and retrieve metadata about the dataset\n# 2. Explore the structure of the NWB files in the dataset\n# 3. Load and examine the calcium imaging data from one recording session\n# 4. Visualize neural responses to the different movie stimuli\n# 5. Analyze eye tracking and running behavior data\n# 6. Explore the relationship between neural activity and behavioral measurements\n# \n# By the end of this notebook, you should have a good understanding of the data structure and how to access and analyze the various types of data in this dataset.\n\n# %% [markdown]\n# ## Required Packages\n\n# %%\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport h5py\nimport remfile\nimport pynwb\nfrom dandi.dandiapi import DandiAPIClient\n\n# Use a cleaner plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = [10, 6]\nplt.rcParams['font.size'] = 12\n\n# %% [markdown]\n# ## Exploring the Dandiset Metadata\n\n# %%\n# Connect to the DANDI Archive and retrieve metadata\nclient = DandiAPIClient()\ndandiset = client.get_dandiset(\"000617\", \"0.250312.0130\")\nmetadata = dandiset.get_raw_metadata()\n\nprint(f\"Dandiset name: {metadata['name']}\")\nprint(f\"Dandiset URL: {metadata['url']}\")\nprint(f\"Dandiset DOI: {metadata['doi']}\")\nprint(f\"Version: {metadata['version']}\")\n\n# %% [markdown]\n# Let's extract some additional information from the metadata:\n\n# %%\nprint(\"Description:\")\nprint(\"------------\")\n# Format the description for better readability\ndescription_lines = metadata['description'].split(\"\\n\")\nfor line in description_lines:\n    print(line.strip())\n    \nprint(\"\\nKeywords:\")\nprint(\"---------\")\nprint(\", \".join(metadata['keywords']))\n\n# %% [markdown]\n# ## Exploring Files in the Dandiset\n\n# %%\n# Get a list of assets in the dandiset\nassets = list(dandiset.get_assets())\n\n# Print the total number of files\nprint(f\"Total number of files: {len(assets)}\")\n\n# Get file paths for a specific subject\nsubject_files = [asset.path for asset in assets if \"sub-684475\" in asset.path]\nprint(f\"\\nNumber of files for subject 684475: {len(subject_files)}\")\n\n# Print a few example files\nprint(\"\\nExample files:\")\nfor file in subject_files[:10]:\n    print(f\"  {file}\")\n\n# %% [markdown]\n# Let's examine the file structure more closely to understand the organization of the dataset:\n\n# %%\n# Extract session IDs from the filenames\nsessions = set()\nfor file in subject_files:\n    if \"ses-\" in file:\n        session_id = file.split(\"ses-\")[1].split(\"-\")[0]\n        sessions.add(session_id)\n\nprint(f\"Subject 684475 has {len(sessions)} recording sessions:\")\nfor session in sorted(sessions):\n    print(f\"  Session {session}\")\n\n# Count files by acquisition\nacquisition_counts = {}\nfor file in subject_files:\n    if \"acq-\" in file:\n        acq_id = file.split(\"acq-\")[1].split(\"-\")[0]\n        acquisition_counts[acq_id] = acquisition_counts.get(acq_id, 0) + 1\n\nprint(\"\\nFiles by acquisition ID:\")\nfor acq_id, count in acquisition_counts.items():\n    print(f\"  Acquisition {acq_id}: {count} files\")\n\n# %% [markdown]\n# ## Loading and Exploring an NWB File\n\n# %% [markdown]\n# Let's load an NWB file from the dataset and explore its structure:\n\n# %%\n# Let's use one of the files we saw earlier\nfile_path = \"sub-684475/sub-684475_ses-1294084428-acq-1294179945-raw-movies_ophys.nwb\"\n\n# Get the download URL for the asset\nasset = next(dandiset.get_assets_by_glob(file_path))\nurl = asset.download_url\n\n# Use remfile to access the remote file without downloading it\nremote_file = remfile.File(url)\nh5_file = h5py.File(remote_file)\nio = pynwb.NWBHDF5IO(file=h5_file)\nnwb = io.read()\n\n# Print basic information about the NWB file\nprint(f\"Session ID: {nwb.identifier}\")\nprint(f\"Session description: {nwb.session_description}\")\nprint(f\"Session start time: {nwb.session_start_time}\")\nprint(f\"Experiment description: {nwb.experiment_description}\")\nprint(f\"Institution: {nwb.institution}\")\n\n# %% [markdown]\n# Let's examine the subject information:\n\n# %%\n# Print subject information\nsubject = nwb.subject\nprint(f\"Subject ID: {subject.subject_id}\")\nprint(f\"Age: {subject.age}\")\nprint(f\"Sex: {subject.sex}\")\nprint(f\"Species: {subject.species}\")\nprint(f\"Genotype: {subject.genotype}\")\nprint(f\"Description: {subject.description}\")\n\n# %% [markdown]\n# ## Exploring Neural Activity Data\n\n# %% [markdown]\n# Let's examine the calcium imaging data available in the file:\n\n# %%\n# Check what processing modules are available\nprint(\"Available processing modules:\")\nfor module_name in nwb.processing:\n    print(f\"  {module_name}\")\n\n# %% [markdown]\n# Let's look at the ophys module, which contains the calcium imaging data:\n\n# %%\n# Get the ophys module\nophys = nwb.processing[\"ophys\"]\nprint(\"Data interfaces in the ophys module:\")\nfor name in ophys.data_interfaces:\n    print(f\"  {name}\")\n\n# %% [markdown]\n# ### Examining the segmented ROIs (cells)\n\n# %%\n# Get information about the segmented ROIs (cells)\nimage_segmentation = ophys.data_interfaces[\"image_segmentation\"]\nplane_segmentation = image_segmentation.plane_segmentations[\"cell_specimen_table\"]\n\n# Convert to DataFrame for easier viewing\nroi_df = plane_segmentation.to_dataframe()\nprint(f\"Number of segmented ROIs: {len(roi_df)}\")\nprint(\"\\nROI properties:\")\nprint(roi_df.head())\n\n# %% [markdown]\n# Let's visualize the locations of the ROIs:\n\n# %%\n# Visualize the ROI positions\nplt.figure(figsize=(10, 8))\nplt.scatter(roi_df['x'], roi_df['y'], s=roi_df['width']*roi_df['height']/10, \n            alpha=0.7, c=roi_df['valid_roi'], cmap='viridis')\nplt.colorbar(label='Valid ROI')\nplt.title('ROI Positions')\nplt.xlabel('X position (pixels)')\nplt.ylabel('Y position (pixels)')\nplt.gca().invert_yaxis()  # Invert y-axis to match imaging convention\nplt.grid(True)\nplt.show()\n\n# %% [markdown]\n# Let's look at the imaging data itself to understand the field of view:\n\n# %%\n# Get the images from the ophys module\nimages = ophys.data_interfaces[\"images\"]\navailable_images = list(images.images.keys())\nprint(f\"Available images: {available_images}\")\n\n# Display the max projection image\nmax_proj = images.images[\"max_projection\"]\nplt.figure(figsize=(10, 8))\nplt.imshow(max_proj.data[:], cmap='gray')\nplt.title('Max Projection of Field of View')\nplt.colorbar(label='Intensity')\nplt.axis('off')\nplt.show()\n\n# %% [markdown]\n# ### Examining \u0394F/F (dF/F) traces\n\n# %%\n# Get the dF/F data\ndff = ophys.data_interfaces[\"dff\"]\nroi_response_series = dff.roi_response_series[\"traces\"]\n\n# Get the timestamps and data\ntimestamps = roi_response_series.timestamps[:]\ndff_data = roi_response_series.data[:,:]\n\nprint(f\"dF/F data shape: {dff_data.shape}\")  # (time points, ROIs)\nprint(f\"Timestamps length: {len(timestamps)}\")\nprint(f\"Recording duration: {timestamps[-1] - timestamps[0]:.2f} seconds\")\n\n# %% [markdown]\n# Let's plot the dF/F traces for a few example ROIs:\n\n# %%\n# Plot dF/F traces for a few ROIs\nn_roi_to_plot = 5\ntime_window = 300  # seconds to plot\n\n# Find the index corresponding to the time window\nend_idx = np.searchsorted(timestamps, timestamps[0] + time_window)\n\nplt.figure(figsize=(14, 10))\nfor i in range(n_roi_to_plot):\n    plt.plot(timestamps[:end_idx], dff_data[:end_idx, i] + i*3, \n             label=f'ROI {roi_df.index[i]}')\n\nplt.xlabel('Time (s)')\nplt.ylabel('\u0394F/F + offset')\nplt.title(f'\u0394F/F Traces for {n_roi_to_plot} Example ROIs')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Examining Stimulus Presentations\n\n# %%\n# Check the available intervals in the NWB file\nprint(\"Available intervals:\")\nfor interval_name in nwb.intervals:\n    print(f\"  {interval_name}\")\n\n# %% [markdown]\n# Let's look at the movie clip presentations and their timing:\n\n# %%\n# Get dataframes for each stimulus type\ngray_df = nwb.intervals['gray_presentations'].to_dataframe()\nmovie_a_df = nwb.intervals['movie_clip_A_presentations'].to_dataframe()\nmovie_b_df = nwb.intervals['movie_clip_B_presentations'].to_dataframe()\nmovie_c_df = nwb.intervals['movie_clip_C_presentations'].to_dataframe()\n\nprint(f\"Number of gray presentations: {len(gray_df)}\")\nprint(f\"Number of movie clip A presentations: {len(movie_a_df)}\")\nprint(f\"Number of movie clip B presentations: {len(movie_b_df)}\")\nprint(f\"Number of movie clip C presentations: {len(movie_c_df)}\")\n\n# Print some examples\nprint(\"\\nExample movie clip A presentations:\")\nprint(movie_a_df.head())\n\n# %% [markdown]\n# Let's visualize when the different stimuli were presented:\n\n# %%\n# Create a timeline of stimulus presentations\n# We'll color-code each stimulus type\nmax_time = 600  # Plot first 10 minutes for clarity\n\n# Filter dataframes to the time window we want to plot\ngray_df_subset = gray_df[gray_df['start_time'] < max_time]\nmovie_a_df_subset = movie_a_df[movie_a_df['start_time'] < max_time]\nmovie_b_df_subset = movie_b_df[movie_b_df['start_time'] < max_time]\nmovie_c_df_subset = movie_c_df[movie_c_df['start_time'] < max_time]\n\nplt.figure(figsize=(15, 5))\n\n# Plot each stimulus type as horizontal bars\nfor i, (start, stop) in enumerate(zip(gray_df_subset['start_time'], gray_df_subset['stop_time'])):\n    plt.plot([start, stop], [0, 0], 'k-', linewidth=5, alpha=0.7)\n    \nfor i, (start, stop) in enumerate(zip(movie_a_df_subset['start_time'], movie_a_df_subset['stop_time'])):\n    plt.plot([start, stop], [1, 1], 'r-', linewidth=5, alpha=0.7)\n    \nfor i, (start, stop) in enumerate(zip(movie_b_df_subset['start_time'], movie_b_df_subset['stop_time'])):\n    plt.plot([start, stop], [2, 2], 'g-', linewidth=5, alpha=0.7)\n    \nfor i, (start, stop) in enumerate(zip(movie_c_df_subset['start_time'], movie_c_df_subset['stop_time'])):\n    plt.plot([start, stop], [3, 3], 'b-', linewidth=5, alpha=0.7)\n\nplt.yticks([0, 1, 2, 3], ['Gray', 'Movie A', 'Movie B', 'Movie C'])\nplt.xlabel('Time (s)')\nplt.title('Stimulus Presentation Timeline')\nplt.xlim(0, max_time)\nplt.grid(True, axis='x')\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Analyzing Neural Responses to Movie Stimuli\n\n# %% [markdown]\n# Now, let's analyze how neurons respond to the different movie clips:\n\n# %%\n# Define a function to extract trial-averaged responses for each stimulus type\ndef extract_trial_responses(dff_data, timestamps, stim_df, pre_time=0.5, post_time=2.5):\n    \"\"\"\n    Extract trial-averaged responses for each stimulus.\n    \n    Parameters:\n    -----------\n    dff_data: array\n        The dF/F data (time x ROIs)\n    timestamps: array\n        Timestamps for each dF/F data point\n    stim_df: DataFrame\n        Stimulus presentation dataframe\n    pre_time: float\n        Time before stimulus onset to include (seconds)\n    post_time: float\n        Time after stimulus onset to include (seconds)\n    \n    Returns:\n    --------\n    trial_responses: dict\n        Dictionary with trial-averaged responses\n    \"\"\"\n    # Initialize dictionary to store responses\n    trial_responses = {\n        'time': np.arange(-pre_time, post_time, 0.1),  # Assuming 10Hz sampling\n        'mean_response': [],\n        'sem_response': [],\n    }\n    \n    n_rois = dff_data.shape[1]\n    n_timepoints = len(trial_responses['time'])\n    all_trials = np.zeros((len(stim_df), n_timepoints, n_rois))\n    \n    # Loop through all presentations\n    for i, (start_time, _) in enumerate(zip(stim_df['start_time'], stim_df['stop_time'])):\n        # Find indices in dff_data that correspond to this time window\n        start_idx = np.max([0, np.searchsorted(timestamps, start_time - pre_time)])\n        end_idx = np.min([len(timestamps), np.searchsorted(timestamps, start_time + post_time)])\n        \n        # Skip if the window is too short\n        if end_idx - start_idx < n_timepoints:\n            continue\n        \n        # Extract the dF/F data for this trial\n        trial_data = dff_data[start_idx:start_idx+n_timepoints, :]\n        all_trials[i, :, :] = trial_data\n    \n    # Calculate mean and SEM across trials\n    trial_responses['mean_response'] = np.nanmean(all_trials, axis=0)  # time x ROIs\n    trial_responses['sem_response'] = np.nanstd(all_trials, axis=0) / np.sqrt(len(stim_df))\n    \n    return trial_responses\n\n# %% [markdown]\n# Extract trial-averaged responses for each stimulus type:\n\n# %%\n# Extract trial-averaged responses\ngray_responses = extract_trial_responses(dff_data, timestamps, gray_df)\nmovie_a_responses = extract_trial_responses(dff_data, timestamps, movie_a_df)\nmovie_b_responses = extract_trial_responses(dff_data, timestamps, movie_b_df)\nmovie_c_responses = extract_trial_responses(dff_data, timestamps, movie_c_df)\n\nprint(f\"Extracted responses for {dff_data.shape[1]} ROIs\")\nprint(f\"Time points per trial: {len(gray_responses['time'])}\")\n\n# %% [markdown]\n# Let's visualize the average responses of a few example neurons to the different stimuli:\n\n# %%\n# Plot the average responses for a few example ROIs\nn_roi_to_plot = 4\nroi_indices = np.random.choice(dff_data.shape[1], n_roi_to_plot, replace=False)\n\nfig, axes = plt.subplots(n_roi_to_plot, 1, figsize=(12, 3*n_roi_to_plot), sharex=True)\n\nfor i, roi_idx in enumerate(roi_indices):\n    ax = axes[i]\n    \n    # Plot responses to each stimulus\n    ax.plot(gray_responses['time'], gray_responses['mean_response'][:, roi_idx], 'k-', label='Gray')\n    ax.fill_between(gray_responses['time'], \n                    gray_responses['mean_response'][:, roi_idx] - gray_responses['sem_response'][:, roi_idx], \n                    gray_responses['mean_response'][:, roi_idx] + gray_responses['sem_response'][:, roi_idx], \n                    color='k', alpha=0.2)\n    \n    ax.plot(movie_a_responses['time'], movie_a_responses['mean_response'][:, roi_idx], 'r-', label='Movie A')\n    ax.fill_between(movie_a_responses['time'], \n                    movie_a_responses['mean_response'][:, roi_idx] - movie_a_responses['sem_response'][:, roi_idx], \n                    movie_a_responses['mean_response'][:, roi_idx] + movie_a_responses['sem_response'][:, roi_idx], \n                    color='r', alpha=0.2)\n    \n    ax.plot(movie_b_responses['time'], movie_b_responses['mean_response'][:, roi_idx], 'g-', label='Movie B')\n    ax.fill_between(movie_b_responses['time'], \n                    movie_b_responses['mean_response'][:, roi_idx] - movie_b_responses['sem_response'][:, roi_idx], \n                    movie_b_responses['mean_response'][:, roi_idx] + movie_b_responses['sem_response'][:, roi_idx], \n                    color='g', alpha=0.2)\n    \n    ax.plot(movie_c_responses['time'], movie_c_responses['mean_response'][:, roi_idx], 'b-', label='Movie C')\n    ax.fill_between(movie_c_responses['time'], \n                    movie_c_responses['mean_response'][:, roi_idx] - movie_c_responses['sem_response'][:, roi_idx], \n                    movie_c_responses['mean_response'][:, roi_idx] + movie_c_responses['sem_response'][:, roi_idx], \n                    color='b', alpha=0.2)\n    \n    ax.axvline(x=0, linestyle='--', color='gray', alpha=0.7)  # Mark stimulus onset\n    ax.set_ylabel('\u0394F/F')\n    ax.set_title(f'ROI {roi_df.index[roi_idx]} Responses')\n    \n    if i == 0:\n        ax.legend(loc='upper right')\n\naxes[-1].set_xlabel('Time from stimulus onset (s)')\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Exploring Eye Tracking Data\n\n# %%\n# Access the eye tracking data\neye_tracking = nwb.acquisition[\"EyeTracking\"]\neye_data = eye_tracking.eye_tracking.data[:]\neye_timestamps = eye_tracking.eye_tracking.timestamps[:]\npupil_area = eye_tracking.pupil_tracking.area[:]\nblinks = eye_tracking.likely_blink.data[:]\n\nprint(f\"Eye tracking data shape: {eye_data.shape}\")\nprint(f\"Number of data points: {len(eye_timestamps)}\")\nprint(f\"Recording duration: {eye_timestamps[-1] - eye_timestamps[0]:.2f} seconds\")\n\n# %% [markdown]\n# Let's visualize the eye tracking data:\n\n# %%\n# Plot eye position over time\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n\n# Plot x position\naxes[0].plot(eye_timestamps[::100], eye_data[::100, 0], 'b-', alpha=0.8)\naxes[0].set_ylabel('X Position')\naxes[0].set_title('Eye Tracking - X Position')\n\n# Plot y position\naxes[1].plot(eye_timestamps[::100], eye_data[::100, 1], 'r-', alpha=0.8)\naxes[1].set_ylabel('Y Position')\naxes[1].set_title('Eye Tracking - Y Position')\n\n# Plot pupil area\naxes[2].plot(eye_timestamps[::100], pupil_area[::100], 'g-', alpha=0.8)\naxes[2].set_ylabel('Pupil Area')\naxes[2].set_xlabel('Time (s)')\naxes[2].set_title('Pupil Size')\n\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# Let's visualize the eye position as a heatmap:\n\n# %%\n# Create a heatmap of eye positions\nplt.figure(figsize=(10, 8))\nplt.hexbin(eye_data[::10, 0], eye_data[::10, 1], gridsize=50, cmap='hot')\nplt.colorbar(label='Count')\nplt.title('Eye Position Heatmap')\nplt.xlabel('X Position')\nplt.ylabel('Y Position')\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Examining Running Behavior\n\n# %%\n# Access running data\nrunning = nwb.processing[\"running\"]\nspeed = running.data_interfaces[\"speed\"].data[:]\nspeed_timestamps = running.data_interfaces[\"speed\"].timestamps[:]\n\nprint(f\"Running data length: {len(speed)}\")\nprint(f\"Recording duration: {speed_timestamps[-1] - speed_timestamps[0]:.2f} seconds\")\nprint(f\"Mean running speed: {np.mean(speed):.2f} cm/s\")\nprint(f\"Max running speed: {np.max(speed):.2f} cm/s\")\n\n# %% [markdown]\n# Let's visualize the running speed over time:\n\n# %%\n# Plot running speed\nplt.figure(figsize=(14, 6))\nplt.plot(speed_timestamps[::100], speed[::100], 'k-', alpha=0.8)\nplt.axhline(y=0, color='gray', linestyle='--')\nplt.ylabel('Running Speed (cm/s)')\nplt.xlabel('Time (s)')\nplt.title('Running Speed Over Time')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Relating Neural Activity to Behavior\n\n# %% [markdown]\n# Let's examine how neural activity relates to running speed by computing the correlation between each neuron's activity and the animal's running speed:\n\n# %%\n# Resample the running speed to match the dF/F timestamps\nfrom scipy.interpolate import interp1d\n\n# Create an interpolation function for running speed\nspeed_interp = interp1d(speed_timestamps, speed, bounds_error=False, fill_value=np.nan)\n\n# Interpolate speed to match dF/F timestamps\ninterp_speed = speed_interp(timestamps)\n\n# Calculate correlation between each neuron and running speed\ncorrelations = []\np_values = []\nfrom scipy.stats import pearsonr\nfor i in range(dff_data.shape[1]):\n    # Remove NaNs for correlation calculation\n    valid_idx = ~np.isnan(interp_speed)\n    if np.sum(valid_idx) > 100:  # Only compute if we have enough data\n        r, p = pearsonr(dff_data[valid_idx, i], interp_speed[valid_idx])\n        correlations.append(r)\n        p_values.append(p)\n    else:\n        correlations.append(np.nan)\n        p_values.append(np.nan)\n\n# Convert to numpy arrays\ncorrelations = np.array(correlations)\np_values = np.array(p_values)\n\n# %% [markdown]\n# Let's visualize the distribution of correlations between neural activity and running speed:\n\n# %%\n# Plot histogram of correlations\nplt.figure(figsize=(10, 6))\nplt.hist(correlations, bins=20, alpha=0.7, color='blue')\nplt.axvline(x=0, color='k', linestyle='--')\nplt.xlabel('Correlation Coefficient (r)')\nplt.ylabel('Number of ROIs')\nplt.title('Distribution of Correlations Between Neural Activity and Running Speed')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# Let's visualize the dF/F traces of the neurons with the highest positive and negative correlations with running speed:\n\n# %%\n# Find the neurons with highest positive and negative correlations\ntop_pos_idx = np.nanargmax(correlations)\ntop_neg_idx = np.nanargmin(correlations)\n\nprint(f\"Neuron with highest positive correlation: ROI {roi_df.index[top_pos_idx]}, r = {correlations[top_pos_idx]:.3f}\")\nprint(f\"Neuron with highest negative correlation: ROI {roi_df.index[top_neg_idx]}, r = {correlations[top_neg_idx]:.3f}\")\n\n# Plot a time window of data to see the relationship\ntime_window = 300  # seconds\nstart_time = 100  # start at 100s to skip initial transients\nend_idx = np.searchsorted(timestamps, start_time + time_window)\nstart_idx = np.searchsorted(timestamps, start_time)\n\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n\n# Plot running speed\naxes[0].plot(timestamps[start_idx:end_idx], interp_speed[start_idx:end_idx], 'k-', alpha=0.8)\naxes[0].set_ylabel('Running Speed (cm/s)')\naxes[0].set_title('Running Speed')\n\n# Plot positively correlated neuron\naxes[1].plot(timestamps[start_idx:end_idx], dff_data[start_idx:end_idx, top_pos_idx], 'g-', alpha=0.8)\naxes[1].set_ylabel('\u0394F/F')\naxes[1].set_title(f'Positively Correlated ROI {roi_df.index[top_pos_idx]} (r = {correlations[top_pos_idx]:.3f})')\n\n# Plot negatively correlated neuron\naxes[2].plot(timestamps[start_idx:end_idx], dff_data[start_idx:end_idx, top_neg_idx], 'r-', alpha=0.8)\naxes[2].set_ylabel('\u0394F/F')\naxes[2].set_xlabel('Time (s)')\naxes[2].set_title(f'Negatively Correlated ROI {roi_df.index[top_neg_idx]} (r = {correlations[top_neg_idx]:.3f})')\n\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# ## Summary\n\n# %% [markdown]\n# In this notebook, we've explored the Allen Institute Openscope - Sequence Learning Project dataset. We've learned how to:\n# \n# 1. Access metadata about the dandiset using the DANDI API\n# 2. Load and explore NWB files containing two-photon calcium imaging data\n# 3. Examine ROI (neuron) properties and visualize their locations\n# 4. Analyze neural responses to different movie stimuli\n# 5. Explore eye tracking and running behavior data\n# 6. Correlate neural activity with behavioral measurements\n# \n# For future exploration, you might consider:\n# - Analyzing data across multiple sessions to study sequence learning effects\n# - Comparing responses across different cortical areas\n# - Developing decoding models to predict stimulus identity from neural activity\n# - Implementing more advanced analyses of eye movements during stimulus presentations\n</notebook>"
  }
]